{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TSAD_with_image_encodings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsg31/TS-AD-with-2D-CAEs/blob/master/TSAD_with_image_GAF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIu_2uhQVVv3",
        "colab_type": "text"
      },
      "source": [
        "# Time Series Anomaly Detection with Image Encodings\n",
        "\n",
        "This colab is built from the code developed by Gabriel Rodriguez Garcia in the frame of the collaboration research work made between Airbus AI Research and the Chair of Intelligent Maintenance Systems of ETH Zurich. \n",
        "\n",
        "ETH Zurich:\n",
        "  - Pr. Olga Fink\n",
        "  - Dr. Gabriel Michau\n",
        "  - **Gabriel Rodriguez Garcia**\n",
        "\n",
        "Airbus AI Research\n",
        "  - Mélanie Ducoffe\n",
        "  - Jayant Sen Gupta\n",
        "  \n",
        "  \n",
        "The idea is to transform (1D) time series data into images and use anomaly detection techniques on images to spot anomalies.  \n",
        "\n",
        "*note: code is not yet put into a python, so it is a bit dirty*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3SdBtzweMXd",
        "colab_type": "code",
        "outputId": "d910e5a7-e398-4c52-e48a-a1f2ebb3b308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/gabriel-rodriguez-garcia/TS-AD-with-2D-CAEs.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TS-AD-with-2D-CAEs'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/22)\u001b[K\rremote: Counting objects:   9% (2/22)\u001b[K\rremote: Counting objects:  13% (3/22)\u001b[K\rremote: Counting objects:  18% (4/22)\u001b[K\rremote: Counting objects:  22% (5/22)\u001b[K\rremote: Counting objects:  27% (6/22)\u001b[K\rremote: Counting objects:  31% (7/22)\u001b[K\rremote: Counting objects:  36% (8/22)\u001b[K\rremote: Counting objects:  40% (9/22)\u001b[K\rremote: Counting objects:  45% (10/22)\u001b[K\rremote: Counting objects:  50% (11/22)\u001b[K\rremote: Counting objects:  54% (12/22)\u001b[K\rremote: Counting objects:  59% (13/22)\u001b[K\rremote: Counting objects:  63% (14/22)\u001b[K\rremote: Counting objects:  68% (15/22)\u001b[K\rremote: Counting objects:  72% (16/22)\u001b[K\rremote: Counting objects:  77% (17/22)\u001b[K\rremote: Counting objects:  81% (18/22)\u001b[K\rremote: Counting objects:  86% (19/22)\u001b[K\rremote: Counting objects:  90% (20/22)\u001b[K\rremote: Counting objects:  95% (21/22)\u001b[K\rremote: Counting objects: 100% (22/22)\u001b[K\rremote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 275 (delta 9), reused 0 (delta 0), pack-reused 253\u001b[K\n",
            "Receiving objects: 100% (275/275), 791.51 KiB | 2.06 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHkOjYFTcI4R",
        "colab_type": "code",
        "outputId": "6d16c05a-f079-4358-b089-ae4ca866a589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  TS-AD-with-2D-CAEs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqUBeTxfdw0a",
        "colab_type": "code",
        "outputId": "269d770d-d6e6-48c3-b8a2-f0dc02418eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip3 install pyts\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyts\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/80/5f531de808dbf1625af0434f2fcc55f8c6488980d97ba2cab5cce285c527/pyts-0.9.0-py3-none-any.whl (2.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 24.2MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 6.6MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 9.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |▋                               | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 9.7MB/s eta 0:00:01\r\u001b[K     |█                               | 81kB 10.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92kB 11.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 102kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 112kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 122kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 133kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 143kB 9.6MB/s eta 0:00:01\r\u001b[K     |██                              | 153kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 163kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 174kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 184kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 194kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 204kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 215kB 9.6MB/s eta 0:00:01\r\u001b[K     |███                             | 225kB 9.6MB/s eta 0:00:01\r\u001b[K     |███                             | 235kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 245kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 256kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 266kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 276kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 286kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 296kB 9.6MB/s eta 0:00:01\r\u001b[K     |████                            | 307kB 9.6MB/s eta 0:00:01\r\u001b[K     |████                            | 317kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 327kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 337kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 348kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 358kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 368kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 378kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 389kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 399kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 409kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 419kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 430kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 440kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 450kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 460kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 471kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 481kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 491kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 501kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 512kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 522kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 532kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 542kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 552kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 563kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 573kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 583kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 593kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 604kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 614kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 624kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 634kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 645kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 655kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 665kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 675kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 686kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 696kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 706kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 716kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 727kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 737kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 747kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 757kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 768kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 778kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 788kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 798kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 808kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 819kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 829kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 839kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 849kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 860kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 870kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 880kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 890kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 901kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 911kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 921kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 931kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 942kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 952kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 962kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 972kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 983kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 993kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.6MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.7MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.7MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.7MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.7MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.7MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.7MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.7MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.8MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.0MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.1MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.2MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.3MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.4MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4scipy>=1.3.0scikit-learn>=0.20.1numba>=0.41.0 in /usr/local/lib/python3.6/dist-packages (from pyts) (1.16.5)\n",
            "Installing collected packages: pyts\n",
            "Successfully installed pyts-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w31HtxHEdB-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load TS-AD-with-2D-CAEs/Part1_Encoding/GAF/GAF.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHWIxAZj_C7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pyts.image import GramianAngularField\n",
        "from skimage.measure import block_reduce\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-Ld5Ory7pQP",
        "colab_type": "code",
        "outputId": "72bb0129-a172-4e04-e107-b276811bf248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "!pip3 install wget\n",
        "import wget\n",
        "url = 'https://advised-public-data.s3-eu-west-1.amazonaws.com/dftrain.h5'\n",
        "filename = wget.download(url)\n",
        "filename\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=ebcc1d58986b2b3438cea3ff08910161bc6a19b806d57da4a7b54acda94d4656\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dftrain.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0Bb7elWdtsJ",
        "colab_type": "code",
        "outputId": "cc099cc5-7540-4e91-e96b-4e1c33688d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = np.array(pd.read_hdf(filename))\n",
        "print('Data set shape:',np.shape(X))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data set shape: (1677, 61440)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQHDanZsfI8J",
        "colab_type": "code",
        "outputId": "efdbb6c7-a57f-4b1f-8fcf-4ec7a389ff7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "url2 = 'https://advised-public-data.s3-eu-west-1.amazonaws.com/dfvalid.h5'\n",
        "filename2 = wget.download(url2)\n",
        "filename2\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dfvalid.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StDoVVMz8IAz",
        "colab_type": "code",
        "outputId": "d2ecc471-40b0-4258-b797-f1bdbb02b3c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xtest = np.array(pd.read_hdf(filename2))\n",
        "print('Data set shape:',np.shape(Xtest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data set shape: (594, 61440)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byao50K281Uk",
        "colab_type": "code",
        "outputId": "69994680-9c09-4d83-ce4f-73d551653df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\" Cut each time series in the dataset into equally sized slices of size -> Length of time series/n_slices.\n",
        "The function returns a matrix with shape (nr. of slices, slice length)\"\"\"\n",
        "\n",
        "n_slices = 120\n",
        "\n",
        "n,l = np.shape(X)\n",
        "X1 = np.reshape(X,(n*n_slices,l//n_slices))\n",
        "\n",
        "print('sliced data shape (nr. of slices, slice length):',np.shape(X1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sliced data shape (nr. of slices, slice length): (201240, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVQdwZO49Huy",
        "colab_type": "code",
        "outputId": "9f5827fa-5938-4120-cb5d-351cb6f2ef50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n,l = np.shape(Xtest)\n",
        "X2 = np.reshape(Xtest,(n*n_slices,l//n_slices))\n",
        "\n",
        "print('sliced data shape (nr. of slices, slice length):',np.shape(X2))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sliced data shape (nr. of slices, slice length): (71280, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGtP4nRc9Xcx",
        "colab_type": "code",
        "outputId": "11fdda14-32d7-43d8-9fb9-5c9d9b3688b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "\"\"\" \n",
        "Computation of encodings has to be done in batches due to the large size of the dataset.\n",
        "Otherwise the kernel will die!\n",
        "\n",
        "For downscaling pick np.mean (average pooling) or np.max (max pooling) respectively.\n",
        "If downscaling is not required choose downscale_factor=1.\n",
        "Keep in mind the network expects an input image size of 64x64.\n",
        "\n",
        "The function returns a 3D matrix.\n",
        "The new 3D matrix contains several 2D matrices, which correspond to the time series encodings/images.\n",
        "The order of the objects does not change, which means for example that the 23rd slice of the \n",
        "input dataset corresponds to the 23rd encoding in the 3D Matrix.\n",
        "\"\"\"\n",
        "\n",
        "batch_size=172\n",
        "downscale_factor=8\n",
        "pooling_function=np.mean\n",
        "# limiting to 100 lines for memory reason\n",
        "#N  = 2\n",
        "#dataset = X1[:N*batch_size]\n",
        "#print(dataset)\n",
        "dataset = X1\n",
        "\n",
        "n,l = np.shape(dataset)\n",
        "f = downscale_factor\n",
        "n_batches = n//batch_size\n",
        "#print(n_batches)\n",
        "batches = np.linspace(1,n_batches,n_batches, dtype=int) * batch_size\n",
        "\n",
        "gaf = GramianAngularField(image_size=1., method='summation')\n",
        "\n",
        "print('Encoding started...')\n",
        "for p in range(n_batches):\n",
        "    if p==0:\n",
        "        X_gaf = gaf.transform(dataset[0:batches[p],:])\n",
        "        sample=block_reduce(X_gaf[0], block_size=(f, f), func=pooling_function)\n",
        "        l_red = sample.shape[0]\n",
        "        X_gaf_red = np.zeros((n,l_red,l_red))\n",
        "        print('output 3D Matrix shape: ', np.shape(X_gaf_red))\n",
        "\n",
        "        j=0\n",
        "        for i in range(0,batches[p]):\n",
        "            X_gaf_red[i] = block_reduce(X_gaf[j], block_size=(f, f) , func=pooling_function)\n",
        "            j+=1\n",
        "\n",
        "    else:                          \n",
        "        X_gaf = gaf.transform(dataset[batches[p-1]:batches[p],:])\n",
        "\n",
        "        j=0\n",
        "        for i in range(batches[p-1],batches[p]):\n",
        "            X_gaf_red[i] = block_reduce(X_gaf[j], block_size=(f, f) , func=pooling_function)\n",
        "            j+=1\n",
        "\n",
        "print('Encoding successful!')\n",
        "print('#####################################')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding started...\n",
            "output 3D Matrix shape:  (201240, 64, 64)\n",
            "Encoding successful!\n",
            "#####################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA_6CVAIAUEj",
        "colab_type": "code",
        "outputId": "45b38633-a059-4c9e-98e0-939fd14cda2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset_name = 'X_gaf.npy'\n",
        "directory = 'GAF/GAF_Images'\n",
        "\n",
        "if not os.path.exists('GAF'):\n",
        "    os.makedirs('GAF')\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "\n",
        "print('Saving data...')\n",
        "np.save(os.path.join(directory,dataset_name), X_gaf_red)\n",
        "print(dataset_name,' saved at: ',os.path.join(directory,dataset_name))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving data...\n",
            "X_gaf.npy  saved at:  GAF/GAF_Images/X_gaf.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXscwMQl-Hlc",
        "colab_type": "code",
        "outputId": "41915353-2e30-431e-87a9-2de03afad3d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "\"\"\"\n",
        "same for test data\n",
        "\"\"\"\n",
        "\n",
        "#dataset = X2[:N*batch_size]\n",
        "dataset = X2\n",
        "n,l = np.shape(dataset)\n",
        "f = downscale_factor\n",
        "n_batches = n//batch_size\n",
        "batches = np.linspace(1,n_batches,n_batches, dtype=int) * batch_size\n",
        "\n",
        "gaf = GramianAngularField(image_size=1., method='summation')\n",
        "\n",
        "print('Encoding started...')\n",
        "for p in range(n_batches):\n",
        "    if p==0:\n",
        "        X_gaf = gaf.transform(dataset[0:batches[p],:])\n",
        "        sample=block_reduce(X_gaf[0], block_size=(f, f), func=pooling_function)\n",
        "        l_red = sample.shape[0]\n",
        "        X_gaf_red = np.zeros((n,l_red,l_red))\n",
        "        print('output 3D Matrix shape: ', np.shape(X_gaf_red))\n",
        "\n",
        "        j=0\n",
        "        for i in range(0,batches[p]):\n",
        "            X_gaf_red[i] = block_reduce(X_gaf[j], block_size=(f, f) , func=pooling_function)\n",
        "            j+=1\n",
        "\n",
        "    else:                          \n",
        "        X_gaf = gaf.transform(dataset[batches[p-1]:batches[p],:])\n",
        "\n",
        "        j=0\n",
        "        for i in range(batches[p-1],batches[p]):\n",
        "            X_gaf_red[i] = block_reduce(X_gaf[j], block_size=(f, f) , func=pooling_function)\n",
        "            j+=1\n",
        "\n",
        "print('Encoding successful!')\n",
        "print('#####################################')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding started...\n",
            "output 3D Matrix shape:  (71280, 64, 64)\n",
            "Encoding successful!\n",
            "#####################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAhUvvII_f3x",
        "colab_type": "code",
        "outputId": "b459f451-b87f-4af1-de05-3870ad69e345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset_name = 'X_test_gaf.npy'\n",
        "directory = 'GAF/GAF_Images'\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "\n",
        "print('Saving data...')\n",
        "np.save(os.path.join(directory,dataset_name), X_gaf_red)\n",
        "print(dataset_name,' saved at: ',os.path.join(directory,dataset_name))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving data...\n",
            "X_test_gaf.npy  saved at:  GAF/GAF_Images/X_test_gaf.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6daD8GxA4AF",
        "colab_type": "code",
        "outputId": "7af6ebb2-6819-4303-d10c-b5254afbcd3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "X_gaf=np.load('GAF/GAF_Images/X_gaf.npy')\n",
        "sample_image=X_gaf[0] \n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(sample_image,cmap='jet')\n",
        "plt.colorbar(fraction=0.0457, pad=0.04)\n",
        "plt.clim(-1,1)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEiCAYAAACxy7qgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGub/AyN0XlyohUldAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BHb5_o1p4oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import progressbar\n",
        "from scipy import signal\n",
        "from skimage.measure import block_reduce\n",
        "import os\n",
        "#from scipy.misc import imresize\n",
        "from PIL import Image\n",
        "\n",
        "factor=8\n",
        "smoothness_factor=4\n",
        "widths = np.linspace(1,64,64)\n",
        "widths= 2**(widths/smoothness_factor)\n",
        "dataset = X1\n",
        "n=np.shape(dataset)[0]\n",
        "X_sc=np.zeros((n,64,64))\n",
        "signal_type = signal.ricker\n",
        "pooling_function=np.mean\n",
        "for i in range(0,n):\n",
        "  print(i)\n",
        "  cwtmatr = signal.cwt(dataset[i,:], signal_type, widths)\n",
        "  X_sc[i]=block_reduce(cwtmatr, block_size=(1, factor), func=pooling_function)\n",
        "\n",
        "print('Encoding successful!')\n",
        "print('#####################################')\n",
        "\n",
        "if not os.path.exists('SC'):\n",
        "  os.makedirs('SC')\n",
        "\n",
        "directory = 'SC/SC_Images'\n",
        "dataset_name = 'X_sc.npy'\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "print('Saving data...')\n",
        "np.save(os.path.join(directory,dataset_name),dataset)\n",
        "print(dataset_name,' saved at: ',os.path.join(directory,dataset_name))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5BJtdZFMJvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = X2\n",
        "n=np.shape(dataset)[0]\n",
        "X_sc=np.zeros((n,64,64))\n",
        "signal_type = signal.ricker\n",
        "pooling_function=np.mean\n",
        "for i in range(0,n):\n",
        "    cwtmatr = signal.cwt(dataset[i,:], signal_type, widths)\n",
        "    X_sc[i]=block_reduce(cwtmatr, block_size=(1, factor), func=pooling_function)\n",
        "\n",
        "print('Encoding successful!')\n",
        "print('#####################################')\n",
        "dataset_name = 'X_test_sc.npy'\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "print('Saving data...')\n",
        "np.save(os.path.join(directory,dataset_name),dataset)\n",
        "print(dataset_name,' saved at: ',os.path.join(directory,dataset_name))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQFVy5POOxf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uuGplN3iWwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Prepare Batch #######################################################\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import newaxis\n",
        "\n",
        "\n",
        "class Batch(object):\n",
        "    \"\"\"\n",
        "    Prepare data batches for training and testing.\n",
        "\n",
        "    For Training the batches are selected randomly.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.training_images = None\n",
        "        self.testing_images = None\n",
        "\n",
        "    def get_batch(self, batch_size, dataset, dataset_name):\n",
        "        \"\"\"\n",
        "        get a batch of images\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if dataset_name == 'training':\n",
        "\n",
        "            training_set_3d = dataset\n",
        "\n",
        "            if self.training_images is None:\n",
        "                self.training_images = training_set_3d\n",
        "            #               # Pad images if necessary...\n",
        "            #             images = np.pad(self.training_images, pad_width=[[0, 0], [3, 4],[3, 4]],mode= \"constant\",constant_values=0)\n",
        "\n",
        "            images = self.training_images[:, :, :, newaxis]\n",
        "\n",
        "\n",
        "        elif dataset_name == 'testing':\n",
        "            validation_set_3d = dataset\n",
        "\n",
        "            if self.testing_images is None:\n",
        "                self.testing_images = validation_set_3d\n",
        "            #               # Pad images if necessary...\n",
        "            #             images = np.pad(self.training_images, pad_width=[[0, 0], [3, 4],[3, 4]],mode= \"constant\",constant_values=0)\n",
        "            images = self.testing_images[:, :, :, newaxis]\n",
        "\n",
        "\n",
        "        else:\n",
        "\n",
        "            return\n",
        "\n",
        "        num_samples = images.shape[0]\n",
        "        idx = np.random.randint(num_samples, size=batch_size)\n",
        "\n",
        "        return images[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMg6VpJliW7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Architecture SPECIAL Building Blocks\n",
        "#######################################################\n",
        "\n",
        "# Each Layer is defined as a class and later on used as a building block for the Architecture.\n",
        "import tensorflow as tf\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "\n",
        "class Layer(object, metaclass=ABCMeta):\n",
        "    \"\"\"\n",
        "    Abstract Class used for general Building Blocks\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def call(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.call(*args, **kwargs)\n",
        "\n",
        "\n",
        "class Unfold(Layer):\n",
        "    \"\"\"\n",
        "    Unfold Building Block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 scope=''):\n",
        "        Layer.__init__(self)\n",
        "\n",
        "        self.scope = scope\n",
        "\n",
        "    def build(self, input_tensor):\n",
        "        num_batch, height, width, num_channels = input_tensor.get_shape()\n",
        "\n",
        "        return tf.reshape(input_tensor, [-1, (height * width * num_channels).value])\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        if self.scope:\n",
        "            with tf.variable_scope(self.scope) as scope:\n",
        "                return self.build(input_tensor)\n",
        "        else:\n",
        "            return self.build(input_tensor)\n",
        "\n",
        "\n",
        "class Fold(Layer):\n",
        "    \"\"\"\n",
        "    Fold Building Block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 fold_shape,\n",
        "                 scope=''):\n",
        "        Layer.__init__(self)\n",
        "\n",
        "        self.fold_shape = fold_shape\n",
        "        self.scope = scope\n",
        "\n",
        "    def build(self, input_tensor):\n",
        "        return tf.reshape(input_tensor, self.fold_shape)\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        if self.scope:\n",
        "            with tf.variable_scope(self.scope) as scope:\n",
        "                return self.build(input_tensor)\n",
        "        else:\n",
        "            return self.build(input_tensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GkCtszViW4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Trainingschedule #######################################################\n",
        "import tensorflow as tf\n",
        "\n",
        "class Models(object):\n",
        "    \"\"\"\n",
        "    Definition of the training procedure.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    #@staticmethod\n",
        "    def start_new_session(self, sess):\n",
        "        saver = tf.train.Saver()\n",
        "        global_step = 0\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        return saver, global_step\n",
        "\n",
        "    #@staticmethod\n",
        "    def continue_previous_session(self, sess, ckpt_file):\n",
        "        saver = tf.train.Saver()  # create a saver\n",
        "\n",
        "        with open(ckpt_file) as file:  # read checkpoint file\n",
        "            line = file.readline()  # read the first line, which contains the file name of the latest checkpoint\n",
        "            ckpt = line.split('\"')[1]\n",
        "            global_step = int(ckpt.split('-')[1])\n",
        "            print(ckpt)\n",
        "        # restore\n",
        "        saver.restore(sess, 'saver/'+ckpt)\n",
        "        print('restored from checkpoint ' + ckpt)\n",
        "\n",
        "        return saver, global_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hRN2LCAA8tw",
        "colab_type": "code",
        "outputId": "d8554866-a53c-4ddf-a738-2696d1b77c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense,Conv2D, Conv2DTranspose\n",
        "from keras.layers import AveragePooling2D, UpSampling2D\n",
        "\n",
        "### Global Parameters\n",
        "########################################################\n",
        "\n",
        "# Change Parameters here and use them everywhere in the code using: argp.<variable_name>\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Paths/Directories\n",
        "parser.add_argument(\"--path_data\", type=str, help=\"Path to folder of input data\")\n",
        "\n",
        "# Work Schedule\n",
        "\n",
        "parser.add_argument(\"--mode\", choices=[\"new_training\",\"continue_training\",\"testing\"])\n",
        "parser.add_argument(\"--dataset\", choices=[\"training\",\"testing\"])\n",
        "parser.add_argument(\"--cycles\",                type=int, help=\"Number of gradient descent steps\")\n",
        "parser.add_argument(\"--performance_eval_steps\",type=int, help=\"Interval: number of steps to compute the loss\")\n",
        "parser.add_argument(\"--checkpoint_save_steps\", type=int, help=\"Interval: number of steps to create a checkpoint of the model's state\")\n",
        "parser.add_argument(\"--batch_size_testing\",    type=int, help=\"Number of images to show when testing\")\n",
        "parser.add_argument(\"--batch_size\",            type=int, help=\"Number of images to use for computing loss while training\")\n",
        "\n",
        "# Architecture parameters\n",
        "\n",
        "parser.add_argument(\"--conv_kernel_size_1\",    type=int)\n",
        "parser.add_argument(\"--conv_stride_1\",         type=int)\n",
        "parser.add_argument(\"--pool_kernel_size\",      type=int)\n",
        "parser.add_argument(\"--pool_stride\",           type=int)\n",
        "parser.add_argument(\"--nr_channels_1\",         type=int, help=\"Number of channels in first layer\")\n",
        "parser.add_argument(\"--bottleneck_size\",       type=int, help=\"Number of neurons in the bottleneck layer\")\n",
        "\n",
        "# Encodings\n",
        "parser.add_argument(\"--encoding\",              type=str, \n",
        "                    choices=[\"GAF\",\"MTF\",\"RP\",\"SP\",\"SC\",\"GS\"],\n",
        "                    help=\"The encoding image matrix (e.g. X_gaf.npy) must be created first in Part 1!\")\n",
        "\n",
        "\n",
        "argp = parser.parse_args(\n",
        "    ['--path_data','.',\n",
        "     '--mode','new_training',\n",
        "     '--dataset','training',\n",
        "     '--cycles','50000',\n",
        "     '--conv_kernel_size_1','4',\n",
        "     '--conv_stride_1','2',\n",
        "     '--pool_kernel_size','2',\n",
        "     '--pool_stride','2',\n",
        "     '--nr_channels_1','32', \n",
        "     '--bottleneck_size','160',\n",
        "     '--batch_size','100',\n",
        "     '--batch_size_testing','10',\n",
        "     '--performance_eval_steps','10',\n",
        "     '--checkpoint_save_steps','10000',\n",
        "     '--encoding','GAF'])\n",
        "\n",
        "### Import Encoding Matrix\n",
        "########################################################\n",
        "\n",
        "if argp.dataset=='training':\n",
        "    \n",
        "    if argp.encoding == 'GAF':\n",
        "        X=np.load(os.path.join(argp.path_data,'GAF/GAF_Images/X_gaf.npy'))\n",
        "    elif argp.encoding == 'MTF':\n",
        "        X=np.load(os.path.join(argp.path_data,'MTF/MTF_Images/X_mtf.npy'))\n",
        "    elif argp.encoding == 'RP':\n",
        "        X=np.load(os.path.join(argp.path_data,'RP/RP_Images/X_rp.npy'))\n",
        "    elif argp.encoding == 'SP':\n",
        "        X=np.load(os.path.join(argp.path_data,'SP/SP_Images/X_sp.npy'))\n",
        "    elif argp.encoding == 'SC':\n",
        "        X=np.load(os.path.join(argp.path_data,'SC/SC_Images/X_sc.npy'))\n",
        "    elif argp.encoding == 'GS':\n",
        "        X=np.load(os.path.join(argp.path_data,'GS/GS_Images/X_gs.npy'))\n",
        "    else:\n",
        "        print('Check if encoding variable matches one of the following names:')\n",
        "        print('GAF','MTF','RP','SP','SC','GS')\n",
        "\n",
        "    n,l,_=X.shape\n",
        "    print('number of images in training set:',n)\n",
        "    \n",
        "elif argp.dataset== 'testing': \n",
        "  \n",
        "    if argp.encoding == 'GAF':\n",
        "        X_test=np.load(os.path.join(argp.path_data,'GAF/GAF_Images/X_test_gaf.npy'))\n",
        "    elif argp.encoding == 'MTF':\n",
        "        X_test=np.load(os.path.join(argp.path_data,'MTF/MTF_Images/X_test_mtf.npy'))\n",
        "    elif argp.encoding == 'RP':\n",
        "        X_test=np.load(os.path.join(argp.path_data,'RP/RP_Images/X_test_rp.npy'))\n",
        "    elif argp.encoding == 'SP':\n",
        "        X_test=np.load(os.path.join(argp.path_data,'SP/SP_Images/X_test_sp.npy'))\n",
        "    elif argp.encoding == 'SC':\n",
        "        X_test=np.load(os.path.join(argp.path_data,'SC/SC_Images/X_test_sc.npy'))\n",
        "    elif argp.encoding == 'GS':\n",
        "        X_test=np.load(os.path.join(argp.path_data,'GS/GS_Images/X_test_gs.npy'))\n",
        "    else:\n",
        "        print('Check if encoding variable matches one of the following names:')\n",
        "        print('GAF', 'MTF', 'RP', 'SP', 'SC', 'GS')\n",
        "\n",
        "    n_valid,l,_=X_test.shape\n",
        "    print('number of images in testing set:',n_valid)\n",
        "\n",
        "\n",
        "print('Encoding:',argp.encoding)\n",
        "print('image size:',l,'x',l)\n",
        "\n",
        "class ConvolutionalAutoencoder(object):\n",
        "    \"\"\"\n",
        "    Build the model using building Blocks\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        build the Graph\n",
        "        \"\"\"\n",
        "        \n",
        "        x = tf.placeholder(tf.float32, shape=[None, l, l, 1]) # [# batch, img_height, img_width, #channels]\n",
        "        print('input',x.get_shape())\n",
        "        \n",
        "        h = Conv2D(filters=argp.nr_channels_1, \n",
        "                   kernel_size=argp.conv_kernel_size_1, \n",
        "                   strides=(argp.conv_stride_1, argp.conv_stride_1), \n",
        "                   padding='same',\n",
        "                   activation=tf.nn.leaky_relu)(x)\n",
        "        print('conv1',h.get_shape())\n",
        "        \n",
        "        pool1 = AveragePooling2D(pool_size=(argp.pool_kernel_size, argp.pool_kernel_size), \n",
        "                                 strides=argp.pool_stride, \n",
        "                                 padding='same')(h)\n",
        "        print('pool1',pool1.get_shape())\n",
        "        \n",
        "        unfold = Unfold(scope='unfold')(pool1)\n",
        "        print('unfold',unfold.get_shape())\n",
        "        \n",
        "        h = Dense(units=argp.bottleneck_size, activation=tf.nn.leaky_relu)(unfold)\n",
        "        print('dense1',h.get_shape())\n",
        "        \n",
        "        h = Dense(units=int(unfold.get_shape()[1]), activation=tf.nn.leaky_relu)(h)\n",
        "        print('dense2',h.get_shape())\n",
        "        \n",
        "        h = Fold(fold_shape = [-1, int(pool1.get_shape()[1]), int(pool1.get_shape()[1]), argp.nr_channels_1], \n",
        "                 scope      = 'fold')(h)\n",
        "        print('fold',h.get_shape())\n",
        "        \n",
        "        h = UpSampling2D(size=(argp.pool_kernel_size,argp.pool_kernel_size))(h)\n",
        "        print('unpool',h.get_shape())\n",
        "        \n",
        "        reconstruction = Conv2DTranspose(filters=1, \n",
        "                                         kernel_size=argp.conv_kernel_size_1, \n",
        "                                         strides=argp.conv_stride_1,\n",
        "                                         padding='same',\n",
        "                                         activation=tf.nn.leaky_relu)(h)\n",
        "        \n",
        "        print('reconstruction',reconstruction.get_shape())\n",
        "        \n",
        "        # Loss\n",
        "        loss = tf.divide(tf.nn.l2_loss(x - reconstruction), argp.batch_size)  # L2 loss\n",
        "        \n",
        "        # Optimizer\n",
        "        training = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
        "        \n",
        "        tf.summary.scalar('loss',loss)\n",
        "\n",
        "        self.x = x\n",
        "        self.reconstruction = reconstruction\n",
        "        self.loss = loss\n",
        "        self.training = training\n",
        "        \n",
        "    def train(self, batch_size, passes, new_training):\n",
        "            \"\"\"\n",
        "            training process configuration\n",
        "\n",
        "            \"\"\"\n",
        "            batch = Batch()\n",
        "\n",
        "            with tf.Session() as sess:\n",
        "                # prepare session\n",
        "                if new_training:\n",
        "                    print('new_training')\n",
        "\n",
        "                    merged_summary = tf.summary.merge_all()\n",
        "                    file_writer = tf.summary.FileWriter('tensorboard_summary', sess.graph)\n",
        "                    model = Models()\n",
        "                    saver, global_step = model.start_new_session(sess)\n",
        "                else:\n",
        "\n",
        "                    merged_summary = tf.summary.merge_all()\n",
        "                    file_writer = tf.summary.FileWriter('tensorboard_summary', sess.graph)\n",
        "                    model = Models()\n",
        "                    saver, global_step = model.continue_previous_session(sess, ckpt_file='saver/checkpoint')\n",
        "\n",
        "                # start training\n",
        "                for step in range(1+global_step, 1+passes+global_step):\n",
        "                    x = batch.get_batch(batch_size, dataset_name=argp.dataset, dataset=X)\n",
        "                    self.training.run(feed_dict={self.x: x})\n",
        "\n",
        "                    if step % argp.performance_eval_steps == 0:\n",
        "                        print('Evaluating Performance...')\n",
        "                        loss_eval = self.loss.eval(feed_dict={self.x: x})\n",
        "\n",
        "                        result = sess.run(merged_summary,feed_dict={self.x: x})\n",
        "                        file_writer.add_summary(result, step)\n",
        "\n",
        "                        print(\"pass {}, training loss {}\".format(step, loss_eval))\n",
        "\n",
        "                    if step % argp.checkpoint_save_steps == 0:  # save weights\n",
        "                        print('Saving Checkpoint...')\n",
        "                        saver.save(sess, 'saver/cnn', global_step=step, write_meta_graph=True)\n",
        "                        print('checkpoint saved')\n",
        "\n",
        "    def reconstruct(self): \n",
        "        \"\"\"\n",
        "        reconstruction process configuration\n",
        "        \"\"\"\n",
        "\n",
        "        batch = Batch()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            saver, global_step = Model.continue_previous_session(sess, ckpt_file='saver/checkpoint')\n",
        "\n",
        "            \n",
        "            batch_size = argp.batch_size_testing\n",
        "\n",
        "            if argp.dataset == 'training':\n",
        "                x = batch.get_batch(batch_size, dataset_name=argp.dataset, dataset=X)\n",
        "            elif argp.dataset == 'testing':\n",
        "                x = batch.get_batch(batch_size, dataset_name=argp.dataset, dataset=X_test)\n",
        "            else:\n",
        "                sys.exit('Unexpected argument parser value for variable \"dataset\" ')\n",
        "\n",
        "            org, recon = sess.run((self.x, self.reconstruction), feed_dict={self.x: x})\n",
        "\n",
        "            org = np.squeeze(org,axis=3)\n",
        "            recon = np.squeeze(recon,axis=3)\n",
        "            \n",
        "            \n",
        "            for b in range (batch_size):\n",
        "                fig_org, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
        "                plt.subplots_adjust(wspace=1)\n",
        "                if argp.encoding!='GS':\n",
        "                    image_org = axes[0].imshow(org[b],cmap='jet')\n",
        "                    image_recon = axes[1].imshow(recon[b],cmap='jet')\n",
        "                    image_residual = axes[2].imshow(np.abs(org[b]-recon[b]),cmap='jet')\n",
        "                else:\n",
        "                    image_org = axes[0].imshow(org[b],cmap='gray')\n",
        "                    image_recon = axes[1].imshow(recon[b],cmap='gray')\n",
        "                    image_residual = axes[2].imshow(np.abs(org[b]-recon[b]),cmap='gray')\n",
        "                    \n",
        "                # print(np.sum((org[b]-recon[b])**2)/2)\n",
        "                plt.colorbar(image_org, fraction=0.0457, pad=0.04, ax=axes[0])\n",
        "                plt.colorbar(image_recon, fraction=0.0457, pad=0.04, ax=axes[1])\n",
        "                plt.colorbar(image_residual, fraction=0.0457, pad=0.04, ax=axes[2])\n",
        "                \n",
        "                # the colorbar limits can either be changed or left out entirely. In this case the limits get set automatically.\n",
        "                if argp.encoding=='GAF':\n",
        "                    image_org.set_clim(-1, 0.5)\n",
        "                    image_recon.set_clim(-1, 0.5)\n",
        "                    image_residual.set_clim(0,0.4)\n",
        "                    axes[0].set_title('GAF: Original',fontsize=15)\n",
        "                    axes[1].set_title('GAF: Reconstruction',fontsize=15)\n",
        "                    axes[2].set_title('GAF: Residual',fontsize=15)\n",
        "                  \n",
        "                elif argp.encoding=='MTF':\n",
        "                    image_org.set_clim(0, 0.4)\n",
        "                    image_recon.set_clim(0,0.4)\n",
        "                    image_residual.set_clim(0,0.1)\n",
        "                    axes[0].set_title('MTF: Original',fontsize=15)\n",
        "                    axes[1].set_title('MTF: Reconstruction',fontsize=15)\n",
        "                    axes[2].set_title('MTF: Residual',fontsize=15)\n",
        "                  \n",
        "                elif argp.encoding=='RP':\n",
        "                    image_org.set_clim(0, 2)\n",
        "                    image_recon.set_clim(0, 2)\n",
        "                    image_residual.set_clim(0, 1)\n",
        "                    axes[0].set_title('RP: Original',fontsize=15)\n",
        "                    axes[1].set_title('RP: Reconstruction',fontsize=15)\n",
        "                    axes[2].set_title('RP: Residual',fontsize=15)\n",
        "                  \n",
        "                elif argp.encoding=='Spectro':\n",
        "                    image_org.set_clim(0,-120)\n",
        "                    image_recon.set_clim(0,-120)\n",
        "                    image_residual.set_clim(0,40)\n",
        "                    axes[0].set_title('SP: Original',fontsize=15)\n",
        "                    axes[1].set_title('SP: Reconstruction',fontsize=15)\n",
        "                    axes[2].set_title('SP: Residual',fontsize=15)\n",
        "                  \n",
        "                elif argp.encoding=='Scalo':\n",
        "                    image_org.set_clim(np.min(org[b]*1.2),np.max(org[b]*1.2))\n",
        "                    image_recon.set_clim(np.min(org[b]*1.2),np.max(org[b]*1.2))\n",
        "#                   image_residual.set_clim(0, 0.5)\n",
        "                    axes[0].set_title('SC: Original',fontsize=15)\n",
        "                    axes[1].set_title('SC: Reconstruction',fontsize=15)\n",
        "                    axes[2].set_title('SC: Residual',fontsize=15)\n",
        "            \n",
        "                elif argp.encoding=='GS':\n",
        "                    image_org.set_clim(np.min(org[b]*1),np.max(org[b]*1))\n",
        "                    image_recon.set_clim(np.min(org[b]*1),np.max(org[b]*1))\n",
        "                    axes[0].set_title('GS: Original',fontsize=15)\n",
        "                    axes[1].set_title('GS: Reconstruction',fontsize=15)\n",
        "                    axes[2].set_title('GS: Residual',fontsize=15)\n",
        "\n",
        "                plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of images in training set: 201240\n",
            "Encoding: GAF\n",
            "image size: 64 x 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtM_2Ld2iKl2",
        "colab_type": "code",
        "outputId": "26984022-7d6b-4a5d-8cc1-de612abf0db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "conv_autoencoder = ConvolutionalAutoencoder()\n",
        "\n",
        "if argp.mode == 'new_training':\n",
        "\n",
        "    conv_autoencoder.train(batch_size   = argp.batch_size,\n",
        "                           passes       = argp.cycles, \n",
        "                           new_training = True)\n",
        "\n",
        "elif argp.mode == 'continue_training' :\n",
        "\n",
        "    conv_autoencoder.train(batch_size   = argp.batch_size,\n",
        "                           passes       = argp.cycles, \n",
        "                           new_training = False)\n",
        "\n",
        "elif argp.mode == 'testing':\n",
        "\n",
        "    conv_autoencoder.reconstruct()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input (?, 64, 64, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "conv1 (?, 32, 32, 32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "pool1 (?, 16, 16, 32)\n",
            "unfold (?, 8192)\n",
            "dense1 (?, 160)\n",
            "dense2 (?, 8192)\n",
            "fold (?, 16, 16, 32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "unpool (?, 32, 32, 32)\n",
            "reconstruction (?, ?, ?, 1)\n",
            "new_training\n",
            "Evaluating Performance...\n",
            "pass 10, training loss 1340.880859375\n",
            "Evaluating Performance...\n",
            "pass 20, training loss 1229.27880859375\n",
            "Evaluating Performance...\n",
            "pass 30, training loss 827.5814819335938\n",
            "Evaluating Performance...\n",
            "pass 40, training loss 284.9801025390625\n",
            "Evaluating Performance...\n",
            "pass 50, training loss 93.31673431396484\n",
            "Evaluating Performance...\n",
            "pass 60, training loss 63.59898376464844\n",
            "Evaluating Performance...\n",
            "pass 70, training loss 53.13603210449219\n",
            "Evaluating Performance...\n",
            "pass 80, training loss 37.7520866394043\n",
            "Evaluating Performance...\n",
            "pass 90, training loss 37.048641204833984\n",
            "Evaluating Performance...\n",
            "pass 100, training loss 31.25661277770996\n",
            "Evaluating Performance...\n",
            "pass 110, training loss 36.156837463378906\n",
            "Evaluating Performance...\n",
            "pass 120, training loss 37.62742233276367\n",
            "Evaluating Performance...\n",
            "pass 130, training loss 35.18543243408203\n",
            "Evaluating Performance...\n",
            "pass 140, training loss 33.458839416503906\n",
            "Evaluating Performance...\n",
            "pass 150, training loss 34.80875015258789\n",
            "Evaluating Performance...\n",
            "pass 160, training loss 39.6053352355957\n",
            "Evaluating Performance...\n",
            "pass 170, training loss 30.646608352661133\n",
            "Evaluating Performance...\n",
            "pass 180, training loss 35.16843795776367\n",
            "Evaluating Performance...\n",
            "pass 190, training loss 33.833740234375\n",
            "Evaluating Performance...\n",
            "pass 200, training loss 30.331764221191406\n",
            "Evaluating Performance...\n",
            "pass 210, training loss 32.57438278198242\n",
            "Evaluating Performance...\n",
            "pass 220, training loss 29.097972869873047\n",
            "Evaluating Performance...\n",
            "pass 230, training loss 28.6567325592041\n",
            "Evaluating Performance...\n",
            "pass 240, training loss 34.3281135559082\n",
            "Evaluating Performance...\n",
            "pass 250, training loss 30.912607192993164\n",
            "Evaluating Performance...\n",
            "pass 260, training loss 29.379812240600586\n",
            "Evaluating Performance...\n",
            "pass 270, training loss 28.108154296875\n",
            "Evaluating Performance...\n",
            "pass 280, training loss 30.665748596191406\n",
            "Evaluating Performance...\n",
            "pass 290, training loss 32.83831787109375\n",
            "Evaluating Performance...\n",
            "pass 300, training loss 34.032859802246094\n",
            "Evaluating Performance...\n",
            "pass 310, training loss 30.558401107788086\n",
            "Evaluating Performance...\n",
            "pass 320, training loss 28.570890426635742\n",
            "Evaluating Performance...\n",
            "pass 330, training loss 33.638858795166016\n",
            "Evaluating Performance...\n",
            "pass 340, training loss 32.245933532714844\n",
            "Evaluating Performance...\n",
            "pass 350, training loss 29.019399642944336\n",
            "Evaluating Performance...\n",
            "pass 360, training loss 33.4275016784668\n",
            "Evaluating Performance...\n",
            "pass 370, training loss 30.212812423706055\n",
            "Evaluating Performance...\n",
            "pass 380, training loss 34.55031967163086\n",
            "Evaluating Performance...\n",
            "pass 390, training loss 29.6766357421875\n",
            "Evaluating Performance...\n",
            "pass 400, training loss 28.76999282836914\n",
            "Evaluating Performance...\n",
            "pass 410, training loss 27.2781982421875\n",
            "Evaluating Performance...\n",
            "pass 420, training loss 33.675411224365234\n",
            "Evaluating Performance...\n",
            "pass 430, training loss 38.27033233642578\n",
            "Evaluating Performance...\n",
            "pass 440, training loss 35.222557067871094\n",
            "Evaluating Performance...\n",
            "pass 450, training loss 28.604726791381836\n",
            "Evaluating Performance...\n",
            "pass 460, training loss 29.22866439819336\n",
            "Evaluating Performance...\n",
            "pass 470, training loss 26.479433059692383\n",
            "Evaluating Performance...\n",
            "pass 480, training loss 34.099334716796875\n",
            "Evaluating Performance...\n",
            "pass 490, training loss 26.796579360961914\n",
            "Evaluating Performance...\n",
            "pass 500, training loss 35.28028106689453\n",
            "Evaluating Performance...\n",
            "pass 510, training loss 36.04495620727539\n",
            "Evaluating Performance...\n",
            "pass 520, training loss 32.005943298339844\n",
            "Evaluating Performance...\n",
            "pass 530, training loss 31.334152221679688\n",
            "Evaluating Performance...\n",
            "pass 540, training loss 33.02546691894531\n",
            "Evaluating Performance...\n",
            "pass 550, training loss 23.942440032958984\n",
            "Evaluating Performance...\n",
            "pass 560, training loss 31.488073348999023\n",
            "Evaluating Performance...\n",
            "pass 570, training loss 35.9056396484375\n",
            "Evaluating Performance...\n",
            "pass 580, training loss 35.532081604003906\n",
            "Evaluating Performance...\n",
            "pass 590, training loss 30.18653678894043\n",
            "Evaluating Performance...\n",
            "pass 600, training loss 27.414737701416016\n",
            "Evaluating Performance...\n",
            "pass 610, training loss 31.095077514648438\n",
            "Evaluating Performance...\n",
            "pass 620, training loss 30.61025619506836\n",
            "Evaluating Performance...\n",
            "pass 630, training loss 32.93233871459961\n",
            "Evaluating Performance...\n",
            "pass 640, training loss 35.57110595703125\n",
            "Evaluating Performance...\n",
            "pass 650, training loss 35.78914260864258\n",
            "Evaluating Performance...\n",
            "pass 660, training loss 25.67953109741211\n",
            "Evaluating Performance...\n",
            "pass 670, training loss 25.802404403686523\n",
            "Evaluating Performance...\n",
            "pass 680, training loss 30.334014892578125\n",
            "Evaluating Performance...\n",
            "pass 690, training loss 32.361572265625\n",
            "Evaluating Performance...\n",
            "pass 700, training loss 31.59060287475586\n",
            "Evaluating Performance...\n",
            "pass 710, training loss 33.324581146240234\n",
            "Evaluating Performance...\n",
            "pass 720, training loss 30.154523849487305\n",
            "Evaluating Performance...\n",
            "pass 730, training loss 37.68285369873047\n",
            "Evaluating Performance...\n",
            "pass 740, training loss 30.117555618286133\n",
            "Evaluating Performance...\n",
            "pass 750, training loss 32.647361755371094\n",
            "Evaluating Performance...\n",
            "pass 760, training loss 35.15235137939453\n",
            "Evaluating Performance...\n",
            "pass 770, training loss 33.788658142089844\n",
            "Evaluating Performance...\n",
            "pass 780, training loss 27.645771026611328\n",
            "Evaluating Performance...\n",
            "pass 790, training loss 29.568012237548828\n",
            "Evaluating Performance...\n",
            "pass 800, training loss 31.427419662475586\n",
            "Evaluating Performance...\n",
            "pass 810, training loss 32.59757614135742\n",
            "Evaluating Performance...\n",
            "pass 820, training loss 27.4521484375\n",
            "Evaluating Performance...\n",
            "pass 830, training loss 27.646827697753906\n",
            "Evaluating Performance...\n",
            "pass 840, training loss 27.49616813659668\n",
            "Evaluating Performance...\n",
            "pass 850, training loss 29.9705867767334\n",
            "Evaluating Performance...\n",
            "pass 860, training loss 33.36726379394531\n",
            "Evaluating Performance...\n",
            "pass 870, training loss 30.96904182434082\n",
            "Evaluating Performance...\n",
            "pass 880, training loss 27.444217681884766\n",
            "Evaluating Performance...\n",
            "pass 890, training loss 31.915197372436523\n",
            "Evaluating Performance...\n",
            "pass 900, training loss 28.49749755859375\n",
            "Evaluating Performance...\n",
            "pass 910, training loss 33.8494873046875\n",
            "Evaluating Performance...\n",
            "pass 920, training loss 32.75996398925781\n",
            "Evaluating Performance...\n",
            "pass 930, training loss 28.22073745727539\n",
            "Evaluating Performance...\n",
            "pass 940, training loss 29.761152267456055\n",
            "Evaluating Performance...\n",
            "pass 950, training loss 29.8118953704834\n",
            "Evaluating Performance...\n",
            "pass 960, training loss 34.28810119628906\n",
            "Evaluating Performance...\n",
            "pass 970, training loss 29.427135467529297\n",
            "Evaluating Performance...\n",
            "pass 980, training loss 30.03961181640625\n",
            "Evaluating Performance...\n",
            "pass 990, training loss 32.401145935058594\n",
            "Evaluating Performance...\n",
            "pass 1000, training loss 31.86221694946289\n",
            "Evaluating Performance...\n",
            "pass 1010, training loss 34.024986267089844\n",
            "Evaluating Performance...\n",
            "pass 1020, training loss 35.23316955566406\n",
            "Evaluating Performance...\n",
            "pass 1030, training loss 33.347782135009766\n",
            "Evaluating Performance...\n",
            "pass 1040, training loss 28.19219398498535\n",
            "Evaluating Performance...\n",
            "pass 1050, training loss 27.285409927368164\n",
            "Evaluating Performance...\n",
            "pass 1060, training loss 27.46894645690918\n",
            "Evaluating Performance...\n",
            "pass 1070, training loss 32.75135803222656\n",
            "Evaluating Performance...\n",
            "pass 1080, training loss 32.62795639038086\n",
            "Evaluating Performance...\n",
            "pass 1090, training loss 34.19324493408203\n",
            "Evaluating Performance...\n",
            "pass 1100, training loss 32.476768493652344\n",
            "Evaluating Performance...\n",
            "pass 1110, training loss 35.99177551269531\n",
            "Evaluating Performance...\n",
            "pass 1120, training loss 28.129955291748047\n",
            "Evaluating Performance...\n",
            "pass 1130, training loss 28.774539947509766\n",
            "Evaluating Performance...\n",
            "pass 1140, training loss 29.830455780029297\n",
            "Evaluating Performance...\n",
            "pass 1150, training loss 28.545888900756836\n",
            "Evaluating Performance...\n",
            "pass 1160, training loss 30.10879898071289\n",
            "Evaluating Performance...\n",
            "pass 1170, training loss 30.825815200805664\n",
            "Evaluating Performance...\n",
            "pass 1180, training loss 30.06345558166504\n",
            "Evaluating Performance...\n",
            "pass 1190, training loss 33.18098068237305\n",
            "Evaluating Performance...\n",
            "pass 1200, training loss 32.03689193725586\n",
            "Evaluating Performance...\n",
            "pass 1210, training loss 30.00425148010254\n",
            "Evaluating Performance...\n",
            "pass 1220, training loss 27.92070770263672\n",
            "Evaluating Performance...\n",
            "pass 1230, training loss 29.293720245361328\n",
            "Evaluating Performance...\n",
            "pass 1240, training loss 29.913463592529297\n",
            "Evaluating Performance...\n",
            "pass 1250, training loss 36.901153564453125\n",
            "Evaluating Performance...\n",
            "pass 1260, training loss 33.60021209716797\n",
            "Evaluating Performance...\n",
            "pass 1270, training loss 28.34391212463379\n",
            "Evaluating Performance...\n",
            "pass 1280, training loss 33.8660774230957\n",
            "Evaluating Performance...\n",
            "pass 1290, training loss 34.15150451660156\n",
            "Evaluating Performance...\n",
            "pass 1300, training loss 32.84847640991211\n",
            "Evaluating Performance...\n",
            "pass 1310, training loss 28.98678207397461\n",
            "Evaluating Performance...\n",
            "pass 1320, training loss 32.59385299682617\n",
            "Evaluating Performance...\n",
            "pass 1330, training loss 29.295612335205078\n",
            "Evaluating Performance...\n",
            "pass 1340, training loss 30.22233009338379\n",
            "Evaluating Performance...\n",
            "pass 1350, training loss 30.888195037841797\n",
            "Evaluating Performance...\n",
            "pass 1360, training loss 29.91469383239746\n",
            "Evaluating Performance...\n",
            "pass 1370, training loss 28.830448150634766\n",
            "Evaluating Performance...\n",
            "pass 1380, training loss 34.961421966552734\n",
            "Evaluating Performance...\n",
            "pass 1390, training loss 27.331493377685547\n",
            "Evaluating Performance...\n",
            "pass 1400, training loss 29.921457290649414\n",
            "Evaluating Performance...\n",
            "pass 1410, training loss 29.53404998779297\n",
            "Evaluating Performance...\n",
            "pass 1420, training loss 28.244558334350586\n",
            "Evaluating Performance...\n",
            "pass 1430, training loss 31.34749984741211\n",
            "Evaluating Performance...\n",
            "pass 1440, training loss 31.65148162841797\n",
            "Evaluating Performance...\n",
            "pass 1450, training loss 33.69776153564453\n",
            "Evaluating Performance...\n",
            "pass 1460, training loss 28.924468994140625\n",
            "Evaluating Performance...\n",
            "pass 1470, training loss 26.181501388549805\n",
            "Evaluating Performance...\n",
            "pass 1480, training loss 31.70482063293457\n",
            "Evaluating Performance...\n",
            "pass 1490, training loss 32.2677116394043\n",
            "Evaluating Performance...\n",
            "pass 1500, training loss 30.95186996459961\n",
            "Evaluating Performance...\n",
            "pass 1510, training loss 29.254865646362305\n",
            "Evaluating Performance...\n",
            "pass 1520, training loss 31.377159118652344\n",
            "Evaluating Performance...\n",
            "pass 1530, training loss 30.152658462524414\n",
            "Evaluating Performance...\n",
            "pass 1540, training loss 33.650997161865234\n",
            "Evaluating Performance...\n",
            "pass 1550, training loss 27.88065528869629\n",
            "Evaluating Performance...\n",
            "pass 1560, training loss 30.031721115112305\n",
            "Evaluating Performance...\n",
            "pass 1570, training loss 33.056800842285156\n",
            "Evaluating Performance...\n",
            "pass 1580, training loss 27.637104034423828\n",
            "Evaluating Performance...\n",
            "pass 1590, training loss 30.65520668029785\n",
            "Evaluating Performance...\n",
            "pass 1600, training loss 31.294776916503906\n",
            "Evaluating Performance...\n",
            "pass 1610, training loss 28.868133544921875\n",
            "Evaluating Performance...\n",
            "pass 1620, training loss 31.48954963684082\n",
            "Evaluating Performance...\n",
            "pass 1630, training loss 29.271141052246094\n",
            "Evaluating Performance...\n",
            "pass 1640, training loss 29.44866943359375\n",
            "Evaluating Performance...\n",
            "pass 1650, training loss 28.086820602416992\n",
            "Evaluating Performance...\n",
            "pass 1660, training loss 29.767194747924805\n",
            "Evaluating Performance...\n",
            "pass 1670, training loss 34.32570266723633\n",
            "Evaluating Performance...\n",
            "pass 1680, training loss 24.62140655517578\n",
            "Evaluating Performance...\n",
            "pass 1690, training loss 26.532772064208984\n",
            "Evaluating Performance...\n",
            "pass 1700, training loss 31.58184814453125\n",
            "Evaluating Performance...\n",
            "pass 1710, training loss 28.368803024291992\n",
            "Evaluating Performance...\n",
            "pass 1720, training loss 31.854372024536133\n",
            "Evaluating Performance...\n",
            "pass 1730, training loss 36.64251708984375\n",
            "Evaluating Performance...\n",
            "pass 1740, training loss 26.91672134399414\n",
            "Evaluating Performance...\n",
            "pass 1750, training loss 30.0289249420166\n",
            "Evaluating Performance...\n",
            "pass 1760, training loss 29.912267684936523\n",
            "Evaluating Performance...\n",
            "pass 1770, training loss 30.66481590270996\n",
            "Evaluating Performance...\n",
            "pass 1780, training loss 28.441932678222656\n",
            "Evaluating Performance...\n",
            "pass 1790, training loss 30.27964973449707\n",
            "Evaluating Performance...\n",
            "pass 1800, training loss 35.09586715698242\n",
            "Evaluating Performance...\n",
            "pass 1810, training loss 33.98085021972656\n",
            "Evaluating Performance...\n",
            "pass 1820, training loss 30.385873794555664\n",
            "Evaluating Performance...\n",
            "pass 1830, training loss 31.6358642578125\n",
            "Evaluating Performance...\n",
            "pass 1840, training loss 30.69219398498535\n",
            "Evaluating Performance...\n",
            "pass 1850, training loss 29.989471435546875\n",
            "Evaluating Performance...\n",
            "pass 1860, training loss 30.97516441345215\n",
            "Evaluating Performance...\n",
            "pass 1870, training loss 24.77215576171875\n",
            "Evaluating Performance...\n",
            "pass 1880, training loss 26.841388702392578\n",
            "Evaluating Performance...\n",
            "pass 1890, training loss 30.039813995361328\n",
            "Evaluating Performance...\n",
            "pass 1900, training loss 28.959850311279297\n",
            "Evaluating Performance...\n",
            "pass 1910, training loss 27.909208297729492\n",
            "Evaluating Performance...\n",
            "pass 1920, training loss 29.28322982788086\n",
            "Evaluating Performance...\n",
            "pass 1930, training loss 30.683273315429688\n",
            "Evaluating Performance...\n",
            "pass 1940, training loss 30.256399154663086\n",
            "Evaluating Performance...\n",
            "pass 1950, training loss 29.173803329467773\n",
            "Evaluating Performance...\n",
            "pass 1960, training loss 29.136188507080078\n",
            "Evaluating Performance...\n",
            "pass 1970, training loss 30.04416275024414\n",
            "Evaluating Performance...\n",
            "pass 1980, training loss 29.00027084350586\n",
            "Evaluating Performance...\n",
            "pass 1990, training loss 30.863914489746094\n",
            "Evaluating Performance...\n",
            "pass 2000, training loss 36.75408935546875\n",
            "Evaluating Performance...\n",
            "pass 2010, training loss 31.912460327148438\n",
            "Evaluating Performance...\n",
            "pass 2020, training loss 26.371809005737305\n",
            "Evaluating Performance...\n",
            "pass 2030, training loss 30.580305099487305\n",
            "Evaluating Performance...\n",
            "pass 2040, training loss 28.958412170410156\n",
            "Evaluating Performance...\n",
            "pass 2050, training loss 33.02370834350586\n",
            "Evaluating Performance...\n",
            "pass 2060, training loss 29.63686180114746\n",
            "Evaluating Performance...\n",
            "pass 2070, training loss 29.843517303466797\n",
            "Evaluating Performance...\n",
            "pass 2080, training loss 27.600473403930664\n",
            "Evaluating Performance...\n",
            "pass 2090, training loss 30.83441734313965\n",
            "Evaluating Performance...\n",
            "pass 2100, training loss 28.32486343383789\n",
            "Evaluating Performance...\n",
            "pass 2110, training loss 35.22745132446289\n",
            "Evaluating Performance...\n",
            "pass 2120, training loss 31.82729148864746\n",
            "Evaluating Performance...\n",
            "pass 2130, training loss 27.6617431640625\n",
            "Evaluating Performance...\n",
            "pass 2140, training loss 30.99604606628418\n",
            "Evaluating Performance...\n",
            "pass 2150, training loss 36.63168716430664\n",
            "Evaluating Performance...\n",
            "pass 2160, training loss 27.543352127075195\n",
            "Evaluating Performance...\n",
            "pass 2170, training loss 28.38332748413086\n",
            "Evaluating Performance...\n",
            "pass 2180, training loss 29.93805503845215\n",
            "Evaluating Performance...\n",
            "pass 2190, training loss 28.613632202148438\n",
            "Evaluating Performance...\n",
            "pass 2200, training loss 24.501596450805664\n",
            "Evaluating Performance...\n",
            "pass 2210, training loss 30.433258056640625\n",
            "Evaluating Performance...\n",
            "pass 2220, training loss 32.43403625488281\n",
            "Evaluating Performance...\n",
            "pass 2230, training loss 31.56776237487793\n",
            "Evaluating Performance...\n",
            "pass 2240, training loss 31.342952728271484\n",
            "Evaluating Performance...\n",
            "pass 2250, training loss 28.98350715637207\n",
            "Evaluating Performance...\n",
            "pass 2260, training loss 26.595237731933594\n",
            "Evaluating Performance...\n",
            "pass 2270, training loss 28.45916175842285\n",
            "Evaluating Performance...\n",
            "pass 2280, training loss 29.942880630493164\n",
            "Evaluating Performance...\n",
            "pass 2290, training loss 33.66310119628906\n",
            "Evaluating Performance...\n",
            "pass 2300, training loss 27.430753707885742\n",
            "Evaluating Performance...\n",
            "pass 2310, training loss 32.06810760498047\n",
            "Evaluating Performance...\n",
            "pass 2320, training loss 32.77196502685547\n",
            "Evaluating Performance...\n",
            "pass 2330, training loss 27.97616958618164\n",
            "Evaluating Performance...\n",
            "pass 2340, training loss 37.90103530883789\n",
            "Evaluating Performance...\n",
            "pass 2350, training loss 31.837238311767578\n",
            "Evaluating Performance...\n",
            "pass 2360, training loss 28.790817260742188\n",
            "Evaluating Performance...\n",
            "pass 2370, training loss 25.914106369018555\n",
            "Evaluating Performance...\n",
            "pass 2380, training loss 27.22279930114746\n",
            "Evaluating Performance...\n",
            "pass 2390, training loss 28.970991134643555\n",
            "Evaluating Performance...\n",
            "pass 2400, training loss 27.962663650512695\n",
            "Evaluating Performance...\n",
            "pass 2410, training loss 27.856367111206055\n",
            "Evaluating Performance...\n",
            "pass 2420, training loss 28.031476974487305\n",
            "Evaluating Performance...\n",
            "pass 2430, training loss 28.185260772705078\n",
            "Evaluating Performance...\n",
            "pass 2440, training loss 26.29266357421875\n",
            "Evaluating Performance...\n",
            "pass 2450, training loss 27.159210205078125\n",
            "Evaluating Performance...\n",
            "pass 2460, training loss 32.331485748291016\n",
            "Evaluating Performance...\n",
            "pass 2470, training loss 29.342391967773438\n",
            "Evaluating Performance...\n",
            "pass 2480, training loss 28.35742950439453\n",
            "Evaluating Performance...\n",
            "pass 2490, training loss 30.196258544921875\n",
            "Evaluating Performance...\n",
            "pass 2500, training loss 28.794010162353516\n",
            "Evaluating Performance...\n",
            "pass 2510, training loss 27.686668395996094\n",
            "Evaluating Performance...\n",
            "pass 2520, training loss 28.19576644897461\n",
            "Evaluating Performance...\n",
            "pass 2530, training loss 27.95005989074707\n",
            "Evaluating Performance...\n",
            "pass 2540, training loss 33.14414596557617\n",
            "Evaluating Performance...\n",
            "pass 2550, training loss 24.287826538085938\n",
            "Evaluating Performance...\n",
            "pass 2560, training loss 26.57878303527832\n",
            "Evaluating Performance...\n",
            "pass 2570, training loss 24.52617645263672\n",
            "Evaluating Performance...\n",
            "pass 2580, training loss 28.182369232177734\n",
            "Evaluating Performance...\n",
            "pass 2590, training loss 29.795175552368164\n",
            "Evaluating Performance...\n",
            "pass 2600, training loss 31.41636085510254\n",
            "Evaluating Performance...\n",
            "pass 2610, training loss 33.19283676147461\n",
            "Evaluating Performance...\n",
            "pass 2620, training loss 31.100997924804688\n",
            "Evaluating Performance...\n",
            "pass 2630, training loss 28.79029655456543\n",
            "Evaluating Performance...\n",
            "pass 2640, training loss 29.57594871520996\n",
            "Evaluating Performance...\n",
            "pass 2650, training loss 25.06048583984375\n",
            "Evaluating Performance...\n",
            "pass 2660, training loss 29.420475006103516\n",
            "Evaluating Performance...\n",
            "pass 2670, training loss 32.26881408691406\n",
            "Evaluating Performance...\n",
            "pass 2680, training loss 30.237646102905273\n",
            "Evaluating Performance...\n",
            "pass 2690, training loss 28.99679946899414\n",
            "Evaluating Performance...\n",
            "pass 2700, training loss 32.195701599121094\n",
            "Evaluating Performance...\n",
            "pass 2710, training loss 29.378028869628906\n",
            "Evaluating Performance...\n",
            "pass 2720, training loss 25.938941955566406\n",
            "Evaluating Performance...\n",
            "pass 2730, training loss 27.36622428894043\n",
            "Evaluating Performance...\n",
            "pass 2740, training loss 32.36842727661133\n",
            "Evaluating Performance...\n",
            "pass 2750, training loss 28.89419174194336\n",
            "Evaluating Performance...\n",
            "pass 2760, training loss 25.666419982910156\n",
            "Evaluating Performance...\n",
            "pass 2770, training loss 27.475021362304688\n",
            "Evaluating Performance...\n",
            "pass 2780, training loss 26.11626625061035\n",
            "Evaluating Performance...\n",
            "pass 2790, training loss 34.626705169677734\n",
            "Evaluating Performance...\n",
            "pass 2800, training loss 29.023719787597656\n",
            "Evaluating Performance...\n",
            "pass 2810, training loss 27.463275909423828\n",
            "Evaluating Performance...\n",
            "pass 2820, training loss 28.13470458984375\n",
            "Evaluating Performance...\n",
            "pass 2830, training loss 27.259113311767578\n",
            "Evaluating Performance...\n",
            "pass 2840, training loss 26.983470916748047\n",
            "Evaluating Performance...\n",
            "pass 2850, training loss 32.651058197021484\n",
            "Evaluating Performance...\n",
            "pass 2860, training loss 32.54587173461914\n",
            "Evaluating Performance...\n",
            "pass 2870, training loss 21.51878547668457\n",
            "Evaluating Performance...\n",
            "pass 2880, training loss 34.675636291503906\n",
            "Evaluating Performance...\n",
            "pass 2890, training loss 30.431066513061523\n",
            "Evaluating Performance...\n",
            "pass 2900, training loss 28.77321434020996\n",
            "Evaluating Performance...\n",
            "pass 2910, training loss 32.379642486572266\n",
            "Evaluating Performance...\n",
            "pass 2920, training loss 28.29500389099121\n",
            "Evaluating Performance...\n",
            "pass 2930, training loss 25.324203491210938\n",
            "Evaluating Performance...\n",
            "pass 2940, training loss 26.520177841186523\n",
            "Evaluating Performance...\n",
            "pass 2950, training loss 29.785057067871094\n",
            "Evaluating Performance...\n",
            "pass 2960, training loss 28.8046817779541\n",
            "Evaluating Performance...\n",
            "pass 2970, training loss 26.702552795410156\n",
            "Evaluating Performance...\n",
            "pass 2980, training loss 29.67887306213379\n",
            "Evaluating Performance...\n",
            "pass 2990, training loss 27.147314071655273\n",
            "Evaluating Performance...\n",
            "pass 3000, training loss 27.37915802001953\n",
            "Evaluating Performance...\n",
            "pass 3010, training loss 28.657140731811523\n",
            "Evaluating Performance...\n",
            "pass 3020, training loss 28.34160041809082\n",
            "Evaluating Performance...\n",
            "pass 3030, training loss 28.75533676147461\n",
            "Evaluating Performance...\n",
            "pass 3040, training loss 32.38817596435547\n",
            "Evaluating Performance...\n",
            "pass 3050, training loss 33.514129638671875\n",
            "Evaluating Performance...\n",
            "pass 3060, training loss 26.39674186706543\n",
            "Evaluating Performance...\n",
            "pass 3070, training loss 25.190406799316406\n",
            "Evaluating Performance...\n",
            "pass 3080, training loss 28.55905532836914\n",
            "Evaluating Performance...\n",
            "pass 3090, training loss 26.481956481933594\n",
            "Evaluating Performance...\n",
            "pass 3100, training loss 28.085285186767578\n",
            "Evaluating Performance...\n",
            "pass 3110, training loss 28.880977630615234\n",
            "Evaluating Performance...\n",
            "pass 3120, training loss 28.38128089904785\n",
            "Evaluating Performance...\n",
            "pass 3130, training loss 28.355688095092773\n",
            "Evaluating Performance...\n",
            "pass 3140, training loss 27.249006271362305\n",
            "Evaluating Performance...\n",
            "pass 3150, training loss 37.86504364013672\n",
            "Evaluating Performance...\n",
            "pass 3160, training loss 25.531986236572266\n",
            "Evaluating Performance...\n",
            "pass 3170, training loss 24.74551010131836\n",
            "Evaluating Performance...\n",
            "pass 3180, training loss 23.894330978393555\n",
            "Evaluating Performance...\n",
            "pass 3190, training loss 26.954833984375\n",
            "Evaluating Performance...\n",
            "pass 3200, training loss 26.92151641845703\n",
            "Evaluating Performance...\n",
            "pass 3210, training loss 26.376251220703125\n",
            "Evaluating Performance...\n",
            "pass 3220, training loss 25.538171768188477\n",
            "Evaluating Performance...\n",
            "pass 3230, training loss 29.153627395629883\n",
            "Evaluating Performance...\n",
            "pass 3240, training loss 24.79679298400879\n",
            "Evaluating Performance...\n",
            "pass 3250, training loss 27.991056442260742\n",
            "Evaluating Performance...\n",
            "pass 3260, training loss 28.18441390991211\n",
            "Evaluating Performance...\n",
            "pass 3270, training loss 34.73224639892578\n",
            "Evaluating Performance...\n",
            "pass 3280, training loss 25.67569351196289\n",
            "Evaluating Performance...\n",
            "pass 3290, training loss 25.48849105834961\n",
            "Evaluating Performance...\n",
            "pass 3300, training loss 27.73876190185547\n",
            "Evaluating Performance...\n",
            "pass 3310, training loss 28.896198272705078\n",
            "Evaluating Performance...\n",
            "pass 3320, training loss 25.21384048461914\n",
            "Evaluating Performance...\n",
            "pass 3330, training loss 31.631378173828125\n",
            "Evaluating Performance...\n",
            "pass 3340, training loss 30.0443115234375\n",
            "Evaluating Performance...\n",
            "pass 3350, training loss 26.0683536529541\n",
            "Evaluating Performance...\n",
            "pass 3360, training loss 29.24626922607422\n",
            "Evaluating Performance...\n",
            "pass 3370, training loss 28.92041778564453\n",
            "Evaluating Performance...\n",
            "pass 3380, training loss 29.238195419311523\n",
            "Evaluating Performance...\n",
            "pass 3390, training loss 30.26425552368164\n",
            "Evaluating Performance...\n",
            "pass 3400, training loss 30.783653259277344\n",
            "Evaluating Performance...\n",
            "pass 3410, training loss 25.513290405273438\n",
            "Evaluating Performance...\n",
            "pass 3420, training loss 29.109586715698242\n",
            "Evaluating Performance...\n",
            "pass 3430, training loss 30.228641510009766\n",
            "Evaluating Performance...\n",
            "pass 3440, training loss 35.42594909667969\n",
            "Evaluating Performance...\n",
            "pass 3450, training loss 30.003652572631836\n",
            "Evaluating Performance...\n",
            "pass 3460, training loss 28.80276870727539\n",
            "Evaluating Performance...\n",
            "pass 3470, training loss 30.004140853881836\n",
            "Evaluating Performance...\n",
            "pass 3480, training loss 28.484020233154297\n",
            "Evaluating Performance...\n",
            "pass 3490, training loss 30.987794876098633\n",
            "Evaluating Performance...\n",
            "pass 3500, training loss 27.443998336791992\n",
            "Evaluating Performance...\n",
            "pass 3510, training loss 29.540212631225586\n",
            "Evaluating Performance...\n",
            "pass 3520, training loss 33.00977325439453\n",
            "Evaluating Performance...\n",
            "pass 3530, training loss 27.942289352416992\n",
            "Evaluating Performance...\n",
            "pass 3540, training loss 30.00105094909668\n",
            "Evaluating Performance...\n",
            "pass 3550, training loss 34.49860763549805\n",
            "Evaluating Performance...\n",
            "pass 3560, training loss 25.454654693603516\n",
            "Evaluating Performance...\n",
            "pass 3570, training loss 27.23792266845703\n",
            "Evaluating Performance...\n",
            "pass 3580, training loss 24.87261199951172\n",
            "Evaluating Performance...\n",
            "pass 3590, training loss 28.49350357055664\n",
            "Evaluating Performance...\n",
            "pass 3600, training loss 26.83762550354004\n",
            "Evaluating Performance...\n",
            "pass 3610, training loss 26.639232635498047\n",
            "Evaluating Performance...\n",
            "pass 3620, training loss 31.305700302124023\n",
            "Evaluating Performance...\n",
            "pass 3630, training loss 24.606658935546875\n",
            "Evaluating Performance...\n",
            "pass 3640, training loss 24.367189407348633\n",
            "Evaluating Performance...\n",
            "pass 3650, training loss 28.323923110961914\n",
            "Evaluating Performance...\n",
            "pass 3660, training loss 28.919130325317383\n",
            "Evaluating Performance...\n",
            "pass 3670, training loss 26.522172927856445\n",
            "Evaluating Performance...\n",
            "pass 3680, training loss 25.62236785888672\n",
            "Evaluating Performance...\n",
            "pass 3690, training loss 27.494375228881836\n",
            "Evaluating Performance...\n",
            "pass 3700, training loss 30.61197853088379\n",
            "Evaluating Performance...\n",
            "pass 3710, training loss 29.624576568603516\n",
            "Evaluating Performance...\n",
            "pass 3720, training loss 25.36507797241211\n",
            "Evaluating Performance...\n",
            "pass 3730, training loss 32.95085525512695\n",
            "Evaluating Performance...\n",
            "pass 3740, training loss 24.244707107543945\n",
            "Evaluating Performance...\n",
            "pass 3750, training loss 24.31842803955078\n",
            "Evaluating Performance...\n",
            "pass 3760, training loss 29.41582489013672\n",
            "Evaluating Performance...\n",
            "pass 3770, training loss 25.027658462524414\n",
            "Evaluating Performance...\n",
            "pass 3780, training loss 30.850597381591797\n",
            "Evaluating Performance...\n",
            "pass 3790, training loss 29.475858688354492\n",
            "Evaluating Performance...\n",
            "pass 3800, training loss 25.38787841796875\n",
            "Evaluating Performance...\n",
            "pass 3810, training loss 25.664308547973633\n",
            "Evaluating Performance...\n",
            "pass 3820, training loss 30.780582427978516\n",
            "Evaluating Performance...\n",
            "pass 3830, training loss 27.10515594482422\n",
            "Evaluating Performance...\n",
            "pass 3840, training loss 23.806434631347656\n",
            "Evaluating Performance...\n",
            "pass 3850, training loss 26.687850952148438\n",
            "Evaluating Performance...\n",
            "pass 3860, training loss 28.35109519958496\n",
            "Evaluating Performance...\n",
            "pass 3870, training loss 28.99502944946289\n",
            "Evaluating Performance...\n",
            "pass 3880, training loss 26.026874542236328\n",
            "Evaluating Performance...\n",
            "pass 3890, training loss 26.267396926879883\n",
            "Evaluating Performance...\n",
            "pass 3900, training loss 24.18544578552246\n",
            "Evaluating Performance...\n",
            "pass 3910, training loss 24.71940803527832\n",
            "Evaluating Performance...\n",
            "pass 3920, training loss 24.435243606567383\n",
            "Evaluating Performance...\n",
            "pass 3930, training loss 28.106159210205078\n",
            "Evaluating Performance...\n",
            "pass 3940, training loss 25.514095306396484\n",
            "Evaluating Performance...\n",
            "pass 3950, training loss 27.302547454833984\n",
            "Evaluating Performance...\n",
            "pass 3960, training loss 25.300067901611328\n",
            "Evaluating Performance...\n",
            "pass 3970, training loss 28.89527702331543\n",
            "Evaluating Performance...\n",
            "pass 3980, training loss 29.0871524810791\n",
            "Evaluating Performance...\n",
            "pass 3990, training loss 30.36311912536621\n",
            "Evaluating Performance...\n",
            "pass 4000, training loss 24.94028663635254\n",
            "Evaluating Performance...\n",
            "pass 4010, training loss 27.68596649169922\n",
            "Evaluating Performance...\n",
            "pass 4020, training loss 29.607484817504883\n",
            "Evaluating Performance...\n",
            "pass 4030, training loss 27.09956932067871\n",
            "Evaluating Performance...\n",
            "pass 4040, training loss 25.217987060546875\n",
            "Evaluating Performance...\n",
            "pass 4050, training loss 25.252635955810547\n",
            "Evaluating Performance...\n",
            "pass 4060, training loss 24.94032096862793\n",
            "Evaluating Performance...\n",
            "pass 4070, training loss 24.379728317260742\n",
            "Evaluating Performance...\n",
            "pass 4080, training loss 23.33189582824707\n",
            "Evaluating Performance...\n",
            "pass 4090, training loss 27.304183959960938\n",
            "Evaluating Performance...\n",
            "pass 4100, training loss 27.32876968383789\n",
            "Evaluating Performance...\n",
            "pass 4110, training loss 25.116422653198242\n",
            "Evaluating Performance...\n",
            "pass 4120, training loss 26.867034912109375\n",
            "Evaluating Performance...\n",
            "pass 4130, training loss 27.313798904418945\n",
            "Evaluating Performance...\n",
            "pass 4140, training loss 28.51131820678711\n",
            "Evaluating Performance...\n",
            "pass 4150, training loss 33.70893096923828\n",
            "Evaluating Performance...\n",
            "pass 4160, training loss 21.72343635559082\n",
            "Evaluating Performance...\n",
            "pass 4170, training loss 29.675180435180664\n",
            "Evaluating Performance...\n",
            "pass 4180, training loss 25.708419799804688\n",
            "Evaluating Performance...\n",
            "pass 4190, training loss 23.3035831451416\n",
            "Evaluating Performance...\n",
            "pass 4200, training loss 28.37479591369629\n",
            "Evaluating Performance...\n",
            "pass 4210, training loss 22.91253089904785\n",
            "Evaluating Performance...\n",
            "pass 4220, training loss 35.49385452270508\n",
            "Evaluating Performance...\n",
            "pass 4230, training loss 23.138227462768555\n",
            "Evaluating Performance...\n",
            "pass 4240, training loss 24.78615379333496\n",
            "Evaluating Performance...\n",
            "pass 4250, training loss 28.639389038085938\n",
            "Evaluating Performance...\n",
            "pass 4260, training loss 24.13727569580078\n",
            "Evaluating Performance...\n",
            "pass 4270, training loss 24.31759262084961\n",
            "Evaluating Performance...\n",
            "pass 4280, training loss 27.24679946899414\n",
            "Evaluating Performance...\n",
            "pass 4290, training loss 27.523311614990234\n",
            "Evaluating Performance...\n",
            "pass 4300, training loss 26.92254066467285\n",
            "Evaluating Performance...\n",
            "pass 4310, training loss 26.138017654418945\n",
            "Evaluating Performance...\n",
            "pass 4320, training loss 24.58867073059082\n",
            "Evaluating Performance...\n",
            "pass 4330, training loss 27.626876831054688\n",
            "Evaluating Performance...\n",
            "pass 4340, training loss 23.320446014404297\n",
            "Evaluating Performance...\n",
            "pass 4350, training loss 23.35588264465332\n",
            "Evaluating Performance...\n",
            "pass 4360, training loss 24.26813507080078\n",
            "Evaluating Performance...\n",
            "pass 4370, training loss 31.085105895996094\n",
            "Evaluating Performance...\n",
            "pass 4380, training loss 23.13973045349121\n",
            "Evaluating Performance...\n",
            "pass 4390, training loss 31.4869384765625\n",
            "Evaluating Performance...\n",
            "pass 4400, training loss 25.006296157836914\n",
            "Evaluating Performance...\n",
            "pass 4410, training loss 26.18729019165039\n",
            "Evaluating Performance...\n",
            "pass 4420, training loss 25.123165130615234\n",
            "Evaluating Performance...\n",
            "pass 4430, training loss 25.3713321685791\n",
            "Evaluating Performance...\n",
            "pass 4440, training loss 23.09133529663086\n",
            "Evaluating Performance...\n",
            "pass 4450, training loss 27.93875503540039\n",
            "Evaluating Performance...\n",
            "pass 4460, training loss 24.58851432800293\n",
            "Evaluating Performance...\n",
            "pass 4470, training loss 22.831478118896484\n",
            "Evaluating Performance...\n",
            "pass 4480, training loss 24.03339958190918\n",
            "Evaluating Performance...\n",
            "pass 4490, training loss 28.59751319885254\n",
            "Evaluating Performance...\n",
            "pass 4500, training loss 25.433605194091797\n",
            "Evaluating Performance...\n",
            "pass 4510, training loss 24.476831436157227\n",
            "Evaluating Performance...\n",
            "pass 4520, training loss 27.231719970703125\n",
            "Evaluating Performance...\n",
            "pass 4530, training loss 26.776229858398438\n",
            "Evaluating Performance...\n",
            "pass 4540, training loss 21.43143081665039\n",
            "Evaluating Performance...\n",
            "pass 4550, training loss 25.81321144104004\n",
            "Evaluating Performance...\n",
            "pass 4560, training loss 22.794879913330078\n",
            "Evaluating Performance...\n",
            "pass 4570, training loss 23.912250518798828\n",
            "Evaluating Performance...\n",
            "pass 4580, training loss 23.344682693481445\n",
            "Evaluating Performance...\n",
            "pass 4590, training loss 23.577880859375\n",
            "Evaluating Performance...\n",
            "pass 4600, training loss 28.670028686523438\n",
            "Evaluating Performance...\n",
            "pass 4610, training loss 24.05540657043457\n",
            "Evaluating Performance...\n",
            "pass 4620, training loss 26.69076156616211\n",
            "Evaluating Performance...\n",
            "pass 4630, training loss 28.533552169799805\n",
            "Evaluating Performance...\n",
            "pass 4640, training loss 30.34916877746582\n",
            "Evaluating Performance...\n",
            "pass 4650, training loss 24.925539016723633\n",
            "Evaluating Performance...\n",
            "pass 4660, training loss 30.92192268371582\n",
            "Evaluating Performance...\n",
            "pass 4670, training loss 28.62742042541504\n",
            "Evaluating Performance...\n",
            "pass 4680, training loss 30.65911102294922\n",
            "Evaluating Performance...\n",
            "pass 4690, training loss 23.379117965698242\n",
            "Evaluating Performance...\n",
            "pass 4700, training loss 25.80302619934082\n",
            "Evaluating Performance...\n",
            "pass 4710, training loss 23.687381744384766\n",
            "Evaluating Performance...\n",
            "pass 4720, training loss 26.74024772644043\n",
            "Evaluating Performance...\n",
            "pass 4730, training loss 26.077056884765625\n",
            "Evaluating Performance...\n",
            "pass 4740, training loss 26.124412536621094\n",
            "Evaluating Performance...\n",
            "pass 4750, training loss 23.03890609741211\n",
            "Evaluating Performance...\n",
            "pass 4760, training loss 23.617387771606445\n",
            "Evaluating Performance...\n",
            "pass 4770, training loss 23.075857162475586\n",
            "Evaluating Performance...\n",
            "pass 4780, training loss 22.885421752929688\n",
            "Evaluating Performance...\n",
            "pass 4790, training loss 25.29669952392578\n",
            "Evaluating Performance...\n",
            "pass 4800, training loss 28.56009864807129\n",
            "Evaluating Performance...\n",
            "pass 4810, training loss 22.270442962646484\n",
            "Evaluating Performance...\n",
            "pass 4820, training loss 23.069623947143555\n",
            "Evaluating Performance...\n",
            "pass 4830, training loss 23.487804412841797\n",
            "Evaluating Performance...\n",
            "pass 4840, training loss 24.733327865600586\n",
            "Evaluating Performance...\n",
            "pass 4850, training loss 28.277801513671875\n",
            "Evaluating Performance...\n",
            "pass 4860, training loss 26.09493637084961\n",
            "Evaluating Performance...\n",
            "pass 4870, training loss 20.94075584411621\n",
            "Evaluating Performance...\n",
            "pass 4880, training loss 26.43588638305664\n",
            "Evaluating Performance...\n",
            "pass 4890, training loss 23.78127098083496\n",
            "Evaluating Performance...\n",
            "pass 4900, training loss 20.770009994506836\n",
            "Evaluating Performance...\n",
            "pass 4910, training loss 32.1369743347168\n",
            "Evaluating Performance...\n",
            "pass 4920, training loss 23.19723892211914\n",
            "Evaluating Performance...\n",
            "pass 4930, training loss 27.885683059692383\n",
            "Evaluating Performance...\n",
            "pass 4940, training loss 27.236480712890625\n",
            "Evaluating Performance...\n",
            "pass 4950, training loss 27.139780044555664\n",
            "Evaluating Performance...\n",
            "pass 4960, training loss 22.761327743530273\n",
            "Evaluating Performance...\n",
            "pass 4970, training loss 24.526607513427734\n",
            "Evaluating Performance...\n",
            "pass 4980, training loss 21.564544677734375\n",
            "Evaluating Performance...\n",
            "pass 4990, training loss 22.490995407104492\n",
            "Evaluating Performance...\n",
            "pass 5000, training loss 23.42329978942871\n",
            "Evaluating Performance...\n",
            "pass 5010, training loss 23.987199783325195\n",
            "Evaluating Performance...\n",
            "pass 5020, training loss 27.19167709350586\n",
            "Evaluating Performance...\n",
            "pass 5030, training loss 22.23472023010254\n",
            "Evaluating Performance...\n",
            "pass 5040, training loss 22.70401954650879\n",
            "Evaluating Performance...\n",
            "pass 5050, training loss 28.4132022857666\n",
            "Evaluating Performance...\n",
            "pass 5060, training loss 27.26129913330078\n",
            "Evaluating Performance...\n",
            "pass 5070, training loss 21.816640853881836\n",
            "Evaluating Performance...\n",
            "pass 5080, training loss 27.864444732666016\n",
            "Evaluating Performance...\n",
            "pass 5090, training loss 24.316362380981445\n",
            "Evaluating Performance...\n",
            "pass 5100, training loss 23.7714786529541\n",
            "Evaluating Performance...\n",
            "pass 5110, training loss 23.370302200317383\n",
            "Evaluating Performance...\n",
            "pass 5120, training loss 24.43891143798828\n",
            "Evaluating Performance...\n",
            "pass 5130, training loss 22.83195686340332\n",
            "Evaluating Performance...\n",
            "pass 5140, training loss 23.038387298583984\n",
            "Evaluating Performance...\n",
            "pass 5150, training loss 25.93991470336914\n",
            "Evaluating Performance...\n",
            "pass 5160, training loss 27.99654197692871\n",
            "Evaluating Performance...\n",
            "pass 5170, training loss 25.970277786254883\n",
            "Evaluating Performance...\n",
            "pass 5180, training loss 25.199541091918945\n",
            "Evaluating Performance...\n",
            "pass 5190, training loss 23.661006927490234\n",
            "Evaluating Performance...\n",
            "pass 5200, training loss 26.24970245361328\n",
            "Evaluating Performance...\n",
            "pass 5210, training loss 23.38298797607422\n",
            "Evaluating Performance...\n",
            "pass 5220, training loss 21.34126091003418\n",
            "Evaluating Performance...\n",
            "pass 5230, training loss 19.831539154052734\n",
            "Evaluating Performance...\n",
            "pass 5240, training loss 25.262292861938477\n",
            "Evaluating Performance...\n",
            "pass 5250, training loss 24.518117904663086\n",
            "Evaluating Performance...\n",
            "pass 5260, training loss 21.104772567749023\n",
            "Evaluating Performance...\n",
            "pass 5270, training loss 27.078413009643555\n",
            "Evaluating Performance...\n",
            "pass 5280, training loss 22.126527786254883\n",
            "Evaluating Performance...\n",
            "pass 5290, training loss 29.570323944091797\n",
            "Evaluating Performance...\n",
            "pass 5300, training loss 22.84744644165039\n",
            "Evaluating Performance...\n",
            "pass 5310, training loss 20.861976623535156\n",
            "Evaluating Performance...\n",
            "pass 5320, training loss 24.72296905517578\n",
            "Evaluating Performance...\n",
            "pass 5330, training loss 20.638160705566406\n",
            "Evaluating Performance...\n",
            "pass 5340, training loss 25.159732818603516\n",
            "Evaluating Performance...\n",
            "pass 5350, training loss 23.46398162841797\n",
            "Evaluating Performance...\n",
            "pass 5360, training loss 28.937807083129883\n",
            "Evaluating Performance...\n",
            "pass 5370, training loss 21.31854248046875\n",
            "Evaluating Performance...\n",
            "pass 5380, training loss 23.56328582763672\n",
            "Evaluating Performance...\n",
            "pass 5390, training loss 25.47745704650879\n",
            "Evaluating Performance...\n",
            "pass 5400, training loss 23.37092399597168\n",
            "Evaluating Performance...\n",
            "pass 5410, training loss 23.44072723388672\n",
            "Evaluating Performance...\n",
            "pass 5420, training loss 22.022974014282227\n",
            "Evaluating Performance...\n",
            "pass 5430, training loss 29.531715393066406\n",
            "Evaluating Performance...\n",
            "pass 5440, training loss 24.5272216796875\n",
            "Evaluating Performance...\n",
            "pass 5450, training loss 24.777597427368164\n",
            "Evaluating Performance...\n",
            "pass 5460, training loss 21.93486785888672\n",
            "Evaluating Performance...\n",
            "pass 5470, training loss 23.41474723815918\n",
            "Evaluating Performance...\n",
            "pass 5480, training loss 27.05040168762207\n",
            "Evaluating Performance...\n",
            "pass 5490, training loss 19.530529022216797\n",
            "Evaluating Performance...\n",
            "pass 5500, training loss 21.079092025756836\n",
            "Evaluating Performance...\n",
            "pass 5510, training loss 25.984785079956055\n",
            "Evaluating Performance...\n",
            "pass 5520, training loss 22.17050552368164\n",
            "Evaluating Performance...\n",
            "pass 5530, training loss 24.759403228759766\n",
            "Evaluating Performance...\n",
            "pass 5540, training loss 23.56665802001953\n",
            "Evaluating Performance...\n",
            "pass 5550, training loss 26.17405891418457\n",
            "Evaluating Performance...\n",
            "pass 5560, training loss 19.887493133544922\n",
            "Evaluating Performance...\n",
            "pass 5570, training loss 23.507625579833984\n",
            "Evaluating Performance...\n",
            "pass 5580, training loss 23.695724487304688\n",
            "Evaluating Performance...\n",
            "pass 5590, training loss 24.74778175354004\n",
            "Evaluating Performance...\n",
            "pass 5600, training loss 25.349777221679688\n",
            "Evaluating Performance...\n",
            "pass 5610, training loss 21.132150650024414\n",
            "Evaluating Performance...\n",
            "pass 5620, training loss 24.549190521240234\n",
            "Evaluating Performance...\n",
            "pass 5630, training loss 25.846113204956055\n",
            "Evaluating Performance...\n",
            "pass 5640, training loss 22.205080032348633\n",
            "Evaluating Performance...\n",
            "pass 5650, training loss 24.638383865356445\n",
            "Evaluating Performance...\n",
            "pass 5660, training loss 24.951366424560547\n",
            "Evaluating Performance...\n",
            "pass 5670, training loss 23.598066329956055\n",
            "Evaluating Performance...\n",
            "pass 5680, training loss 23.079532623291016\n",
            "Evaluating Performance...\n",
            "pass 5690, training loss 25.58902931213379\n",
            "Evaluating Performance...\n",
            "pass 5700, training loss 24.17816162109375\n",
            "Evaluating Performance...\n",
            "pass 5710, training loss 26.492687225341797\n",
            "Evaluating Performance...\n",
            "pass 5720, training loss 26.841928482055664\n",
            "Evaluating Performance...\n",
            "pass 5730, training loss 25.13311004638672\n",
            "Evaluating Performance...\n",
            "pass 5740, training loss 21.325275421142578\n",
            "Evaluating Performance...\n",
            "pass 5750, training loss 27.35834503173828\n",
            "Evaluating Performance...\n",
            "pass 5760, training loss 21.45026397705078\n",
            "Evaluating Performance...\n",
            "pass 5770, training loss 26.171016693115234\n",
            "Evaluating Performance...\n",
            "pass 5780, training loss 25.17681121826172\n",
            "Evaluating Performance...\n",
            "pass 5790, training loss 20.1464900970459\n",
            "Evaluating Performance...\n",
            "pass 5800, training loss 25.017526626586914\n",
            "Evaluating Performance...\n",
            "pass 5810, training loss 25.262975692749023\n",
            "Evaluating Performance...\n",
            "pass 5820, training loss 23.31351089477539\n",
            "Evaluating Performance...\n",
            "pass 5830, training loss 22.649682998657227\n",
            "Evaluating Performance...\n",
            "pass 5840, training loss 23.985509872436523\n",
            "Evaluating Performance...\n",
            "pass 5850, training loss 25.130739212036133\n",
            "Evaluating Performance...\n",
            "pass 5860, training loss 20.53384780883789\n",
            "Evaluating Performance...\n",
            "pass 5870, training loss 24.66297149658203\n",
            "Evaluating Performance...\n",
            "pass 5880, training loss 25.665685653686523\n",
            "Evaluating Performance...\n",
            "pass 5890, training loss 24.228553771972656\n",
            "Evaluating Performance...\n",
            "pass 5900, training loss 21.935651779174805\n",
            "Evaluating Performance...\n",
            "pass 5910, training loss 20.25383949279785\n",
            "Evaluating Performance...\n",
            "pass 5920, training loss 22.92426300048828\n",
            "Evaluating Performance...\n",
            "pass 5930, training loss 20.809024810791016\n",
            "Evaluating Performance...\n",
            "pass 5940, training loss 26.365901947021484\n",
            "Evaluating Performance...\n",
            "pass 5950, training loss 28.619346618652344\n",
            "Evaluating Performance...\n",
            "pass 5960, training loss 23.23953628540039\n",
            "Evaluating Performance...\n",
            "pass 5970, training loss 20.679819107055664\n",
            "Evaluating Performance...\n",
            "pass 5980, training loss 25.873903274536133\n",
            "Evaluating Performance...\n",
            "pass 5990, training loss 21.69273567199707\n",
            "Evaluating Performance...\n",
            "pass 6000, training loss 21.894426345825195\n",
            "Evaluating Performance...\n",
            "pass 6010, training loss 21.686264038085938\n",
            "Evaluating Performance...\n",
            "pass 6020, training loss 24.723493576049805\n",
            "Evaluating Performance...\n",
            "pass 6030, training loss 23.674177169799805\n",
            "Evaluating Performance...\n",
            "pass 6040, training loss 20.43486213684082\n",
            "Evaluating Performance...\n",
            "pass 6050, training loss 24.292476654052734\n",
            "Evaluating Performance...\n",
            "pass 6060, training loss 25.518510818481445\n",
            "Evaluating Performance...\n",
            "pass 6070, training loss 25.86785125732422\n",
            "Evaluating Performance...\n",
            "pass 6080, training loss 23.43210792541504\n",
            "Evaluating Performance...\n",
            "pass 6090, training loss 17.893648147583008\n",
            "Evaluating Performance...\n",
            "pass 6100, training loss 24.77322769165039\n",
            "Evaluating Performance...\n",
            "pass 6110, training loss 22.149145126342773\n",
            "Evaluating Performance...\n",
            "pass 6120, training loss 31.232770919799805\n",
            "Evaluating Performance...\n",
            "pass 6130, training loss 21.369728088378906\n",
            "Evaluating Performance...\n",
            "pass 6140, training loss 22.539419174194336\n",
            "Evaluating Performance...\n",
            "pass 6150, training loss 20.403568267822266\n",
            "Evaluating Performance...\n",
            "pass 6160, training loss 20.41047477722168\n",
            "Evaluating Performance...\n",
            "pass 6170, training loss 22.26756477355957\n",
            "Evaluating Performance...\n",
            "pass 6180, training loss 19.457265853881836\n",
            "Evaluating Performance...\n",
            "pass 6190, training loss 20.441179275512695\n",
            "Evaluating Performance...\n",
            "pass 6200, training loss 21.00845718383789\n",
            "Evaluating Performance...\n",
            "pass 6210, training loss 21.64765167236328\n",
            "Evaluating Performance...\n",
            "pass 6220, training loss 22.137264251708984\n",
            "Evaluating Performance...\n",
            "pass 6230, training loss 20.2922306060791\n",
            "Evaluating Performance...\n",
            "pass 6240, training loss 25.740434646606445\n",
            "Evaluating Performance...\n",
            "pass 6250, training loss 20.90208625793457\n",
            "Evaluating Performance...\n",
            "pass 6260, training loss 18.672910690307617\n",
            "Evaluating Performance...\n",
            "pass 6270, training loss 25.78224754333496\n",
            "Evaluating Performance...\n",
            "pass 6280, training loss 22.75225257873535\n",
            "Evaluating Performance...\n",
            "pass 6290, training loss 23.1989688873291\n",
            "Evaluating Performance...\n",
            "pass 6300, training loss 16.81597328186035\n",
            "Evaluating Performance...\n",
            "pass 6310, training loss 21.475648880004883\n",
            "Evaluating Performance...\n",
            "pass 6320, training loss 24.542171478271484\n",
            "Evaluating Performance...\n",
            "pass 6330, training loss 23.864904403686523\n",
            "Evaluating Performance...\n",
            "pass 6340, training loss 21.14906120300293\n",
            "Evaluating Performance...\n",
            "pass 6350, training loss 22.87187957763672\n",
            "Evaluating Performance...\n",
            "pass 6360, training loss 24.28179168701172\n",
            "Evaluating Performance...\n",
            "pass 6370, training loss 20.90502166748047\n",
            "Evaluating Performance...\n",
            "pass 6380, training loss 19.891077041625977\n",
            "Evaluating Performance...\n",
            "pass 6390, training loss 19.36846351623535\n",
            "Evaluating Performance...\n",
            "pass 6400, training loss 23.892908096313477\n",
            "Evaluating Performance...\n",
            "pass 6410, training loss 19.408388137817383\n",
            "Evaluating Performance...\n",
            "pass 6420, training loss 27.357839584350586\n",
            "Evaluating Performance...\n",
            "pass 6430, training loss 21.393295288085938\n",
            "Evaluating Performance...\n",
            "pass 6440, training loss 22.679967880249023\n",
            "Evaluating Performance...\n",
            "pass 6450, training loss 18.04891586303711\n",
            "Evaluating Performance...\n",
            "pass 6460, training loss 19.292654037475586\n",
            "Evaluating Performance...\n",
            "pass 6470, training loss 17.170454025268555\n",
            "Evaluating Performance...\n",
            "pass 6480, training loss 22.927114486694336\n",
            "Evaluating Performance...\n",
            "pass 6490, training loss 21.94339370727539\n",
            "Evaluating Performance...\n",
            "pass 6500, training loss 16.93202018737793\n",
            "Evaluating Performance...\n",
            "pass 6510, training loss 24.075517654418945\n",
            "Evaluating Performance...\n",
            "pass 6520, training loss 21.619056701660156\n",
            "Evaluating Performance...\n",
            "pass 6530, training loss 24.283161163330078\n",
            "Evaluating Performance...\n",
            "pass 6540, training loss 17.342863082885742\n",
            "Evaluating Performance...\n",
            "pass 6550, training loss 22.48976707458496\n",
            "Evaluating Performance...\n",
            "pass 6560, training loss 22.56035041809082\n",
            "Evaluating Performance...\n",
            "pass 6570, training loss 19.540645599365234\n",
            "Evaluating Performance...\n",
            "pass 6580, training loss 21.17453384399414\n",
            "Evaluating Performance...\n",
            "pass 6590, training loss 18.734695434570312\n",
            "Evaluating Performance...\n",
            "pass 6600, training loss 20.427080154418945\n",
            "Evaluating Performance...\n",
            "pass 6610, training loss 20.074819564819336\n",
            "Evaluating Performance...\n",
            "pass 6620, training loss 18.065126419067383\n",
            "Evaluating Performance...\n",
            "pass 6630, training loss 21.88174057006836\n",
            "Evaluating Performance...\n",
            "pass 6640, training loss 21.593969345092773\n",
            "Evaluating Performance...\n",
            "pass 6650, training loss 18.53076934814453\n",
            "Evaluating Performance...\n",
            "pass 6660, training loss 20.908954620361328\n",
            "Evaluating Performance...\n",
            "pass 6670, training loss 19.28538703918457\n",
            "Evaluating Performance...\n",
            "pass 6680, training loss 20.13809585571289\n",
            "Evaluating Performance...\n",
            "pass 6690, training loss 19.92510414123535\n",
            "Evaluating Performance...\n",
            "pass 6700, training loss 18.9012393951416\n",
            "Evaluating Performance...\n",
            "pass 6710, training loss 19.548114776611328\n",
            "Evaluating Performance...\n",
            "pass 6720, training loss 17.93305015563965\n",
            "Evaluating Performance...\n",
            "pass 6730, training loss 24.928329467773438\n",
            "Evaluating Performance...\n",
            "pass 6740, training loss 18.1639461517334\n",
            "Evaluating Performance...\n",
            "pass 6750, training loss 17.331039428710938\n",
            "Evaluating Performance...\n",
            "pass 6760, training loss 18.862451553344727\n",
            "Evaluating Performance...\n",
            "pass 6770, training loss 21.924943923950195\n",
            "Evaluating Performance...\n",
            "pass 6780, training loss 21.49806785583496\n",
            "Evaluating Performance...\n",
            "pass 6790, training loss 19.894140243530273\n",
            "Evaluating Performance...\n",
            "pass 6800, training loss 18.538055419921875\n",
            "Evaluating Performance...\n",
            "pass 6810, training loss 20.908464431762695\n",
            "Evaluating Performance...\n",
            "pass 6820, training loss 18.32660675048828\n",
            "Evaluating Performance...\n",
            "pass 6830, training loss 21.469093322753906\n",
            "Evaluating Performance...\n",
            "pass 6840, training loss 18.171674728393555\n",
            "Evaluating Performance...\n",
            "pass 6850, training loss 20.3280029296875\n",
            "Evaluating Performance...\n",
            "pass 6860, training loss 20.3967227935791\n",
            "Evaluating Performance...\n",
            "pass 6870, training loss 18.921892166137695\n",
            "Evaluating Performance...\n",
            "pass 6880, training loss 18.31026840209961\n",
            "Evaluating Performance...\n",
            "pass 6890, training loss 17.57452964782715\n",
            "Evaluating Performance...\n",
            "pass 6900, training loss 18.373950958251953\n",
            "Evaluating Performance...\n",
            "pass 6910, training loss 20.973731994628906\n",
            "Evaluating Performance...\n",
            "pass 6920, training loss 23.47787094116211\n",
            "Evaluating Performance...\n",
            "pass 6930, training loss 16.415668487548828\n",
            "Evaluating Performance...\n",
            "pass 6940, training loss 18.364192962646484\n",
            "Evaluating Performance...\n",
            "pass 6950, training loss 16.900632858276367\n",
            "Evaluating Performance...\n",
            "pass 6960, training loss 20.721818923950195\n",
            "Evaluating Performance...\n",
            "pass 6970, training loss 19.48297119140625\n",
            "Evaluating Performance...\n",
            "pass 6980, training loss 20.174795150756836\n",
            "Evaluating Performance...\n",
            "pass 6990, training loss 19.115718841552734\n",
            "Evaluating Performance...\n",
            "pass 7000, training loss 16.15730094909668\n",
            "Evaluating Performance...\n",
            "pass 7010, training loss 17.954755783081055\n",
            "Evaluating Performance...\n",
            "pass 7020, training loss 19.803701400756836\n",
            "Evaluating Performance...\n",
            "pass 7030, training loss 18.726356506347656\n",
            "Evaluating Performance...\n",
            "pass 7040, training loss 20.65899085998535\n",
            "Evaluating Performance...\n",
            "pass 7050, training loss 19.51563262939453\n",
            "Evaluating Performance...\n",
            "pass 7060, training loss 16.913585662841797\n",
            "Evaluating Performance...\n",
            "pass 7070, training loss 17.49324607849121\n",
            "Evaluating Performance...\n",
            "pass 7080, training loss 19.80375862121582\n",
            "Evaluating Performance...\n",
            "pass 7090, training loss 19.90015983581543\n",
            "Evaluating Performance...\n",
            "pass 7100, training loss 21.313749313354492\n",
            "Evaluating Performance...\n",
            "pass 7110, training loss 18.134248733520508\n",
            "Evaluating Performance...\n",
            "pass 7120, training loss 23.582233428955078\n",
            "Evaluating Performance...\n",
            "pass 7130, training loss 21.03105926513672\n",
            "Evaluating Performance...\n",
            "pass 7140, training loss 21.99713897705078\n",
            "Evaluating Performance...\n",
            "pass 7150, training loss 18.463388442993164\n",
            "Evaluating Performance...\n",
            "pass 7160, training loss 19.388986587524414\n",
            "Evaluating Performance...\n",
            "pass 7170, training loss 24.816097259521484\n",
            "Evaluating Performance...\n",
            "pass 7180, training loss 22.31380844116211\n",
            "Evaluating Performance...\n",
            "pass 7190, training loss 25.409984588623047\n",
            "Evaluating Performance...\n",
            "pass 7200, training loss 17.267345428466797\n",
            "Evaluating Performance...\n",
            "pass 7210, training loss 19.853708267211914\n",
            "Evaluating Performance...\n",
            "pass 7220, training loss 17.767860412597656\n",
            "Evaluating Performance...\n",
            "pass 7230, training loss 19.489519119262695\n",
            "Evaluating Performance...\n",
            "pass 7240, training loss 23.60777473449707\n",
            "Evaluating Performance...\n",
            "pass 7250, training loss 18.567657470703125\n",
            "Evaluating Performance...\n",
            "pass 7260, training loss 20.030590057373047\n",
            "Evaluating Performance...\n",
            "pass 7270, training loss 20.31706428527832\n",
            "Evaluating Performance...\n",
            "pass 7280, training loss 19.387929916381836\n",
            "Evaluating Performance...\n",
            "pass 7290, training loss 17.70496940612793\n",
            "Evaluating Performance...\n",
            "pass 7300, training loss 18.327428817749023\n",
            "Evaluating Performance...\n",
            "pass 7310, training loss 20.37648582458496\n",
            "Evaluating Performance...\n",
            "pass 7320, training loss 21.871471405029297\n",
            "Evaluating Performance...\n",
            "pass 7330, training loss 18.11937141418457\n",
            "Evaluating Performance...\n",
            "pass 7340, training loss 19.37764549255371\n",
            "Evaluating Performance...\n",
            "pass 7350, training loss 18.136945724487305\n",
            "Evaluating Performance...\n",
            "pass 7360, training loss 21.253019332885742\n",
            "Evaluating Performance...\n",
            "pass 7370, training loss 20.79703140258789\n",
            "Evaluating Performance...\n",
            "pass 7380, training loss 15.569189071655273\n",
            "Evaluating Performance...\n",
            "pass 7390, training loss 18.729278564453125\n",
            "Evaluating Performance...\n",
            "pass 7400, training loss 18.846893310546875\n",
            "Evaluating Performance...\n",
            "pass 7410, training loss 16.758665084838867\n",
            "Evaluating Performance...\n",
            "pass 7420, training loss 16.550983428955078\n",
            "Evaluating Performance...\n",
            "pass 7430, training loss 17.76628303527832\n",
            "Evaluating Performance...\n",
            "pass 7440, training loss 19.14892578125\n",
            "Evaluating Performance...\n",
            "pass 7450, training loss 20.822668075561523\n",
            "Evaluating Performance...\n",
            "pass 7460, training loss 19.507978439331055\n",
            "Evaluating Performance...\n",
            "pass 7470, training loss 18.772655487060547\n",
            "Evaluating Performance...\n",
            "pass 7480, training loss 19.49288558959961\n",
            "Evaluating Performance...\n",
            "pass 7490, training loss 18.58326530456543\n",
            "Evaluating Performance...\n",
            "pass 7500, training loss 17.7178955078125\n",
            "Evaluating Performance...\n",
            "pass 7510, training loss 19.412216186523438\n",
            "Evaluating Performance...\n",
            "pass 7520, training loss 18.652912139892578\n",
            "Evaluating Performance...\n",
            "pass 7530, training loss 19.786115646362305\n",
            "Evaluating Performance...\n",
            "pass 7540, training loss 20.992956161499023\n",
            "Evaluating Performance...\n",
            "pass 7550, training loss 18.02260398864746\n",
            "Evaluating Performance...\n",
            "pass 7560, training loss 22.58495330810547\n",
            "Evaluating Performance...\n",
            "pass 7570, training loss 20.712238311767578\n",
            "Evaluating Performance...\n",
            "pass 7580, training loss 20.982751846313477\n",
            "Evaluating Performance...\n",
            "pass 7590, training loss 22.754911422729492\n",
            "Evaluating Performance...\n",
            "pass 7600, training loss 18.3804874420166\n",
            "Evaluating Performance...\n",
            "pass 7610, training loss 17.9472713470459\n",
            "Evaluating Performance...\n",
            "pass 7620, training loss 17.38451385498047\n",
            "Evaluating Performance...\n",
            "pass 7630, training loss 18.060972213745117\n",
            "Evaluating Performance...\n",
            "pass 7640, training loss 17.750286102294922\n",
            "Evaluating Performance...\n",
            "pass 7650, training loss 19.369951248168945\n",
            "Evaluating Performance...\n",
            "pass 7660, training loss 17.458477020263672\n",
            "Evaluating Performance...\n",
            "pass 7670, training loss 20.785802841186523\n",
            "Evaluating Performance...\n",
            "pass 7680, training loss 14.743325233459473\n",
            "Evaluating Performance...\n",
            "pass 7690, training loss 22.13611602783203\n",
            "Evaluating Performance...\n",
            "pass 7700, training loss 17.215438842773438\n",
            "Evaluating Performance...\n",
            "pass 7710, training loss 19.91834259033203\n",
            "Evaluating Performance...\n",
            "pass 7720, training loss 16.92616844177246\n",
            "Evaluating Performance...\n",
            "pass 7730, training loss 20.189882278442383\n",
            "Evaluating Performance...\n",
            "pass 7740, training loss 18.08890151977539\n",
            "Evaluating Performance...\n",
            "pass 7750, training loss 19.146520614624023\n",
            "Evaluating Performance...\n",
            "pass 7760, training loss 18.257793426513672\n",
            "Evaluating Performance...\n",
            "pass 7770, training loss 23.055700302124023\n",
            "Evaluating Performance...\n",
            "pass 7780, training loss 16.31682777404785\n",
            "Evaluating Performance...\n",
            "pass 7790, training loss 17.940235137939453\n",
            "Evaluating Performance...\n",
            "pass 7800, training loss 19.83983612060547\n",
            "Evaluating Performance...\n",
            "pass 7810, training loss 16.00326156616211\n",
            "Evaluating Performance...\n",
            "pass 7820, training loss 19.60108184814453\n",
            "Evaluating Performance...\n",
            "pass 7830, training loss 20.9739933013916\n",
            "Evaluating Performance...\n",
            "pass 7840, training loss 19.862735748291016\n",
            "Evaluating Performance...\n",
            "pass 7850, training loss 18.495203018188477\n",
            "Evaluating Performance...\n",
            "pass 7860, training loss 19.805194854736328\n",
            "Evaluating Performance...\n",
            "pass 7870, training loss 19.53055191040039\n",
            "Evaluating Performance...\n",
            "pass 7880, training loss 15.05175495147705\n",
            "Evaluating Performance...\n",
            "pass 7890, training loss 18.9984130859375\n",
            "Evaluating Performance...\n",
            "pass 7900, training loss 16.680999755859375\n",
            "Evaluating Performance...\n",
            "pass 7910, training loss 19.774234771728516\n",
            "Evaluating Performance...\n",
            "pass 7920, training loss 17.12843132019043\n",
            "Evaluating Performance...\n",
            "pass 7930, training loss 19.73511505126953\n",
            "Evaluating Performance...\n",
            "pass 7940, training loss 19.4068660736084\n",
            "Evaluating Performance...\n",
            "pass 7950, training loss 17.015737533569336\n",
            "Evaluating Performance...\n",
            "pass 7960, training loss 17.62409210205078\n",
            "Evaluating Performance...\n",
            "pass 7970, training loss 17.955656051635742\n",
            "Evaluating Performance...\n",
            "pass 7980, training loss 15.73540210723877\n",
            "Evaluating Performance...\n",
            "pass 7990, training loss 18.477272033691406\n",
            "Evaluating Performance...\n",
            "pass 8000, training loss 19.040796279907227\n",
            "Evaluating Performance...\n",
            "pass 8010, training loss 18.978862762451172\n",
            "Evaluating Performance...\n",
            "pass 8020, training loss 17.684730529785156\n",
            "Evaluating Performance...\n",
            "pass 8030, training loss 21.832393646240234\n",
            "Evaluating Performance...\n",
            "pass 8040, training loss 17.46875\n",
            "Evaluating Performance...\n",
            "pass 8050, training loss 17.961627960205078\n",
            "Evaluating Performance...\n",
            "pass 8060, training loss 16.625001907348633\n",
            "Evaluating Performance...\n",
            "pass 8070, training loss 24.207120895385742\n",
            "Evaluating Performance...\n",
            "pass 8080, training loss 19.955671310424805\n",
            "Evaluating Performance...\n",
            "pass 8090, training loss 21.824443817138672\n",
            "Evaluating Performance...\n",
            "pass 8100, training loss 17.617042541503906\n",
            "Evaluating Performance...\n",
            "pass 8110, training loss 17.94624137878418\n",
            "Evaluating Performance...\n",
            "pass 8120, training loss 16.7581787109375\n",
            "Evaluating Performance...\n",
            "pass 8130, training loss 19.323108673095703\n",
            "Evaluating Performance...\n",
            "pass 8140, training loss 17.702611923217773\n",
            "Evaluating Performance...\n",
            "pass 8150, training loss 19.08868408203125\n",
            "Evaluating Performance...\n",
            "pass 8160, training loss 17.981279373168945\n",
            "Evaluating Performance...\n",
            "pass 8170, training loss 19.607011795043945\n",
            "Evaluating Performance...\n",
            "pass 8180, training loss 18.8514461517334\n",
            "Evaluating Performance...\n",
            "pass 8190, training loss 19.67007827758789\n",
            "Evaluating Performance...\n",
            "pass 8200, training loss 18.32892417907715\n",
            "Evaluating Performance...\n",
            "pass 8210, training loss 21.441999435424805\n",
            "Evaluating Performance...\n",
            "pass 8220, training loss 18.282238006591797\n",
            "Evaluating Performance...\n",
            "pass 8230, training loss 17.1021671295166\n",
            "Evaluating Performance...\n",
            "pass 8240, training loss 17.884212493896484\n",
            "Evaluating Performance...\n",
            "pass 8250, training loss 13.991883277893066\n",
            "Evaluating Performance...\n",
            "pass 8260, training loss 21.69148063659668\n",
            "Evaluating Performance...\n",
            "pass 8270, training loss 17.858585357666016\n",
            "Evaluating Performance...\n",
            "pass 8280, training loss 18.81633949279785\n",
            "Evaluating Performance...\n",
            "pass 8290, training loss 16.914737701416016\n",
            "Evaluating Performance...\n",
            "pass 8300, training loss 16.266036987304688\n",
            "Evaluating Performance...\n",
            "pass 8310, training loss 17.47292709350586\n",
            "Evaluating Performance...\n",
            "pass 8320, training loss 16.125818252563477\n",
            "Evaluating Performance...\n",
            "pass 8330, training loss 15.794841766357422\n",
            "Evaluating Performance...\n",
            "pass 8340, training loss 16.10137939453125\n",
            "Evaluating Performance...\n",
            "pass 8350, training loss 18.908897399902344\n",
            "Evaluating Performance...\n",
            "pass 8360, training loss 18.95206069946289\n",
            "Evaluating Performance...\n",
            "pass 8370, training loss 19.047346115112305\n",
            "Evaluating Performance...\n",
            "pass 8380, training loss 16.817424774169922\n",
            "Evaluating Performance...\n",
            "pass 8390, training loss 20.3946590423584\n",
            "Evaluating Performance...\n",
            "pass 8400, training loss 18.899837493896484\n",
            "Evaluating Performance...\n",
            "pass 8410, training loss 15.495327949523926\n",
            "Evaluating Performance...\n",
            "pass 8420, training loss 15.89349365234375\n",
            "Evaluating Performance...\n",
            "pass 8430, training loss 18.403919219970703\n",
            "Evaluating Performance...\n",
            "pass 8440, training loss 16.926816940307617\n",
            "Evaluating Performance...\n",
            "pass 8450, training loss 15.369945526123047\n",
            "Evaluating Performance...\n",
            "pass 8460, training loss 18.88460922241211\n",
            "Evaluating Performance...\n",
            "pass 8470, training loss 19.141117095947266\n",
            "Evaluating Performance...\n",
            "pass 8480, training loss 17.767234802246094\n",
            "Evaluating Performance...\n",
            "pass 8490, training loss 19.44786834716797\n",
            "Evaluating Performance...\n",
            "pass 8500, training loss 14.99693775177002\n",
            "Evaluating Performance...\n",
            "pass 8510, training loss 15.691547393798828\n",
            "Evaluating Performance...\n",
            "pass 8520, training loss 18.22330665588379\n",
            "Evaluating Performance...\n",
            "pass 8530, training loss 16.70513153076172\n",
            "Evaluating Performance...\n",
            "pass 8540, training loss 13.660187721252441\n",
            "Evaluating Performance...\n",
            "pass 8550, training loss 15.898605346679688\n",
            "Evaluating Performance...\n",
            "pass 8560, training loss 15.520283699035645\n",
            "Evaluating Performance...\n",
            "pass 8570, training loss 18.36767578125\n",
            "Evaluating Performance...\n",
            "pass 8580, training loss 14.795891761779785\n",
            "Evaluating Performance...\n",
            "pass 8590, training loss 22.040740966796875\n",
            "Evaluating Performance...\n",
            "pass 8600, training loss 17.477989196777344\n",
            "Evaluating Performance...\n",
            "pass 8610, training loss 16.566194534301758\n",
            "Evaluating Performance...\n",
            "pass 8620, training loss 16.790470123291016\n",
            "Evaluating Performance...\n",
            "pass 8630, training loss 18.617982864379883\n",
            "Evaluating Performance...\n",
            "pass 8640, training loss 17.99360466003418\n",
            "Evaluating Performance...\n",
            "pass 8650, training loss 14.690373420715332\n",
            "Evaluating Performance...\n",
            "pass 8660, training loss 14.545572280883789\n",
            "Evaluating Performance...\n",
            "pass 8670, training loss 18.35567855834961\n",
            "Evaluating Performance...\n",
            "pass 8680, training loss 19.223526000976562\n",
            "Evaluating Performance...\n",
            "pass 8690, training loss 16.123008728027344\n",
            "Evaluating Performance...\n",
            "pass 8700, training loss 20.467790603637695\n",
            "Evaluating Performance...\n",
            "pass 8710, training loss 15.54725170135498\n",
            "Evaluating Performance...\n",
            "pass 8720, training loss 16.198440551757812\n",
            "Evaluating Performance...\n",
            "pass 8730, training loss 16.616588592529297\n",
            "Evaluating Performance...\n",
            "pass 8740, training loss 17.09890365600586\n",
            "Evaluating Performance...\n",
            "pass 8750, training loss 16.058412551879883\n",
            "Evaluating Performance...\n",
            "pass 8760, training loss 20.172130584716797\n",
            "Evaluating Performance...\n",
            "pass 8770, training loss 17.641244888305664\n",
            "Evaluating Performance...\n",
            "pass 8780, training loss 17.771404266357422\n",
            "Evaluating Performance...\n",
            "pass 8790, training loss 18.267303466796875\n",
            "Evaluating Performance...\n",
            "pass 8800, training loss 15.690211296081543\n",
            "Evaluating Performance...\n",
            "pass 8810, training loss 18.370508193969727\n",
            "Evaluating Performance...\n",
            "pass 8820, training loss 20.290430068969727\n",
            "Evaluating Performance...\n",
            "pass 8830, training loss 17.546911239624023\n",
            "Evaluating Performance...\n",
            "pass 8840, training loss 21.162378311157227\n",
            "Evaluating Performance...\n",
            "pass 8850, training loss 20.127277374267578\n",
            "Evaluating Performance...\n",
            "pass 8860, training loss 20.180009841918945\n",
            "Evaluating Performance...\n",
            "pass 8870, training loss 17.63593292236328\n",
            "Evaluating Performance...\n",
            "pass 8880, training loss 17.450971603393555\n",
            "Evaluating Performance...\n",
            "pass 8890, training loss 17.407798767089844\n",
            "Evaluating Performance...\n",
            "pass 8900, training loss 20.433008193969727\n",
            "Evaluating Performance...\n",
            "pass 8910, training loss 15.271625518798828\n",
            "Evaluating Performance...\n",
            "pass 8920, training loss 19.02306365966797\n",
            "Evaluating Performance...\n",
            "pass 8930, training loss 17.42250633239746\n",
            "Evaluating Performance...\n",
            "pass 8940, training loss 18.88661003112793\n",
            "Evaluating Performance...\n",
            "pass 8950, training loss 13.624063491821289\n",
            "Evaluating Performance...\n",
            "pass 8960, training loss 18.766765594482422\n",
            "Evaluating Performance...\n",
            "pass 8970, training loss 15.759800910949707\n",
            "Evaluating Performance...\n",
            "pass 8980, training loss 15.572108268737793\n",
            "Evaluating Performance...\n",
            "pass 8990, training loss 17.35730743408203\n",
            "Evaluating Performance...\n",
            "pass 9000, training loss 16.0505313873291\n",
            "Evaluating Performance...\n",
            "pass 9010, training loss 16.385801315307617\n",
            "Evaluating Performance...\n",
            "pass 9020, training loss 17.644113540649414\n",
            "Evaluating Performance...\n",
            "pass 9030, training loss 18.535322189331055\n",
            "Evaluating Performance...\n",
            "pass 9040, training loss 17.471763610839844\n",
            "Evaluating Performance...\n",
            "pass 9050, training loss 16.810237884521484\n",
            "Evaluating Performance...\n",
            "pass 9060, training loss 17.036266326904297\n",
            "Evaluating Performance...\n",
            "pass 9070, training loss 18.859464645385742\n",
            "Evaluating Performance...\n",
            "pass 9080, training loss 17.38693618774414\n",
            "Evaluating Performance...\n",
            "pass 9090, training loss 18.86920738220215\n",
            "Evaluating Performance...\n",
            "pass 9100, training loss 17.41858673095703\n",
            "Evaluating Performance...\n",
            "pass 9110, training loss 18.352054595947266\n",
            "Evaluating Performance...\n",
            "pass 9120, training loss 15.74394416809082\n",
            "Evaluating Performance...\n",
            "pass 9130, training loss 15.722617149353027\n",
            "Evaluating Performance...\n",
            "pass 9140, training loss 16.843795776367188\n",
            "Evaluating Performance...\n",
            "pass 9150, training loss 17.56743621826172\n",
            "Evaluating Performance...\n",
            "pass 9160, training loss 16.15718650817871\n",
            "Evaluating Performance...\n",
            "pass 9170, training loss 15.436965942382812\n",
            "Evaluating Performance...\n",
            "pass 9180, training loss 16.51766586303711\n",
            "Evaluating Performance...\n",
            "pass 9190, training loss 18.42014503479004\n",
            "Evaluating Performance...\n",
            "pass 9200, training loss 17.3671875\n",
            "Evaluating Performance...\n",
            "pass 9210, training loss 16.157672882080078\n",
            "Evaluating Performance...\n",
            "pass 9220, training loss 17.535703659057617\n",
            "Evaluating Performance...\n",
            "pass 9230, training loss 15.885082244873047\n",
            "Evaluating Performance...\n",
            "pass 9240, training loss 17.897546768188477\n",
            "Evaluating Performance...\n",
            "pass 9250, training loss 19.654497146606445\n",
            "Evaluating Performance...\n",
            "pass 9260, training loss 13.308403015136719\n",
            "Evaluating Performance...\n",
            "pass 9270, training loss 17.073514938354492\n",
            "Evaluating Performance...\n",
            "pass 9280, training loss 17.5352725982666\n",
            "Evaluating Performance...\n",
            "pass 9290, training loss 15.676542282104492\n",
            "Evaluating Performance...\n",
            "pass 9300, training loss 16.961063385009766\n",
            "Evaluating Performance...\n",
            "pass 9310, training loss 16.953073501586914\n",
            "Evaluating Performance...\n",
            "pass 9320, training loss 15.555243492126465\n",
            "Evaluating Performance...\n",
            "pass 9330, training loss 21.367645263671875\n",
            "Evaluating Performance...\n",
            "pass 9340, training loss 14.796868324279785\n",
            "Evaluating Performance...\n",
            "pass 9350, training loss 19.06626319885254\n",
            "Evaluating Performance...\n",
            "pass 9360, training loss 14.310184478759766\n",
            "Evaluating Performance...\n",
            "pass 9370, training loss 14.990187644958496\n",
            "Evaluating Performance...\n",
            "pass 9380, training loss 16.820768356323242\n",
            "Evaluating Performance...\n",
            "pass 9390, training loss 15.469064712524414\n",
            "Evaluating Performance...\n",
            "pass 9400, training loss 13.221343994140625\n",
            "Evaluating Performance...\n",
            "pass 9410, training loss 17.359243392944336\n",
            "Evaluating Performance...\n",
            "pass 9420, training loss 18.631633758544922\n",
            "Evaluating Performance...\n",
            "pass 9430, training loss 16.929141998291016\n",
            "Evaluating Performance...\n",
            "pass 9440, training loss 16.69292640686035\n",
            "Evaluating Performance...\n",
            "pass 9450, training loss 14.826461791992188\n",
            "Evaluating Performance...\n",
            "pass 9460, training loss 19.759113311767578\n",
            "Evaluating Performance...\n",
            "pass 9470, training loss 15.854001998901367\n",
            "Evaluating Performance...\n",
            "pass 9480, training loss 17.304704666137695\n",
            "Evaluating Performance...\n",
            "pass 9490, training loss 18.30667495727539\n",
            "Evaluating Performance...\n",
            "pass 9500, training loss 17.95936393737793\n",
            "Evaluating Performance...\n",
            "pass 9510, training loss 19.833791732788086\n",
            "Evaluating Performance...\n",
            "pass 9520, training loss 15.444087028503418\n",
            "Evaluating Performance...\n",
            "pass 9530, training loss 19.773378372192383\n",
            "Evaluating Performance...\n",
            "pass 9540, training loss 15.75517749786377\n",
            "Evaluating Performance...\n",
            "pass 9550, training loss 20.16809844970703\n",
            "Evaluating Performance...\n",
            "pass 9560, training loss 16.159099578857422\n",
            "Evaluating Performance...\n",
            "pass 9570, training loss 17.124250411987305\n",
            "Evaluating Performance...\n",
            "pass 9580, training loss 14.627434730529785\n",
            "Evaluating Performance...\n",
            "pass 9590, training loss 18.094797134399414\n",
            "Evaluating Performance...\n",
            "pass 9600, training loss 18.151363372802734\n",
            "Evaluating Performance...\n",
            "pass 9610, training loss 18.0927677154541\n",
            "Evaluating Performance...\n",
            "pass 9620, training loss 16.465410232543945\n",
            "Evaluating Performance...\n",
            "pass 9630, training loss 16.093738555908203\n",
            "Evaluating Performance...\n",
            "pass 9640, training loss 14.939651489257812\n",
            "Evaluating Performance...\n",
            "pass 9650, training loss 16.977054595947266\n",
            "Evaluating Performance...\n",
            "pass 9660, training loss 14.91849422454834\n",
            "Evaluating Performance...\n",
            "pass 9670, training loss 17.037883758544922\n",
            "Evaluating Performance...\n",
            "pass 9680, training loss 19.2598819732666\n",
            "Evaluating Performance...\n",
            "pass 9690, training loss 14.027471542358398\n",
            "Evaluating Performance...\n",
            "pass 9700, training loss 14.075165748596191\n",
            "Evaluating Performance...\n",
            "pass 9710, training loss 15.092207908630371\n",
            "Evaluating Performance...\n",
            "pass 9720, training loss 15.599496841430664\n",
            "Evaluating Performance...\n",
            "pass 9730, training loss 13.170854568481445\n",
            "Evaluating Performance...\n",
            "pass 9740, training loss 17.437931060791016\n",
            "Evaluating Performance...\n",
            "pass 9750, training loss 14.933673858642578\n",
            "Evaluating Performance...\n",
            "pass 9760, training loss 16.23713493347168\n",
            "Evaluating Performance...\n",
            "pass 9770, training loss 15.162492752075195\n",
            "Evaluating Performance...\n",
            "pass 9780, training loss 17.543731689453125\n",
            "Evaluating Performance...\n",
            "pass 9790, training loss 16.172101974487305\n",
            "Evaluating Performance...\n",
            "pass 9800, training loss 16.61716079711914\n",
            "Evaluating Performance...\n",
            "pass 9810, training loss 13.011026382446289\n",
            "Evaluating Performance...\n",
            "pass 9820, training loss 17.164331436157227\n",
            "Evaluating Performance...\n",
            "pass 9830, training loss 15.453802108764648\n",
            "Evaluating Performance...\n",
            "pass 9840, training loss 16.062416076660156\n",
            "Evaluating Performance...\n",
            "pass 9850, training loss 17.7905216217041\n",
            "Evaluating Performance...\n",
            "pass 9860, training loss 15.248273849487305\n",
            "Evaluating Performance...\n",
            "pass 9870, training loss 13.645395278930664\n",
            "Evaluating Performance...\n",
            "pass 9880, training loss 18.1823673248291\n",
            "Evaluating Performance...\n",
            "pass 9890, training loss 15.683120727539062\n",
            "Evaluating Performance...\n",
            "pass 9900, training loss 13.919976234436035\n",
            "Evaluating Performance...\n",
            "pass 9910, training loss 16.319625854492188\n",
            "Evaluating Performance...\n",
            "pass 9920, training loss 17.85902214050293\n",
            "Evaluating Performance...\n",
            "pass 9930, training loss 15.538339614868164\n",
            "Evaluating Performance...\n",
            "pass 9940, training loss 15.373592376708984\n",
            "Evaluating Performance...\n",
            "pass 9950, training loss 12.413639068603516\n",
            "Evaluating Performance...\n",
            "pass 9960, training loss 12.598846435546875\n",
            "Evaluating Performance...\n",
            "pass 9970, training loss 19.324636459350586\n",
            "Evaluating Performance...\n",
            "pass 9980, training loss 15.187352180480957\n",
            "Evaluating Performance...\n",
            "pass 9990, training loss 13.07320785522461\n",
            "Evaluating Performance...\n",
            "pass 10000, training loss 15.588191986083984\n",
            "Saving Checkpoint...\n",
            "checkpoint saved\n",
            "Evaluating Performance...\n",
            "pass 10010, training loss 19.810819625854492\n",
            "Evaluating Performance...\n",
            "pass 10020, training loss 14.63045883178711\n",
            "Evaluating Performance...\n",
            "pass 10030, training loss 12.392806053161621\n",
            "Evaluating Performance...\n",
            "pass 10040, training loss 18.497678756713867\n",
            "Evaluating Performance...\n",
            "pass 10050, training loss 17.88706398010254\n",
            "Evaluating Performance...\n",
            "pass 10060, training loss 13.895509719848633\n",
            "Evaluating Performance...\n",
            "pass 10070, training loss 16.875179290771484\n",
            "Evaluating Performance...\n",
            "pass 10080, training loss 14.72004222869873\n",
            "Evaluating Performance...\n",
            "pass 10090, training loss 13.07795524597168\n",
            "Evaluating Performance...\n",
            "pass 10100, training loss 16.821653366088867\n",
            "Evaluating Performance...\n",
            "pass 10110, training loss 14.497617721557617\n",
            "Evaluating Performance...\n",
            "pass 10120, training loss 16.778600692749023\n",
            "Evaluating Performance...\n",
            "pass 10130, training loss 14.011603355407715\n",
            "Evaluating Performance...\n",
            "pass 10140, training loss 16.33027458190918\n",
            "Evaluating Performance...\n",
            "pass 10150, training loss 14.305320739746094\n",
            "Evaluating Performance...\n",
            "pass 10160, training loss 12.841659545898438\n",
            "Evaluating Performance...\n",
            "pass 10170, training loss 18.34626007080078\n",
            "Evaluating Performance...\n",
            "pass 10180, training loss 16.831031799316406\n",
            "Evaluating Performance...\n",
            "pass 10190, training loss 14.018539428710938\n",
            "Evaluating Performance...\n",
            "pass 10200, training loss 16.329822540283203\n",
            "Evaluating Performance...\n",
            "pass 10210, training loss 16.93978500366211\n",
            "Evaluating Performance...\n",
            "pass 10220, training loss 16.385318756103516\n",
            "Evaluating Performance...\n",
            "pass 10230, training loss 18.886945724487305\n",
            "Evaluating Performance...\n",
            "pass 10240, training loss 14.929545402526855\n",
            "Evaluating Performance...\n",
            "pass 10250, training loss 14.291735649108887\n",
            "Evaluating Performance...\n",
            "pass 10260, training loss 13.990466117858887\n",
            "Evaluating Performance...\n",
            "pass 10270, training loss 15.338221549987793\n",
            "Evaluating Performance...\n",
            "pass 10280, training loss 14.749235153198242\n",
            "Evaluating Performance...\n",
            "pass 10290, training loss 14.207892417907715\n",
            "Evaluating Performance...\n",
            "pass 10300, training loss 17.25349235534668\n",
            "Evaluating Performance...\n",
            "pass 10310, training loss 17.05617904663086\n",
            "Evaluating Performance...\n",
            "pass 10320, training loss 11.800756454467773\n",
            "Evaluating Performance...\n",
            "pass 10330, training loss 15.7366304397583\n",
            "Evaluating Performance...\n",
            "pass 10340, training loss 14.468485832214355\n",
            "Evaluating Performance...\n",
            "pass 10350, training loss 14.379758834838867\n",
            "Evaluating Performance...\n",
            "pass 10360, training loss 12.652458190917969\n",
            "Evaluating Performance...\n",
            "pass 10370, training loss 16.59479331970215\n",
            "Evaluating Performance...\n",
            "pass 10380, training loss 10.334927558898926\n",
            "Evaluating Performance...\n",
            "pass 10390, training loss 16.874601364135742\n",
            "Evaluating Performance...\n",
            "pass 10400, training loss 16.720340728759766\n",
            "Evaluating Performance...\n",
            "pass 10410, training loss 15.820745468139648\n",
            "Evaluating Performance...\n",
            "pass 10420, training loss 15.574856758117676\n",
            "Evaluating Performance...\n",
            "pass 10430, training loss 13.736923217773438\n",
            "Evaluating Performance...\n",
            "pass 10440, training loss 20.283601760864258\n",
            "Evaluating Performance...\n",
            "pass 10450, training loss 15.929247856140137\n",
            "Evaluating Performance...\n",
            "pass 10460, training loss 14.161260604858398\n",
            "Evaluating Performance...\n",
            "pass 10470, training loss 16.62452507019043\n",
            "Evaluating Performance...\n",
            "pass 10480, training loss 14.50011920928955\n",
            "Evaluating Performance...\n",
            "pass 10490, training loss 17.188814163208008\n",
            "Evaluating Performance...\n",
            "pass 10500, training loss 15.45754623413086\n",
            "Evaluating Performance...\n",
            "pass 10510, training loss 15.028162956237793\n",
            "Evaluating Performance...\n",
            "pass 10520, training loss 16.810449600219727\n",
            "Evaluating Performance...\n",
            "pass 10530, training loss 13.959577560424805\n",
            "Evaluating Performance...\n",
            "pass 10540, training loss 14.993403434753418\n",
            "Evaluating Performance...\n",
            "pass 10550, training loss 17.948904037475586\n",
            "Evaluating Performance...\n",
            "pass 10560, training loss 14.372551918029785\n",
            "Evaluating Performance...\n",
            "pass 10570, training loss 17.984718322753906\n",
            "Evaluating Performance...\n",
            "pass 10580, training loss 10.992902755737305\n",
            "Evaluating Performance...\n",
            "pass 10590, training loss 16.267826080322266\n",
            "Evaluating Performance...\n",
            "pass 10600, training loss 15.661470413208008\n",
            "Evaluating Performance...\n",
            "pass 10610, training loss 15.018438339233398\n",
            "Evaluating Performance...\n",
            "pass 10620, training loss 15.928743362426758\n",
            "Evaluating Performance...\n",
            "pass 10630, training loss 14.2859468460083\n",
            "Evaluating Performance...\n",
            "pass 10640, training loss 17.289030075073242\n",
            "Evaluating Performance...\n",
            "pass 10650, training loss 18.351247787475586\n",
            "Evaluating Performance...\n",
            "pass 10660, training loss 14.66071891784668\n",
            "Evaluating Performance...\n",
            "pass 10670, training loss 13.7393159866333\n",
            "Evaluating Performance...\n",
            "pass 10680, training loss 15.786791801452637\n",
            "Evaluating Performance...\n",
            "pass 10690, training loss 18.42032241821289\n",
            "Evaluating Performance...\n",
            "pass 10700, training loss 15.151774406433105\n",
            "Evaluating Performance...\n",
            "pass 10710, training loss 14.736028671264648\n",
            "Evaluating Performance...\n",
            "pass 10720, training loss 12.238357543945312\n",
            "Evaluating Performance...\n",
            "pass 10730, training loss 15.56064510345459\n",
            "Evaluating Performance...\n",
            "pass 10740, training loss 14.51235294342041\n",
            "Evaluating Performance...\n",
            "pass 10750, training loss 15.336353302001953\n",
            "Evaluating Performance...\n",
            "pass 10760, training loss 16.02956199645996\n",
            "Evaluating Performance...\n",
            "pass 10770, training loss 15.248797416687012\n",
            "Evaluating Performance...\n",
            "pass 10780, training loss 15.755428314208984\n",
            "Evaluating Performance...\n",
            "pass 10790, training loss 15.15938949584961\n",
            "Evaluating Performance...\n",
            "pass 10800, training loss 13.967704772949219\n",
            "Evaluating Performance...\n",
            "pass 10810, training loss 16.35131072998047\n",
            "Evaluating Performance...\n",
            "pass 10820, training loss 16.549949645996094\n",
            "Evaluating Performance...\n",
            "pass 10830, training loss 16.02076530456543\n",
            "Evaluating Performance...\n",
            "pass 10840, training loss 16.37312889099121\n",
            "Evaluating Performance...\n",
            "pass 10850, training loss 16.1146183013916\n",
            "Evaluating Performance...\n",
            "pass 10860, training loss 14.192423820495605\n",
            "Evaluating Performance...\n",
            "pass 10870, training loss 15.004378318786621\n",
            "Evaluating Performance...\n",
            "pass 10880, training loss 16.28651237487793\n",
            "Evaluating Performance...\n",
            "pass 10890, training loss 14.87086296081543\n",
            "Evaluating Performance...\n",
            "pass 10900, training loss 12.844416618347168\n",
            "Evaluating Performance...\n",
            "pass 10910, training loss 14.132003784179688\n",
            "Evaluating Performance...\n",
            "pass 10920, training loss 17.320541381835938\n",
            "Evaluating Performance...\n",
            "pass 10930, training loss 14.976938247680664\n",
            "Evaluating Performance...\n",
            "pass 10940, training loss 17.829885482788086\n",
            "Evaluating Performance...\n",
            "pass 10950, training loss 13.276123046875\n",
            "Evaluating Performance...\n",
            "pass 10960, training loss 19.200912475585938\n",
            "Evaluating Performance...\n",
            "pass 10970, training loss 14.795815467834473\n",
            "Evaluating Performance...\n",
            "pass 10980, training loss 15.439781188964844\n",
            "Evaluating Performance...\n",
            "pass 10990, training loss 16.76728057861328\n",
            "Evaluating Performance...\n",
            "pass 11000, training loss 15.869474411010742\n",
            "Evaluating Performance...\n",
            "pass 11010, training loss 13.96198558807373\n",
            "Evaluating Performance...\n",
            "pass 11020, training loss 17.77730369567871\n",
            "Evaluating Performance...\n",
            "pass 11030, training loss 14.5377779006958\n",
            "Evaluating Performance...\n",
            "pass 11040, training loss 11.99789810180664\n",
            "Evaluating Performance...\n",
            "pass 11050, training loss 13.941459655761719\n",
            "Evaluating Performance...\n",
            "pass 11060, training loss 14.519865036010742\n",
            "Evaluating Performance...\n",
            "pass 11070, training loss 15.716177940368652\n",
            "Evaluating Performance...\n",
            "pass 11080, training loss 14.257963180541992\n",
            "Evaluating Performance...\n",
            "pass 11090, training loss 14.122401237487793\n",
            "Evaluating Performance...\n",
            "pass 11100, training loss 14.318568229675293\n",
            "Evaluating Performance...\n",
            "pass 11110, training loss 16.79057502746582\n",
            "Evaluating Performance...\n",
            "pass 11120, training loss 12.95567798614502\n",
            "Evaluating Performance...\n",
            "pass 11130, training loss 12.358112335205078\n",
            "Evaluating Performance...\n",
            "pass 11140, training loss 15.094499588012695\n",
            "Evaluating Performance...\n",
            "pass 11150, training loss 14.293610572814941\n",
            "Evaluating Performance...\n",
            "pass 11160, training loss 14.093427658081055\n",
            "Evaluating Performance...\n",
            "pass 11170, training loss 13.694036483764648\n",
            "Evaluating Performance...\n",
            "pass 11180, training loss 14.041690826416016\n",
            "Evaluating Performance...\n",
            "pass 11190, training loss 14.186882972717285\n",
            "Evaluating Performance...\n",
            "pass 11200, training loss 11.688791275024414\n",
            "Evaluating Performance...\n",
            "pass 11210, training loss 13.54072380065918\n",
            "Evaluating Performance...\n",
            "pass 11220, training loss 14.494423866271973\n",
            "Evaluating Performance...\n",
            "pass 11230, training loss 14.640910148620605\n",
            "Evaluating Performance...\n",
            "pass 11240, training loss 14.135293960571289\n",
            "Evaluating Performance...\n",
            "pass 11250, training loss 15.434744834899902\n",
            "Evaluating Performance...\n",
            "pass 11260, training loss 14.46916389465332\n",
            "Evaluating Performance...\n",
            "pass 11270, training loss 12.042365074157715\n",
            "Evaluating Performance...\n",
            "pass 11280, training loss 17.252973556518555\n",
            "Evaluating Performance...\n",
            "pass 11290, training loss 14.961641311645508\n",
            "Evaluating Performance...\n",
            "pass 11300, training loss 16.261930465698242\n",
            "Evaluating Performance...\n",
            "pass 11310, training loss 15.984222412109375\n",
            "Evaluating Performance...\n",
            "pass 11320, training loss 15.506625175476074\n",
            "Evaluating Performance...\n",
            "pass 11330, training loss 15.939245223999023\n",
            "Evaluating Performance...\n",
            "pass 11340, training loss 15.211029052734375\n",
            "Evaluating Performance...\n",
            "pass 11350, training loss 12.322653770446777\n",
            "Evaluating Performance...\n",
            "pass 11360, training loss 14.921345710754395\n",
            "Evaluating Performance...\n",
            "pass 11370, training loss 14.810117721557617\n",
            "Evaluating Performance...\n",
            "pass 11380, training loss 12.044434547424316\n",
            "Evaluating Performance...\n",
            "pass 11390, training loss 14.99703311920166\n",
            "Evaluating Performance...\n",
            "pass 11400, training loss 12.703787803649902\n",
            "Evaluating Performance...\n",
            "pass 11410, training loss 13.133294105529785\n",
            "Evaluating Performance...\n",
            "pass 11420, training loss 14.999543190002441\n",
            "Evaluating Performance...\n",
            "pass 11430, training loss 12.471036911010742\n",
            "Evaluating Performance...\n",
            "pass 11440, training loss 14.459228515625\n",
            "Evaluating Performance...\n",
            "pass 11450, training loss 16.801179885864258\n",
            "Evaluating Performance...\n",
            "pass 11460, training loss 13.755358695983887\n",
            "Evaluating Performance...\n",
            "pass 11470, training loss 15.815248489379883\n",
            "Evaluating Performance...\n",
            "pass 11480, training loss 14.056952476501465\n",
            "Evaluating Performance...\n",
            "pass 11490, training loss 16.99903106689453\n",
            "Evaluating Performance...\n",
            "pass 11500, training loss 14.865236282348633\n",
            "Evaluating Performance...\n",
            "pass 11510, training loss 12.007308959960938\n",
            "Evaluating Performance...\n",
            "pass 11520, training loss 14.440693855285645\n",
            "Evaluating Performance...\n",
            "pass 11530, training loss 14.262090682983398\n",
            "Evaluating Performance...\n",
            "pass 11540, training loss 15.778291702270508\n",
            "Evaluating Performance...\n",
            "pass 11550, training loss 15.55528450012207\n",
            "Evaluating Performance...\n",
            "pass 11560, training loss 16.999778747558594\n",
            "Evaluating Performance...\n",
            "pass 11570, training loss 15.402763366699219\n",
            "Evaluating Performance...\n",
            "pass 11580, training loss 14.127382278442383\n",
            "Evaluating Performance...\n",
            "pass 11590, training loss 13.799744606018066\n",
            "Evaluating Performance...\n",
            "pass 11600, training loss 14.454984664916992\n",
            "Evaluating Performance...\n",
            "pass 11610, training loss 13.451372146606445\n",
            "Evaluating Performance...\n",
            "pass 11620, training loss 14.574992179870605\n",
            "Evaluating Performance...\n",
            "pass 11630, training loss 15.1321439743042\n",
            "Evaluating Performance...\n",
            "pass 11640, training loss 12.462234497070312\n",
            "Evaluating Performance...\n",
            "pass 11650, training loss 14.586557388305664\n",
            "Evaluating Performance...\n",
            "pass 11660, training loss 14.247228622436523\n",
            "Evaluating Performance...\n",
            "pass 11670, training loss 14.201375007629395\n",
            "Evaluating Performance...\n",
            "pass 11680, training loss 13.455162048339844\n",
            "Evaluating Performance...\n",
            "pass 11690, training loss 15.155976295471191\n",
            "Evaluating Performance...\n",
            "pass 11700, training loss 17.013431549072266\n",
            "Evaluating Performance...\n",
            "pass 11710, training loss 13.647859573364258\n",
            "Evaluating Performance...\n",
            "pass 11720, training loss 14.844149589538574\n",
            "Evaluating Performance...\n",
            "pass 11730, training loss 15.590995788574219\n",
            "Evaluating Performance...\n",
            "pass 11740, training loss 14.334062576293945\n",
            "Evaluating Performance...\n",
            "pass 11750, training loss 12.39028549194336\n",
            "Evaluating Performance...\n",
            "pass 11760, training loss 13.966997146606445\n",
            "Evaluating Performance...\n",
            "pass 11770, training loss 13.815587043762207\n",
            "Evaluating Performance...\n",
            "pass 11780, training loss 13.661943435668945\n",
            "Evaluating Performance...\n",
            "pass 11790, training loss 13.057315826416016\n",
            "Evaluating Performance...\n",
            "pass 11800, training loss 14.09410285949707\n",
            "Evaluating Performance...\n",
            "pass 11810, training loss 13.331989288330078\n",
            "Evaluating Performance...\n",
            "pass 11820, training loss 13.937753677368164\n",
            "Evaluating Performance...\n",
            "pass 11830, training loss 12.691933631896973\n",
            "Evaluating Performance...\n",
            "pass 11840, training loss 13.68316650390625\n",
            "Evaluating Performance...\n",
            "pass 11850, training loss 15.38447093963623\n",
            "Evaluating Performance...\n",
            "pass 11860, training loss 11.50889778137207\n",
            "Evaluating Performance...\n",
            "pass 11870, training loss 13.589919090270996\n",
            "Evaluating Performance...\n",
            "pass 11880, training loss 13.487895965576172\n",
            "Evaluating Performance...\n",
            "pass 11890, training loss 19.98014259338379\n",
            "Evaluating Performance...\n",
            "pass 11900, training loss 14.606141090393066\n",
            "Evaluating Performance...\n",
            "pass 11910, training loss 11.902481079101562\n",
            "Evaluating Performance...\n",
            "pass 11920, training loss 17.19732666015625\n",
            "Evaluating Performance...\n",
            "pass 11930, training loss 14.559000015258789\n",
            "Evaluating Performance...\n",
            "pass 11940, training loss 15.08344841003418\n",
            "Evaluating Performance...\n",
            "pass 11950, training loss 12.128336906433105\n",
            "Evaluating Performance...\n",
            "pass 11960, training loss 17.242698669433594\n",
            "Evaluating Performance...\n",
            "pass 11970, training loss 14.290263175964355\n",
            "Evaluating Performance...\n",
            "pass 11980, training loss 16.645845413208008\n",
            "Evaluating Performance...\n",
            "pass 11990, training loss 12.785786628723145\n",
            "Evaluating Performance...\n",
            "pass 12000, training loss 10.848451614379883\n",
            "Evaluating Performance...\n",
            "pass 12010, training loss 11.861689567565918\n",
            "Evaluating Performance...\n",
            "pass 12020, training loss 11.796319007873535\n",
            "Evaluating Performance...\n",
            "pass 12030, training loss 14.070051193237305\n",
            "Evaluating Performance...\n",
            "pass 12040, training loss 17.63174819946289\n",
            "Evaluating Performance...\n",
            "pass 12050, training loss 16.779342651367188\n",
            "Evaluating Performance...\n",
            "pass 12060, training loss 14.407864570617676\n",
            "Evaluating Performance...\n",
            "pass 12070, training loss 12.38650894165039\n",
            "Evaluating Performance...\n",
            "pass 12080, training loss 12.600006103515625\n",
            "Evaluating Performance...\n",
            "pass 12090, training loss 11.67038345336914\n",
            "Evaluating Performance...\n",
            "pass 12100, training loss 16.156925201416016\n",
            "Evaluating Performance...\n",
            "pass 12110, training loss 13.43837833404541\n",
            "Evaluating Performance...\n",
            "pass 12120, training loss 12.626108169555664\n",
            "Evaluating Performance...\n",
            "pass 12130, training loss 16.523326873779297\n",
            "Evaluating Performance...\n",
            "pass 12140, training loss 15.073104858398438\n",
            "Evaluating Performance...\n",
            "pass 12150, training loss 13.841222763061523\n",
            "Evaluating Performance...\n",
            "pass 12160, training loss 15.024105072021484\n",
            "Evaluating Performance...\n",
            "pass 12170, training loss 13.173202514648438\n",
            "Evaluating Performance...\n",
            "pass 12180, training loss 14.741830825805664\n",
            "Evaluating Performance...\n",
            "pass 12190, training loss 12.965927124023438\n",
            "Evaluating Performance...\n",
            "pass 12200, training loss 12.425586700439453\n",
            "Evaluating Performance...\n",
            "pass 12210, training loss 14.8169584274292\n",
            "Evaluating Performance...\n",
            "pass 12220, training loss 15.609440803527832\n",
            "Evaluating Performance...\n",
            "pass 12230, training loss 12.989175796508789\n",
            "Evaluating Performance...\n",
            "pass 12240, training loss 13.369254112243652\n",
            "Evaluating Performance...\n",
            "pass 12250, training loss 13.177471160888672\n",
            "Evaluating Performance...\n",
            "pass 12260, training loss 12.408522605895996\n",
            "Evaluating Performance...\n",
            "pass 12270, training loss 15.915109634399414\n",
            "Evaluating Performance...\n",
            "pass 12280, training loss 12.520879745483398\n",
            "Evaluating Performance...\n",
            "pass 12290, training loss 14.766124725341797\n",
            "Evaluating Performance...\n",
            "pass 12300, training loss 16.08516502380371\n",
            "Evaluating Performance...\n",
            "pass 12310, training loss 13.588476181030273\n",
            "Evaluating Performance...\n",
            "pass 12320, training loss 15.97794246673584\n",
            "Evaluating Performance...\n",
            "pass 12330, training loss 13.315898895263672\n",
            "Evaluating Performance...\n",
            "pass 12340, training loss 13.713350296020508\n",
            "Evaluating Performance...\n",
            "pass 12350, training loss 14.347026824951172\n",
            "Evaluating Performance...\n",
            "pass 12360, training loss 13.193082809448242\n",
            "Evaluating Performance...\n",
            "pass 12370, training loss 18.7526798248291\n",
            "Evaluating Performance...\n",
            "pass 12380, training loss 14.890761375427246\n",
            "Evaluating Performance...\n",
            "pass 12390, training loss 11.47698974609375\n",
            "Evaluating Performance...\n",
            "pass 12400, training loss 14.364726066589355\n",
            "Evaluating Performance...\n",
            "pass 12410, training loss 12.97698974609375\n",
            "Evaluating Performance...\n",
            "pass 12420, training loss 12.331889152526855\n",
            "Evaluating Performance...\n",
            "pass 12430, training loss 11.503280639648438\n",
            "Evaluating Performance...\n",
            "pass 12440, training loss 13.986921310424805\n",
            "Evaluating Performance...\n",
            "pass 12450, training loss 12.531746864318848\n",
            "Evaluating Performance...\n",
            "pass 12460, training loss 13.659594535827637\n",
            "Evaluating Performance...\n",
            "pass 12470, training loss 14.540014266967773\n",
            "Evaluating Performance...\n",
            "pass 12480, training loss 16.293426513671875\n",
            "Evaluating Performance...\n",
            "pass 12490, training loss 14.313825607299805\n",
            "Evaluating Performance...\n",
            "pass 12500, training loss 11.51495361328125\n",
            "Evaluating Performance...\n",
            "pass 12510, training loss 14.65548324584961\n",
            "Evaluating Performance...\n",
            "pass 12520, training loss 11.429593086242676\n",
            "Evaluating Performance...\n",
            "pass 12530, training loss 13.510721206665039\n",
            "Evaluating Performance...\n",
            "pass 12540, training loss 11.898005485534668\n",
            "Evaluating Performance...\n",
            "pass 12550, training loss 11.391672134399414\n",
            "Evaluating Performance...\n",
            "pass 12560, training loss 12.037457466125488\n",
            "Evaluating Performance...\n",
            "pass 12570, training loss 15.102004051208496\n",
            "Evaluating Performance...\n",
            "pass 12580, training loss 12.40629768371582\n",
            "Evaluating Performance...\n",
            "pass 12590, training loss 15.648531913757324\n",
            "Evaluating Performance...\n",
            "pass 12600, training loss 12.57988452911377\n",
            "Evaluating Performance...\n",
            "pass 12610, training loss 13.2243013381958\n",
            "Evaluating Performance...\n",
            "pass 12620, training loss 13.136591911315918\n",
            "Evaluating Performance...\n",
            "pass 12630, training loss 13.39960765838623\n",
            "Evaluating Performance...\n",
            "pass 12640, training loss 13.26455307006836\n",
            "Evaluating Performance...\n",
            "pass 12650, training loss 13.238704681396484\n",
            "Evaluating Performance...\n",
            "pass 12660, training loss 12.02816104888916\n",
            "Evaluating Performance...\n",
            "pass 12670, training loss 14.853132247924805\n",
            "Evaluating Performance...\n",
            "pass 12680, training loss 14.06148624420166\n",
            "Evaluating Performance...\n",
            "pass 12690, training loss 11.803115844726562\n",
            "Evaluating Performance...\n",
            "pass 12700, training loss 12.299610137939453\n",
            "Evaluating Performance...\n",
            "pass 12710, training loss 11.650166511535645\n",
            "Evaluating Performance...\n",
            "pass 12720, training loss 14.946195602416992\n",
            "Evaluating Performance...\n",
            "pass 12730, training loss 12.20148754119873\n",
            "Evaluating Performance...\n",
            "pass 12740, training loss 13.635809898376465\n",
            "Evaluating Performance...\n",
            "pass 12750, training loss 15.136737823486328\n",
            "Evaluating Performance...\n",
            "pass 12760, training loss 12.383667945861816\n",
            "Evaluating Performance...\n",
            "pass 12770, training loss 11.844331741333008\n",
            "Evaluating Performance...\n",
            "pass 12780, training loss 14.491037368774414\n",
            "Evaluating Performance...\n",
            "pass 12790, training loss 12.716021537780762\n",
            "Evaluating Performance...\n",
            "pass 12800, training loss 12.218228340148926\n",
            "Evaluating Performance...\n",
            "pass 12810, training loss 14.30019760131836\n",
            "Evaluating Performance...\n",
            "pass 12820, training loss 12.58265495300293\n",
            "Evaluating Performance...\n",
            "pass 12830, training loss 11.712705612182617\n",
            "Evaluating Performance...\n",
            "pass 12840, training loss 13.166055679321289\n",
            "Evaluating Performance...\n",
            "pass 12850, training loss 12.697854042053223\n",
            "Evaluating Performance...\n",
            "pass 12860, training loss 10.117816925048828\n",
            "Evaluating Performance...\n",
            "pass 12870, training loss 11.96990966796875\n",
            "Evaluating Performance...\n",
            "pass 12880, training loss 12.882208824157715\n",
            "Evaluating Performance...\n",
            "pass 12890, training loss 11.273310661315918\n",
            "Evaluating Performance...\n",
            "pass 12900, training loss 13.57646656036377\n",
            "Evaluating Performance...\n",
            "pass 12910, training loss 16.070268630981445\n",
            "Evaluating Performance...\n",
            "pass 12920, training loss 12.002861022949219\n",
            "Evaluating Performance...\n",
            "pass 12930, training loss 13.656429290771484\n",
            "Evaluating Performance...\n",
            "pass 12940, training loss 13.51530647277832\n",
            "Evaluating Performance...\n",
            "pass 12950, training loss 12.916610717773438\n",
            "Evaluating Performance...\n",
            "pass 12960, training loss 14.00291919708252\n",
            "Evaluating Performance...\n",
            "pass 12970, training loss 13.108723640441895\n",
            "Evaluating Performance...\n",
            "pass 12980, training loss 15.315028190612793\n",
            "Evaluating Performance...\n",
            "pass 12990, training loss 12.009039878845215\n",
            "Evaluating Performance...\n",
            "pass 13000, training loss 16.7676944732666\n",
            "Evaluating Performance...\n",
            "pass 13010, training loss 12.518942832946777\n",
            "Evaluating Performance...\n",
            "pass 13020, training loss 12.90715503692627\n",
            "Evaluating Performance...\n",
            "pass 13030, training loss 14.45988941192627\n",
            "Evaluating Performance...\n",
            "pass 13040, training loss 12.234037399291992\n",
            "Evaluating Performance...\n",
            "pass 13050, training loss 14.513046264648438\n",
            "Evaluating Performance...\n",
            "pass 13060, training loss 13.000367164611816\n",
            "Evaluating Performance...\n",
            "pass 13070, training loss 11.022550582885742\n",
            "Evaluating Performance...\n",
            "pass 13080, training loss 14.312703132629395\n",
            "Evaluating Performance...\n",
            "pass 13090, training loss 11.94490909576416\n",
            "Evaluating Performance...\n",
            "pass 13100, training loss 13.47658920288086\n",
            "Evaluating Performance...\n",
            "pass 13110, training loss 12.992445945739746\n",
            "Evaluating Performance...\n",
            "pass 13120, training loss 12.948391914367676\n",
            "Evaluating Performance...\n",
            "pass 13130, training loss 11.509577751159668\n",
            "Evaluating Performance...\n",
            "pass 13140, training loss 14.035593032836914\n",
            "Evaluating Performance...\n",
            "pass 13150, training loss 13.840795516967773\n",
            "Evaluating Performance...\n",
            "pass 13160, training loss 14.339705467224121\n",
            "Evaluating Performance...\n",
            "pass 13170, training loss 13.476346015930176\n",
            "Evaluating Performance...\n",
            "pass 13180, training loss 13.624088287353516\n",
            "Evaluating Performance...\n",
            "pass 13190, training loss 10.506988525390625\n",
            "Evaluating Performance...\n",
            "pass 13200, training loss 10.672757148742676\n",
            "Evaluating Performance...\n",
            "pass 13210, training loss 13.94588851928711\n",
            "Evaluating Performance...\n",
            "pass 13220, training loss 14.571927070617676\n",
            "Evaluating Performance...\n",
            "pass 13230, training loss 13.549129486083984\n",
            "Evaluating Performance...\n",
            "pass 13240, training loss 12.16766357421875\n",
            "Evaluating Performance...\n",
            "pass 13250, training loss 15.735697746276855\n",
            "Evaluating Performance...\n",
            "pass 13260, training loss 12.210734367370605\n",
            "Evaluating Performance...\n",
            "pass 13270, training loss 14.724048614501953\n",
            "Evaluating Performance...\n",
            "pass 13280, training loss 13.479825973510742\n",
            "Evaluating Performance...\n",
            "pass 13290, training loss 12.644362449645996\n",
            "Evaluating Performance...\n",
            "pass 13300, training loss 14.758414268493652\n",
            "Evaluating Performance...\n",
            "pass 13310, training loss 13.943591117858887\n",
            "Evaluating Performance...\n",
            "pass 13320, training loss 16.22981071472168\n",
            "Evaluating Performance...\n",
            "pass 13330, training loss 13.206531524658203\n",
            "Evaluating Performance...\n",
            "pass 13340, training loss 11.859383583068848\n",
            "Evaluating Performance...\n",
            "pass 13350, training loss 14.220911979675293\n",
            "Evaluating Performance...\n",
            "pass 13360, training loss 16.594097137451172\n",
            "Evaluating Performance...\n",
            "pass 13370, training loss 13.574240684509277\n",
            "Evaluating Performance...\n",
            "pass 13380, training loss 14.677863121032715\n",
            "Evaluating Performance...\n",
            "pass 13390, training loss 12.791133880615234\n",
            "Evaluating Performance...\n",
            "pass 13400, training loss 10.782928466796875\n",
            "Evaluating Performance...\n",
            "pass 13410, training loss 11.32606315612793\n",
            "Evaluating Performance...\n",
            "pass 13420, training loss 13.719828605651855\n",
            "Evaluating Performance...\n",
            "pass 13430, training loss 15.45465087890625\n",
            "Evaluating Performance...\n",
            "pass 13440, training loss 13.406448364257812\n",
            "Evaluating Performance...\n",
            "pass 13450, training loss 14.701729774475098\n",
            "Evaluating Performance...\n",
            "pass 13460, training loss 14.902301788330078\n",
            "Evaluating Performance...\n",
            "pass 13470, training loss 13.631450653076172\n",
            "Evaluating Performance...\n",
            "pass 13480, training loss 14.744196891784668\n",
            "Evaluating Performance...\n",
            "pass 13490, training loss 15.94652271270752\n",
            "Evaluating Performance...\n",
            "pass 13500, training loss 12.573756217956543\n",
            "Evaluating Performance...\n",
            "pass 13510, training loss 14.637476921081543\n",
            "Evaluating Performance...\n",
            "pass 13520, training loss 12.605974197387695\n",
            "Evaluating Performance...\n",
            "pass 13530, training loss 13.038168907165527\n",
            "Evaluating Performance...\n",
            "pass 13540, training loss 17.289710998535156\n",
            "Evaluating Performance...\n",
            "pass 13550, training loss 15.179676055908203\n",
            "Evaluating Performance...\n",
            "pass 13560, training loss 13.645756721496582\n",
            "Evaluating Performance...\n",
            "pass 13570, training loss 11.229748725891113\n",
            "Evaluating Performance...\n",
            "pass 13580, training loss 8.678289413452148\n",
            "Evaluating Performance...\n",
            "pass 13590, training loss 15.430065155029297\n",
            "Evaluating Performance...\n",
            "pass 13600, training loss 14.930045127868652\n",
            "Evaluating Performance...\n",
            "pass 13610, training loss 12.261540412902832\n",
            "Evaluating Performance...\n",
            "pass 13620, training loss 14.323253631591797\n",
            "Evaluating Performance...\n",
            "pass 13630, training loss 13.550007820129395\n",
            "Evaluating Performance...\n",
            "pass 13640, training loss 12.640596389770508\n",
            "Evaluating Performance...\n",
            "pass 13650, training loss 13.938522338867188\n",
            "Evaluating Performance...\n",
            "pass 13660, training loss 11.249838829040527\n",
            "Evaluating Performance...\n",
            "pass 13670, training loss 12.053900718688965\n",
            "Evaluating Performance...\n",
            "pass 13680, training loss 12.483173370361328\n",
            "Evaluating Performance...\n",
            "pass 13690, training loss 15.059358596801758\n",
            "Evaluating Performance...\n",
            "pass 13700, training loss 11.118252754211426\n",
            "Evaluating Performance...\n",
            "pass 13710, training loss 11.587821960449219\n",
            "Evaluating Performance...\n",
            "pass 13720, training loss 15.91126823425293\n",
            "Evaluating Performance...\n",
            "pass 13730, training loss 13.772114753723145\n",
            "Evaluating Performance...\n",
            "pass 13740, training loss 12.52957820892334\n",
            "Evaluating Performance...\n",
            "pass 13750, training loss 14.60510540008545\n",
            "Evaluating Performance...\n",
            "pass 13760, training loss 10.60582160949707\n",
            "Evaluating Performance...\n",
            "pass 13770, training loss 12.319367408752441\n",
            "Evaluating Performance...\n",
            "pass 13780, training loss 12.188661575317383\n",
            "Evaluating Performance...\n",
            "pass 13790, training loss 14.21241569519043\n",
            "Evaluating Performance...\n",
            "pass 13800, training loss 15.443937301635742\n",
            "Evaluating Performance...\n",
            "pass 13810, training loss 13.196661949157715\n",
            "Evaluating Performance...\n",
            "pass 13820, training loss 13.378449440002441\n",
            "Evaluating Performance...\n",
            "pass 13830, training loss 12.817398071289062\n",
            "Evaluating Performance...\n",
            "pass 13840, training loss 14.556894302368164\n",
            "Evaluating Performance...\n",
            "pass 13850, training loss 11.203362464904785\n",
            "Evaluating Performance...\n",
            "pass 13860, training loss 14.426511764526367\n",
            "Evaluating Performance...\n",
            "pass 13870, training loss 10.946490287780762\n",
            "Evaluating Performance...\n",
            "pass 13880, training loss 13.32952880859375\n",
            "Evaluating Performance...\n",
            "pass 13890, training loss 14.226563453674316\n",
            "Evaluating Performance...\n",
            "pass 13900, training loss 13.967620849609375\n",
            "Evaluating Performance...\n",
            "pass 13910, training loss 12.301151275634766\n",
            "Evaluating Performance...\n",
            "pass 13920, training loss 13.456282615661621\n",
            "Evaluating Performance...\n",
            "pass 13930, training loss 12.213581085205078\n",
            "Evaluating Performance...\n",
            "pass 13940, training loss 13.20165729522705\n",
            "Evaluating Performance...\n",
            "pass 13950, training loss 11.651436805725098\n",
            "Evaluating Performance...\n",
            "pass 13960, training loss 13.162355422973633\n",
            "Evaluating Performance...\n",
            "pass 13970, training loss 11.45598030090332\n",
            "Evaluating Performance...\n",
            "pass 13980, training loss 17.010950088500977\n",
            "Evaluating Performance...\n",
            "pass 13990, training loss 11.690214157104492\n",
            "Evaluating Performance...\n",
            "pass 14000, training loss 9.678131103515625\n",
            "Evaluating Performance...\n",
            "pass 14010, training loss 11.366329193115234\n",
            "Evaluating Performance...\n",
            "pass 14020, training loss 15.77040958404541\n",
            "Evaluating Performance...\n",
            "pass 14030, training loss 10.847212791442871\n",
            "Evaluating Performance...\n",
            "pass 14040, training loss 14.599157333374023\n",
            "Evaluating Performance...\n",
            "pass 14050, training loss 14.88596248626709\n",
            "Evaluating Performance...\n",
            "pass 14060, training loss 10.745058059692383\n",
            "Evaluating Performance...\n",
            "pass 14070, training loss 12.441351890563965\n",
            "Evaluating Performance...\n",
            "pass 14080, training loss 11.692558288574219\n",
            "Evaluating Performance...\n",
            "pass 14090, training loss 11.859158515930176\n",
            "Evaluating Performance...\n",
            "pass 14100, training loss 11.609697341918945\n",
            "Evaluating Performance...\n",
            "pass 14110, training loss 11.33851146697998\n",
            "Evaluating Performance...\n",
            "pass 14120, training loss 11.688363075256348\n",
            "Evaluating Performance...\n",
            "pass 14130, training loss 12.384081840515137\n",
            "Evaluating Performance...\n",
            "pass 14140, training loss 11.54302978515625\n",
            "Evaluating Performance...\n",
            "pass 14150, training loss 12.937506675720215\n",
            "Evaluating Performance...\n",
            "pass 14160, training loss 13.21163272857666\n",
            "Evaluating Performance...\n",
            "pass 14170, training loss 13.908788681030273\n",
            "Evaluating Performance...\n",
            "pass 14180, training loss 12.81915283203125\n",
            "Evaluating Performance...\n",
            "pass 14190, training loss 15.490030288696289\n",
            "Evaluating Performance...\n",
            "pass 14200, training loss 11.799591064453125\n",
            "Evaluating Performance...\n",
            "pass 14210, training loss 11.537115097045898\n",
            "Evaluating Performance...\n",
            "pass 14220, training loss 10.882447242736816\n",
            "Evaluating Performance...\n",
            "pass 14230, training loss 14.488719940185547\n",
            "Evaluating Performance...\n",
            "pass 14240, training loss 13.369888305664062\n",
            "Evaluating Performance...\n",
            "pass 14250, training loss 12.315465927124023\n",
            "Evaluating Performance...\n",
            "pass 14260, training loss 12.795394897460938\n",
            "Evaluating Performance...\n",
            "pass 14270, training loss 12.12646484375\n",
            "Evaluating Performance...\n",
            "pass 14280, training loss 14.8832426071167\n",
            "Evaluating Performance...\n",
            "pass 14290, training loss 14.005926132202148\n",
            "Evaluating Performance...\n",
            "pass 14300, training loss 18.020681381225586\n",
            "Evaluating Performance...\n",
            "pass 14310, training loss 14.454509735107422\n",
            "Evaluating Performance...\n",
            "pass 14320, training loss 11.874232292175293\n",
            "Evaluating Performance...\n",
            "pass 14330, training loss 10.734492301940918\n",
            "Evaluating Performance...\n",
            "pass 14340, training loss 13.965713500976562\n",
            "Evaluating Performance...\n",
            "pass 14350, training loss 12.086527824401855\n",
            "Evaluating Performance...\n",
            "pass 14360, training loss 10.909834861755371\n",
            "Evaluating Performance...\n",
            "pass 14370, training loss 12.934061050415039\n",
            "Evaluating Performance...\n",
            "pass 14380, training loss 13.290828704833984\n",
            "Evaluating Performance...\n",
            "pass 14390, training loss 13.071378707885742\n",
            "Evaluating Performance...\n",
            "pass 14400, training loss 14.051305770874023\n",
            "Evaluating Performance...\n",
            "pass 14410, training loss 9.364043235778809\n",
            "Evaluating Performance...\n",
            "pass 14420, training loss 14.191768646240234\n",
            "Evaluating Performance...\n",
            "pass 14430, training loss 11.76498794555664\n",
            "Evaluating Performance...\n",
            "pass 14440, training loss 13.142953872680664\n",
            "Evaluating Performance...\n",
            "pass 14450, training loss 13.131040573120117\n",
            "Evaluating Performance...\n",
            "pass 14460, training loss 12.593339920043945\n",
            "Evaluating Performance...\n",
            "pass 14470, training loss 10.904969215393066\n",
            "Evaluating Performance...\n",
            "pass 14480, training loss 13.793633460998535\n",
            "Evaluating Performance...\n",
            "pass 14490, training loss 14.70443344116211\n",
            "Evaluating Performance...\n",
            "pass 14500, training loss 13.609545707702637\n",
            "Evaluating Performance...\n",
            "pass 14510, training loss 14.710444450378418\n",
            "Evaluating Performance...\n",
            "pass 14520, training loss 11.15481185913086\n",
            "Evaluating Performance...\n",
            "pass 14530, training loss 18.424951553344727\n",
            "Evaluating Performance...\n",
            "pass 14540, training loss 12.467378616333008\n",
            "Evaluating Performance...\n",
            "pass 14550, training loss 13.84911823272705\n",
            "Evaluating Performance...\n",
            "pass 14560, training loss 10.530928611755371\n",
            "Evaluating Performance...\n",
            "pass 14570, training loss 12.830018997192383\n",
            "Evaluating Performance...\n",
            "pass 14580, training loss 11.003369331359863\n",
            "Evaluating Performance...\n",
            "pass 14590, training loss 11.939986228942871\n",
            "Evaluating Performance...\n",
            "pass 14600, training loss 11.416141510009766\n",
            "Evaluating Performance...\n",
            "pass 14610, training loss 12.307276725769043\n",
            "Evaluating Performance...\n",
            "pass 14620, training loss 10.548582077026367\n",
            "Evaluating Performance...\n",
            "pass 14630, training loss 10.26904296875\n",
            "Evaluating Performance...\n",
            "pass 14640, training loss 10.635848045349121\n",
            "Evaluating Performance...\n",
            "pass 14650, training loss 10.795292854309082\n",
            "Evaluating Performance...\n",
            "pass 14660, training loss 13.368913650512695\n",
            "Evaluating Performance...\n",
            "pass 14670, training loss 12.135377883911133\n",
            "Evaluating Performance...\n",
            "pass 14680, training loss 11.56123161315918\n",
            "Evaluating Performance...\n",
            "pass 14690, training loss 11.367449760437012\n",
            "Evaluating Performance...\n",
            "pass 14700, training loss 13.776289939880371\n",
            "Evaluating Performance...\n",
            "pass 14710, training loss 11.281862258911133\n",
            "Evaluating Performance...\n",
            "pass 14720, training loss 12.65796947479248\n",
            "Evaluating Performance...\n",
            "pass 14730, training loss 10.753369331359863\n",
            "Evaluating Performance...\n",
            "pass 14740, training loss 12.001309394836426\n",
            "Evaluating Performance...\n",
            "pass 14750, training loss 12.83511734008789\n",
            "Evaluating Performance...\n",
            "pass 14760, training loss 11.973711013793945\n",
            "Evaluating Performance...\n",
            "pass 14770, training loss 12.421177864074707\n",
            "Evaluating Performance...\n",
            "pass 14780, training loss 11.554977416992188\n",
            "Evaluating Performance...\n",
            "pass 14790, training loss 9.853365898132324\n",
            "Evaluating Performance...\n",
            "pass 14800, training loss 12.0360689163208\n",
            "Evaluating Performance...\n",
            "pass 14810, training loss 12.26819896697998\n",
            "Evaluating Performance...\n",
            "pass 14820, training loss 14.464717864990234\n",
            "Evaluating Performance...\n",
            "pass 14830, training loss 13.121113777160645\n",
            "Evaluating Performance...\n",
            "pass 14840, training loss 12.757265090942383\n",
            "Evaluating Performance...\n",
            "pass 14850, training loss 12.680254936218262\n",
            "Evaluating Performance...\n",
            "pass 14860, training loss 11.76798152923584\n",
            "Evaluating Performance...\n",
            "pass 14870, training loss 12.911714553833008\n",
            "Evaluating Performance...\n",
            "pass 14880, training loss 10.43967342376709\n",
            "Evaluating Performance...\n",
            "pass 14890, training loss 14.393085479736328\n",
            "Evaluating Performance...\n",
            "pass 14900, training loss 14.413260459899902\n",
            "Evaluating Performance...\n",
            "pass 14910, training loss 11.5779390335083\n",
            "Evaluating Performance...\n",
            "pass 14920, training loss 12.354202270507812\n",
            "Evaluating Performance...\n",
            "pass 14930, training loss 12.313909530639648\n",
            "Evaluating Performance...\n",
            "pass 14940, training loss 15.347919464111328\n",
            "Evaluating Performance...\n",
            "pass 14950, training loss 14.096790313720703\n",
            "Evaluating Performance...\n",
            "pass 14960, training loss 11.849609375\n",
            "Evaluating Performance...\n",
            "pass 14970, training loss 13.622065544128418\n",
            "Evaluating Performance...\n",
            "pass 14980, training loss 10.027135848999023\n",
            "Evaluating Performance...\n",
            "pass 14990, training loss 13.601582527160645\n",
            "Evaluating Performance...\n",
            "pass 15000, training loss 13.059700965881348\n",
            "Evaluating Performance...\n",
            "pass 15010, training loss 11.696696281433105\n",
            "Evaluating Performance...\n",
            "pass 15020, training loss 14.247565269470215\n",
            "Evaluating Performance...\n",
            "pass 15030, training loss 12.286430358886719\n",
            "Evaluating Performance...\n",
            "pass 15040, training loss 14.014580726623535\n",
            "Evaluating Performance...\n",
            "pass 15050, training loss 16.18959617614746\n",
            "Evaluating Performance...\n",
            "pass 15060, training loss 11.232505798339844\n",
            "Evaluating Performance...\n",
            "pass 15070, training loss 13.337772369384766\n",
            "Evaluating Performance...\n",
            "pass 15080, training loss 14.988177299499512\n",
            "Evaluating Performance...\n",
            "pass 15090, training loss 12.867903709411621\n",
            "Evaluating Performance...\n",
            "pass 15100, training loss 10.942858695983887\n",
            "Evaluating Performance...\n",
            "pass 15110, training loss 13.889955520629883\n",
            "Evaluating Performance...\n",
            "pass 15120, training loss 10.33816909790039\n",
            "Evaluating Performance...\n",
            "pass 15130, training loss 12.427703857421875\n",
            "Evaluating Performance...\n",
            "pass 15140, training loss 13.047164916992188\n",
            "Evaluating Performance...\n",
            "pass 15150, training loss 11.520435333251953\n",
            "Evaluating Performance...\n",
            "pass 15160, training loss 11.80659294128418\n",
            "Evaluating Performance...\n",
            "pass 15170, training loss 10.899188995361328\n",
            "Evaluating Performance...\n",
            "pass 15180, training loss 9.951360702514648\n",
            "Evaluating Performance...\n",
            "pass 15190, training loss 12.000619888305664\n",
            "Evaluating Performance...\n",
            "pass 15200, training loss 14.552082061767578\n",
            "Evaluating Performance...\n",
            "pass 15210, training loss 14.242716789245605\n",
            "Evaluating Performance...\n",
            "pass 15220, training loss 10.824155807495117\n",
            "Evaluating Performance...\n",
            "pass 15230, training loss 13.535174369812012\n",
            "Evaluating Performance...\n",
            "pass 15240, training loss 9.344843864440918\n",
            "Evaluating Performance...\n",
            "pass 15250, training loss 13.61516284942627\n",
            "Evaluating Performance...\n",
            "pass 15260, training loss 10.506902694702148\n",
            "Evaluating Performance...\n",
            "pass 15270, training loss 11.586983680725098\n",
            "Evaluating Performance...\n",
            "pass 15280, training loss 13.184078216552734\n",
            "Evaluating Performance...\n",
            "pass 15290, training loss 10.953836441040039\n",
            "Evaluating Performance...\n",
            "pass 15300, training loss 12.6763916015625\n",
            "Evaluating Performance...\n",
            "pass 15310, training loss 11.017003059387207\n",
            "Evaluating Performance...\n",
            "pass 15320, training loss 13.312195777893066\n",
            "Evaluating Performance...\n",
            "pass 15330, training loss 11.440600395202637\n",
            "Evaluating Performance...\n",
            "pass 15340, training loss 13.81469440460205\n",
            "Evaluating Performance...\n",
            "pass 15350, training loss 11.316619873046875\n",
            "Evaluating Performance...\n",
            "pass 15360, training loss 15.8015718460083\n",
            "Evaluating Performance...\n",
            "pass 15370, training loss 11.446609497070312\n",
            "Evaluating Performance...\n",
            "pass 15380, training loss 14.153693199157715\n",
            "Evaluating Performance...\n",
            "pass 15390, training loss 13.931163787841797\n",
            "Evaluating Performance...\n",
            "pass 15400, training loss 12.47596549987793\n",
            "Evaluating Performance...\n",
            "pass 15410, training loss 14.973200798034668\n",
            "Evaluating Performance...\n",
            "pass 15420, training loss 13.542001724243164\n",
            "Evaluating Performance...\n",
            "pass 15430, training loss 11.908522605895996\n",
            "Evaluating Performance...\n",
            "pass 15440, training loss 12.327201843261719\n",
            "Evaluating Performance...\n",
            "pass 15450, training loss 10.956085205078125\n",
            "Evaluating Performance...\n",
            "pass 15460, training loss 9.103535652160645\n",
            "Evaluating Performance...\n",
            "pass 15470, training loss 12.084722518920898\n",
            "Evaluating Performance...\n",
            "pass 15480, training loss 11.242140769958496\n",
            "Evaluating Performance...\n",
            "pass 15490, training loss 11.196964263916016\n",
            "Evaluating Performance...\n",
            "pass 15500, training loss 12.239188194274902\n",
            "Evaluating Performance...\n",
            "pass 15510, training loss 11.668479919433594\n",
            "Evaluating Performance...\n",
            "pass 15520, training loss 10.334221839904785\n",
            "Evaluating Performance...\n",
            "pass 15530, training loss 16.29343032836914\n",
            "Evaluating Performance...\n",
            "pass 15540, training loss 11.174827575683594\n",
            "Evaluating Performance...\n",
            "pass 15550, training loss 13.886759757995605\n",
            "Evaluating Performance...\n",
            "pass 15560, training loss 12.945585250854492\n",
            "Evaluating Performance...\n",
            "pass 15570, training loss 11.888650894165039\n",
            "Evaluating Performance...\n",
            "pass 15580, training loss 11.296772003173828\n",
            "Evaluating Performance...\n",
            "pass 15590, training loss 13.72304916381836\n",
            "Evaluating Performance...\n",
            "pass 15600, training loss 15.031569480895996\n",
            "Evaluating Performance...\n",
            "pass 15610, training loss 12.810392379760742\n",
            "Evaluating Performance...\n",
            "pass 15620, training loss 11.209773063659668\n",
            "Evaluating Performance...\n",
            "pass 15630, training loss 16.087125778198242\n",
            "Evaluating Performance...\n",
            "pass 15640, training loss 15.601324081420898\n",
            "Evaluating Performance...\n",
            "pass 15650, training loss 13.105216979980469\n",
            "Evaluating Performance...\n",
            "pass 15660, training loss 11.804901123046875\n",
            "Evaluating Performance...\n",
            "pass 15670, training loss 11.049636840820312\n",
            "Evaluating Performance...\n",
            "pass 15680, training loss 13.059393882751465\n",
            "Evaluating Performance...\n",
            "pass 15690, training loss 12.0144624710083\n",
            "Evaluating Performance...\n",
            "pass 15700, training loss 13.579258918762207\n",
            "Evaluating Performance...\n",
            "pass 15710, training loss 11.963846206665039\n",
            "Evaluating Performance...\n",
            "pass 15720, training loss 11.472869873046875\n",
            "Evaluating Performance...\n",
            "pass 15730, training loss 11.426735877990723\n",
            "Evaluating Performance...\n",
            "pass 15740, training loss 13.548158645629883\n",
            "Evaluating Performance...\n",
            "pass 15750, training loss 14.680262565612793\n",
            "Evaluating Performance...\n",
            "pass 15760, training loss 10.857938766479492\n",
            "Evaluating Performance...\n",
            "pass 15770, training loss 13.258628845214844\n",
            "Evaluating Performance...\n",
            "pass 15780, training loss 9.065450668334961\n",
            "Evaluating Performance...\n",
            "pass 15790, training loss 12.27199935913086\n",
            "Evaluating Performance...\n",
            "pass 15800, training loss 11.807178497314453\n",
            "Evaluating Performance...\n",
            "pass 15810, training loss 13.508965492248535\n",
            "Evaluating Performance...\n",
            "pass 15820, training loss 19.13715934753418\n",
            "Evaluating Performance...\n",
            "pass 15830, training loss 12.366435050964355\n",
            "Evaluating Performance...\n",
            "pass 15840, training loss 11.805411338806152\n",
            "Evaluating Performance...\n",
            "pass 15850, training loss 13.094554901123047\n",
            "Evaluating Performance...\n",
            "pass 15860, training loss 11.957769393920898\n",
            "Evaluating Performance...\n",
            "pass 15870, training loss 10.810897827148438\n",
            "Evaluating Performance...\n",
            "pass 15880, training loss 11.943024635314941\n",
            "Evaluating Performance...\n",
            "pass 15890, training loss 12.394431114196777\n",
            "Evaluating Performance...\n",
            "pass 15900, training loss 13.419594764709473\n",
            "Evaluating Performance...\n",
            "pass 15910, training loss 12.791407585144043\n",
            "Evaluating Performance...\n",
            "pass 15920, training loss 11.811187744140625\n",
            "Evaluating Performance...\n",
            "pass 15930, training loss 11.781761169433594\n",
            "Evaluating Performance...\n",
            "pass 15940, training loss 10.782038688659668\n",
            "Evaluating Performance...\n",
            "pass 15950, training loss 13.222990036010742\n",
            "Evaluating Performance...\n",
            "pass 15960, training loss 14.75722599029541\n",
            "Evaluating Performance...\n",
            "pass 15970, training loss 16.532838821411133\n",
            "Evaluating Performance...\n",
            "pass 15980, training loss 12.795345306396484\n",
            "Evaluating Performance...\n",
            "pass 15990, training loss 10.841971397399902\n",
            "Evaluating Performance...\n",
            "pass 16000, training loss 13.617048263549805\n",
            "Evaluating Performance...\n",
            "pass 16010, training loss 11.20037841796875\n",
            "Evaluating Performance...\n",
            "pass 16020, training loss 11.79462718963623\n",
            "Evaluating Performance...\n",
            "pass 16030, training loss 11.391973495483398\n",
            "Evaluating Performance...\n",
            "pass 16040, training loss 11.951033592224121\n",
            "Evaluating Performance...\n",
            "pass 16050, training loss 13.58493423461914\n",
            "Evaluating Performance...\n",
            "pass 16060, training loss 12.242901802062988\n",
            "Evaluating Performance...\n",
            "pass 16070, training loss 11.675376892089844\n",
            "Evaluating Performance...\n",
            "pass 16080, training loss 12.155108451843262\n",
            "Evaluating Performance...\n",
            "pass 16090, training loss 10.278069496154785\n",
            "Evaluating Performance...\n",
            "pass 16100, training loss 10.822942733764648\n",
            "Evaluating Performance...\n",
            "pass 16110, training loss 12.161697387695312\n",
            "Evaluating Performance...\n",
            "pass 16120, training loss 11.892792701721191\n",
            "Evaluating Performance...\n",
            "pass 16130, training loss 13.174473762512207\n",
            "Evaluating Performance...\n",
            "pass 16140, training loss 14.226419448852539\n",
            "Evaluating Performance...\n",
            "pass 16150, training loss 11.821634292602539\n",
            "Evaluating Performance...\n",
            "pass 16160, training loss 9.95359992980957\n",
            "Evaluating Performance...\n",
            "pass 16170, training loss 12.39570426940918\n",
            "Evaluating Performance...\n",
            "pass 16180, training loss 10.737862586975098\n",
            "Evaluating Performance...\n",
            "pass 16190, training loss 13.41472053527832\n",
            "Evaluating Performance...\n",
            "pass 16200, training loss 12.566032409667969\n",
            "Evaluating Performance...\n",
            "pass 16210, training loss 14.096529006958008\n",
            "Evaluating Performance...\n",
            "pass 16220, training loss 12.503915786743164\n",
            "Evaluating Performance...\n",
            "pass 16230, training loss 9.00203800201416\n",
            "Evaluating Performance...\n",
            "pass 16240, training loss 12.794570922851562\n",
            "Evaluating Performance...\n",
            "pass 16250, training loss 13.104494094848633\n",
            "Evaluating Performance...\n",
            "pass 16260, training loss 10.697964668273926\n",
            "Evaluating Performance...\n",
            "pass 16270, training loss 11.255846977233887\n",
            "Evaluating Performance...\n",
            "pass 16280, training loss 11.761594772338867\n",
            "Evaluating Performance...\n",
            "pass 16290, training loss 10.782541275024414\n",
            "Evaluating Performance...\n",
            "pass 16300, training loss 10.944162368774414\n",
            "Evaluating Performance...\n",
            "pass 16310, training loss 12.264474868774414\n",
            "Evaluating Performance...\n",
            "pass 16320, training loss 12.877062797546387\n",
            "Evaluating Performance...\n",
            "pass 16330, training loss 13.823843955993652\n",
            "Evaluating Performance...\n",
            "pass 16340, training loss 11.039429664611816\n",
            "Evaluating Performance...\n",
            "pass 16350, training loss 10.107661247253418\n",
            "Evaluating Performance...\n",
            "pass 16360, training loss 11.269204139709473\n",
            "Evaluating Performance...\n",
            "pass 16370, training loss 13.09667682647705\n",
            "Evaluating Performance...\n",
            "pass 16380, training loss 9.98067569732666\n",
            "Evaluating Performance...\n",
            "pass 16390, training loss 12.743120193481445\n",
            "Evaluating Performance...\n",
            "pass 16400, training loss 11.869874000549316\n",
            "Evaluating Performance...\n",
            "pass 16410, training loss 10.361673355102539\n",
            "Evaluating Performance...\n",
            "pass 16420, training loss 13.14530086517334\n",
            "Evaluating Performance...\n",
            "pass 16430, training loss 13.180070877075195\n",
            "Evaluating Performance...\n",
            "pass 16440, training loss 12.937156677246094\n",
            "Evaluating Performance...\n",
            "pass 16450, training loss 11.190483093261719\n",
            "Evaluating Performance...\n",
            "pass 16460, training loss 10.774934768676758\n",
            "Evaluating Performance...\n",
            "pass 16470, training loss 13.61081600189209\n",
            "Evaluating Performance...\n",
            "pass 16480, training loss 12.08585262298584\n",
            "Evaluating Performance...\n",
            "pass 16490, training loss 10.925716400146484\n",
            "Evaluating Performance...\n",
            "pass 16500, training loss 9.426478385925293\n",
            "Evaluating Performance...\n",
            "pass 16510, training loss 13.7518310546875\n",
            "Evaluating Performance...\n",
            "pass 16520, training loss 12.78425121307373\n",
            "Evaluating Performance...\n",
            "pass 16530, training loss 12.698704719543457\n",
            "Evaluating Performance...\n",
            "pass 16540, training loss 10.764077186584473\n",
            "Evaluating Performance...\n",
            "pass 16550, training loss 12.468889236450195\n",
            "Evaluating Performance...\n",
            "pass 16560, training loss 9.536191940307617\n",
            "Evaluating Performance...\n",
            "pass 16570, training loss 12.441869735717773\n",
            "Evaluating Performance...\n",
            "pass 16580, training loss 12.251940727233887\n",
            "Evaluating Performance...\n",
            "pass 16590, training loss 13.026326179504395\n",
            "Evaluating Performance...\n",
            "pass 16600, training loss 12.925423622131348\n",
            "Evaluating Performance...\n",
            "pass 16610, training loss 11.076663970947266\n",
            "Evaluating Performance...\n",
            "pass 16620, training loss 10.538046836853027\n",
            "Evaluating Performance...\n",
            "pass 16630, training loss 8.375506401062012\n",
            "Evaluating Performance...\n",
            "pass 16640, training loss 11.519036293029785\n",
            "Evaluating Performance...\n",
            "pass 16650, training loss 11.691695213317871\n",
            "Evaluating Performance...\n",
            "pass 16660, training loss 11.832807540893555\n",
            "Evaluating Performance...\n",
            "pass 16670, training loss 13.396862030029297\n",
            "Evaluating Performance...\n",
            "pass 16680, training loss 10.790398597717285\n",
            "Evaluating Performance...\n",
            "pass 16690, training loss 10.769139289855957\n",
            "Evaluating Performance...\n",
            "pass 16700, training loss 10.246734619140625\n",
            "Evaluating Performance...\n",
            "pass 16710, training loss 11.60905647277832\n",
            "Evaluating Performance...\n",
            "pass 16720, training loss 11.342851638793945\n",
            "Evaluating Performance...\n",
            "pass 16730, training loss 12.297922134399414\n",
            "Evaluating Performance...\n",
            "pass 16740, training loss 10.22664737701416\n",
            "Evaluating Performance...\n",
            "pass 16750, training loss 11.84807014465332\n",
            "Evaluating Performance...\n",
            "pass 16760, training loss 9.919296264648438\n",
            "Evaluating Performance...\n",
            "pass 16770, training loss 11.756975173950195\n",
            "Evaluating Performance...\n",
            "pass 16780, training loss 12.079573631286621\n",
            "Evaluating Performance...\n",
            "pass 16790, training loss 12.538225173950195\n",
            "Evaluating Performance...\n",
            "pass 16800, training loss 12.223621368408203\n",
            "Evaluating Performance...\n",
            "pass 16810, training loss 10.695801734924316\n",
            "Evaluating Performance...\n",
            "pass 16820, training loss 11.541895866394043\n",
            "Evaluating Performance...\n",
            "pass 16830, training loss 12.50849437713623\n",
            "Evaluating Performance...\n",
            "pass 16840, training loss 11.408514022827148\n",
            "Evaluating Performance...\n",
            "pass 16850, training loss 10.83443832397461\n",
            "Evaluating Performance...\n",
            "pass 16860, training loss 13.895645141601562\n",
            "Evaluating Performance...\n",
            "pass 16870, training loss 11.517562866210938\n",
            "Evaluating Performance...\n",
            "pass 16880, training loss 10.665487289428711\n",
            "Evaluating Performance...\n",
            "pass 16890, training loss 13.672243118286133\n",
            "Evaluating Performance...\n",
            "pass 16900, training loss 12.672572135925293\n",
            "Evaluating Performance...\n",
            "pass 16910, training loss 10.367069244384766\n",
            "Evaluating Performance...\n",
            "pass 16920, training loss 11.840014457702637\n",
            "Evaluating Performance...\n",
            "pass 16930, training loss 9.58441162109375\n",
            "Evaluating Performance...\n",
            "pass 16940, training loss 12.414666175842285\n",
            "Evaluating Performance...\n",
            "pass 16950, training loss 10.933816909790039\n",
            "Evaluating Performance...\n",
            "pass 16960, training loss 9.786090850830078\n",
            "Evaluating Performance...\n",
            "pass 16970, training loss 12.184588432312012\n",
            "Evaluating Performance...\n",
            "pass 16980, training loss 11.51797866821289\n",
            "Evaluating Performance...\n",
            "pass 16990, training loss 10.98491096496582\n",
            "Evaluating Performance...\n",
            "pass 17000, training loss 8.69362735748291\n",
            "Evaluating Performance...\n",
            "pass 17010, training loss 9.123412132263184\n",
            "Evaluating Performance...\n",
            "pass 17020, training loss 15.33797550201416\n",
            "Evaluating Performance...\n",
            "pass 17030, training loss 9.245835304260254\n",
            "Evaluating Performance...\n",
            "pass 17040, training loss 12.158160209655762\n",
            "Evaluating Performance...\n",
            "pass 17050, training loss 11.894322395324707\n",
            "Evaluating Performance...\n",
            "pass 17060, training loss 8.955000877380371\n",
            "Evaluating Performance...\n",
            "pass 17070, training loss 14.214558601379395\n",
            "Evaluating Performance...\n",
            "pass 17080, training loss 10.237971305847168\n",
            "Evaluating Performance...\n",
            "pass 17090, training loss 11.937894821166992\n",
            "Evaluating Performance...\n",
            "pass 17100, training loss 13.828807830810547\n",
            "Evaluating Performance...\n",
            "pass 17110, training loss 13.199524879455566\n",
            "Evaluating Performance...\n",
            "pass 17120, training loss 10.508352279663086\n",
            "Evaluating Performance...\n",
            "pass 17130, training loss 15.048807144165039\n",
            "Evaluating Performance...\n",
            "pass 17140, training loss 10.893402099609375\n",
            "Evaluating Performance...\n",
            "pass 17150, training loss 10.062228202819824\n",
            "Evaluating Performance...\n",
            "pass 17160, training loss 10.918998718261719\n",
            "Evaluating Performance...\n",
            "pass 17170, training loss 12.095396041870117\n",
            "Evaluating Performance...\n",
            "pass 17180, training loss 10.325850486755371\n",
            "Evaluating Performance...\n",
            "pass 17190, training loss 11.22793960571289\n",
            "Evaluating Performance...\n",
            "pass 17200, training loss 10.2658052444458\n",
            "Evaluating Performance...\n",
            "pass 17210, training loss 10.398417472839355\n",
            "Evaluating Performance...\n",
            "pass 17220, training loss 13.776546478271484\n",
            "Evaluating Performance...\n",
            "pass 17230, training loss 9.53067398071289\n",
            "Evaluating Performance...\n",
            "pass 17240, training loss 11.15224552154541\n",
            "Evaluating Performance...\n",
            "pass 17250, training loss 14.185190200805664\n",
            "Evaluating Performance...\n",
            "pass 17260, training loss 12.415358543395996\n",
            "Evaluating Performance...\n",
            "pass 17270, training loss 11.251997947692871\n",
            "Evaluating Performance...\n",
            "pass 17280, training loss 11.057740211486816\n",
            "Evaluating Performance...\n",
            "pass 17290, training loss 11.453689575195312\n",
            "Evaluating Performance...\n",
            "pass 17300, training loss 12.94975471496582\n",
            "Evaluating Performance...\n",
            "pass 17310, training loss 13.5355224609375\n",
            "Evaluating Performance...\n",
            "pass 17320, training loss 12.751798629760742\n",
            "Evaluating Performance...\n",
            "pass 17330, training loss 11.326080322265625\n",
            "Evaluating Performance...\n",
            "pass 17340, training loss 8.447071075439453\n",
            "Evaluating Performance...\n",
            "pass 17350, training loss 10.122794151306152\n",
            "Evaluating Performance...\n",
            "pass 17360, training loss 11.301639556884766\n",
            "Evaluating Performance...\n",
            "pass 17370, training loss 10.267426490783691\n",
            "Evaluating Performance...\n",
            "pass 17380, training loss 13.505533218383789\n",
            "Evaluating Performance...\n",
            "pass 17390, training loss 9.275618553161621\n",
            "Evaluating Performance...\n",
            "pass 17400, training loss 8.97293472290039\n",
            "Evaluating Performance...\n",
            "pass 17410, training loss 11.741925239562988\n",
            "Evaluating Performance...\n",
            "pass 17420, training loss 12.259714126586914\n",
            "Evaluating Performance...\n",
            "pass 17430, training loss 9.901128768920898\n",
            "Evaluating Performance...\n",
            "pass 17440, training loss 12.052830696105957\n",
            "Evaluating Performance...\n",
            "pass 17450, training loss 12.223567962646484\n",
            "Evaluating Performance...\n",
            "pass 17460, training loss 9.890630722045898\n",
            "Evaluating Performance...\n",
            "pass 17470, training loss 10.028228759765625\n",
            "Evaluating Performance...\n",
            "pass 17480, training loss 12.670275688171387\n",
            "Evaluating Performance...\n",
            "pass 17490, training loss 10.454005241394043\n",
            "Evaluating Performance...\n",
            "pass 17500, training loss 12.3218994140625\n",
            "Evaluating Performance...\n",
            "pass 17510, training loss 14.12752628326416\n",
            "Evaluating Performance...\n",
            "pass 17520, training loss 9.392523765563965\n",
            "Evaluating Performance...\n",
            "pass 17530, training loss 9.131522178649902\n",
            "Evaluating Performance...\n",
            "pass 17540, training loss 9.250344276428223\n",
            "Evaluating Performance...\n",
            "pass 17550, training loss 10.697697639465332\n",
            "Evaluating Performance...\n",
            "pass 17560, training loss 9.887970924377441\n",
            "Evaluating Performance...\n",
            "pass 17570, training loss 12.591193199157715\n",
            "Evaluating Performance...\n",
            "pass 17580, training loss 10.81424331665039\n",
            "Evaluating Performance...\n",
            "pass 17590, training loss 11.171281814575195\n",
            "Evaluating Performance...\n",
            "pass 17600, training loss 12.31198787689209\n",
            "Evaluating Performance...\n",
            "pass 17610, training loss 14.244386672973633\n",
            "Evaluating Performance...\n",
            "pass 17620, training loss 10.329459190368652\n",
            "Evaluating Performance...\n",
            "pass 17630, training loss 12.990556716918945\n",
            "Evaluating Performance...\n",
            "pass 17640, training loss 13.346016883850098\n",
            "Evaluating Performance...\n",
            "pass 17650, training loss 10.881436347961426\n",
            "Evaluating Performance...\n",
            "pass 17660, training loss 11.185972213745117\n",
            "Evaluating Performance...\n",
            "pass 17670, training loss 11.72359848022461\n",
            "Evaluating Performance...\n",
            "pass 17680, training loss 10.549884796142578\n",
            "Evaluating Performance...\n",
            "pass 17690, training loss 10.498927116394043\n",
            "Evaluating Performance...\n",
            "pass 17700, training loss 10.842803955078125\n",
            "Evaluating Performance...\n",
            "pass 17710, training loss 8.321138381958008\n",
            "Evaluating Performance...\n",
            "pass 17720, training loss 13.535475730895996\n",
            "Evaluating Performance...\n",
            "pass 17730, training loss 12.749395370483398\n",
            "Evaluating Performance...\n",
            "pass 17740, training loss 12.958640098571777\n",
            "Evaluating Performance...\n",
            "pass 17750, training loss 11.975725173950195\n",
            "Evaluating Performance...\n",
            "pass 17760, training loss 11.483465194702148\n",
            "Evaluating Performance...\n",
            "pass 17770, training loss 9.24814510345459\n",
            "Evaluating Performance...\n",
            "pass 17780, training loss 11.743417739868164\n",
            "Evaluating Performance...\n",
            "pass 17790, training loss 13.196216583251953\n",
            "Evaluating Performance...\n",
            "pass 17800, training loss 12.11031436920166\n",
            "Evaluating Performance...\n",
            "pass 17810, training loss 11.243888854980469\n",
            "Evaluating Performance...\n",
            "pass 17820, training loss 12.674595832824707\n",
            "Evaluating Performance...\n",
            "pass 17830, training loss 11.016437530517578\n",
            "Evaluating Performance...\n",
            "pass 17840, training loss 7.679965019226074\n",
            "Evaluating Performance...\n",
            "pass 17850, training loss 11.96706771850586\n",
            "Evaluating Performance...\n",
            "pass 17860, training loss 12.979357719421387\n",
            "Evaluating Performance...\n",
            "pass 17870, training loss 10.235563278198242\n",
            "Evaluating Performance...\n",
            "pass 17880, training loss 9.94775390625\n",
            "Evaluating Performance...\n",
            "pass 17890, training loss 11.838086128234863\n",
            "Evaluating Performance...\n",
            "pass 17900, training loss 10.02752685546875\n",
            "Evaluating Performance...\n",
            "pass 17910, training loss 11.27377986907959\n",
            "Evaluating Performance...\n",
            "pass 17920, training loss 11.47869873046875\n",
            "Evaluating Performance...\n",
            "pass 17930, training loss 12.2184419631958\n",
            "Evaluating Performance...\n",
            "pass 17940, training loss 10.841917037963867\n",
            "Evaluating Performance...\n",
            "pass 17950, training loss 11.151127815246582\n",
            "Evaluating Performance...\n",
            "pass 17960, training loss 12.80192756652832\n",
            "Evaluating Performance...\n",
            "pass 17970, training loss 10.84875202178955\n",
            "Evaluating Performance...\n",
            "pass 17980, training loss 8.7758150100708\n",
            "Evaluating Performance...\n",
            "pass 17990, training loss 11.587347984313965\n",
            "Evaluating Performance...\n",
            "pass 18000, training loss 11.17382526397705\n",
            "Evaluating Performance...\n",
            "pass 18010, training loss 11.745192527770996\n",
            "Evaluating Performance...\n",
            "pass 18020, training loss 10.566167831420898\n",
            "Evaluating Performance...\n",
            "pass 18030, training loss 9.330387115478516\n",
            "Evaluating Performance...\n",
            "pass 18040, training loss 14.449014663696289\n",
            "Evaluating Performance...\n",
            "pass 18050, training loss 10.43979263305664\n",
            "Evaluating Performance...\n",
            "pass 18060, training loss 9.74019718170166\n",
            "Evaluating Performance...\n",
            "pass 18070, training loss 10.163957595825195\n",
            "Evaluating Performance...\n",
            "pass 18080, training loss 12.570551872253418\n",
            "Evaluating Performance...\n",
            "pass 18090, training loss 12.866673469543457\n",
            "Evaluating Performance...\n",
            "pass 18100, training loss 11.688102722167969\n",
            "Evaluating Performance...\n",
            "pass 18110, training loss 11.185206413269043\n",
            "Evaluating Performance...\n",
            "pass 18120, training loss 12.541873931884766\n",
            "Evaluating Performance...\n",
            "pass 18130, training loss 9.9610013961792\n",
            "Evaluating Performance...\n",
            "pass 18140, training loss 10.43704891204834\n",
            "Evaluating Performance...\n",
            "pass 18150, training loss 11.83588981628418\n",
            "Evaluating Performance...\n",
            "pass 18160, training loss 9.42014217376709\n",
            "Evaluating Performance...\n",
            "pass 18170, training loss 11.214823722839355\n",
            "Evaluating Performance...\n",
            "pass 18180, training loss 12.224331855773926\n",
            "Evaluating Performance...\n",
            "pass 18190, training loss 10.959867477416992\n",
            "Evaluating Performance...\n",
            "pass 18200, training loss 7.509035587310791\n",
            "Evaluating Performance...\n",
            "pass 18210, training loss 10.85647964477539\n",
            "Evaluating Performance...\n",
            "pass 18220, training loss 13.297144889831543\n",
            "Evaluating Performance...\n",
            "pass 18230, training loss 11.050312042236328\n",
            "Evaluating Performance...\n",
            "pass 18240, training loss 11.712450981140137\n",
            "Evaluating Performance...\n",
            "pass 18250, training loss 11.535310745239258\n",
            "Evaluating Performance...\n",
            "pass 18260, training loss 11.523481369018555\n",
            "Evaluating Performance...\n",
            "pass 18270, training loss 14.000529289245605\n",
            "Evaluating Performance...\n",
            "pass 18280, training loss 10.551009178161621\n",
            "Evaluating Performance...\n",
            "pass 18290, training loss 10.75202751159668\n",
            "Evaluating Performance...\n",
            "pass 18300, training loss 9.16675853729248\n",
            "Evaluating Performance...\n",
            "pass 18310, training loss 13.045854568481445\n",
            "Evaluating Performance...\n",
            "pass 18320, training loss 11.223905563354492\n",
            "Evaluating Performance...\n",
            "pass 18330, training loss 11.370190620422363\n",
            "Evaluating Performance...\n",
            "pass 18340, training loss 12.585336685180664\n",
            "Evaluating Performance...\n",
            "pass 18350, training loss 8.7465181350708\n",
            "Evaluating Performance...\n",
            "pass 18360, training loss 10.6337890625\n",
            "Evaluating Performance...\n",
            "pass 18370, training loss 8.729019165039062\n",
            "Evaluating Performance...\n",
            "pass 18380, training loss 11.448182106018066\n",
            "Evaluating Performance...\n",
            "pass 18390, training loss 14.686728477478027\n",
            "Evaluating Performance...\n",
            "pass 18400, training loss 8.547592163085938\n",
            "Evaluating Performance...\n",
            "pass 18410, training loss 11.981475830078125\n",
            "Evaluating Performance...\n",
            "pass 18420, training loss 9.692061424255371\n",
            "Evaluating Performance...\n",
            "pass 18430, training loss 12.185487747192383\n",
            "Evaluating Performance...\n",
            "pass 18440, training loss 10.941668510437012\n",
            "Evaluating Performance...\n",
            "pass 18450, training loss 8.691915512084961\n",
            "Evaluating Performance...\n",
            "pass 18460, training loss 11.48808479309082\n",
            "Evaluating Performance...\n",
            "pass 18470, training loss 8.96036148071289\n",
            "Evaluating Performance...\n",
            "pass 18480, training loss 10.47462272644043\n",
            "Evaluating Performance...\n",
            "pass 18490, training loss 12.939675331115723\n",
            "Evaluating Performance...\n",
            "pass 18500, training loss 11.479470252990723\n",
            "Evaluating Performance...\n",
            "pass 18510, training loss 9.099831581115723\n",
            "Evaluating Performance...\n",
            "pass 18520, training loss 10.970739364624023\n",
            "Evaluating Performance...\n",
            "pass 18530, training loss 9.253443717956543\n",
            "Evaluating Performance...\n",
            "pass 18540, training loss 11.058167457580566\n",
            "Evaluating Performance...\n",
            "pass 18550, training loss 12.480231285095215\n",
            "Evaluating Performance...\n",
            "pass 18560, training loss 11.198339462280273\n",
            "Evaluating Performance...\n",
            "pass 18570, training loss 9.396772384643555\n",
            "Evaluating Performance...\n",
            "pass 18580, training loss 9.80459976196289\n",
            "Evaluating Performance...\n",
            "pass 18590, training loss 12.413477897644043\n",
            "Evaluating Performance...\n",
            "pass 18600, training loss 13.459966659545898\n",
            "Evaluating Performance...\n",
            "pass 18610, training loss 11.994730949401855\n",
            "Evaluating Performance...\n",
            "pass 18620, training loss 9.784595489501953\n",
            "Evaluating Performance...\n",
            "pass 18630, training loss 9.591761589050293\n",
            "Evaluating Performance...\n",
            "pass 18640, training loss 10.330972671508789\n",
            "Evaluating Performance...\n",
            "pass 18650, training loss 10.309792518615723\n",
            "Evaluating Performance...\n",
            "pass 18660, training loss 13.936118125915527\n",
            "Evaluating Performance...\n",
            "pass 18670, training loss 13.429779052734375\n",
            "Evaluating Performance...\n",
            "pass 18680, training loss 8.508234024047852\n",
            "Evaluating Performance...\n",
            "pass 18690, training loss 11.241801261901855\n",
            "Evaluating Performance...\n",
            "pass 18700, training loss 11.704687118530273\n",
            "Evaluating Performance...\n",
            "pass 18710, training loss 9.052350044250488\n",
            "Evaluating Performance...\n",
            "pass 18720, training loss 10.749181747436523\n",
            "Evaluating Performance...\n",
            "pass 18730, training loss 9.77951717376709\n",
            "Evaluating Performance...\n",
            "pass 18740, training loss 15.07665729522705\n",
            "Evaluating Performance...\n",
            "pass 18750, training loss 10.537318229675293\n",
            "Evaluating Performance...\n",
            "pass 18760, training loss 9.62922191619873\n",
            "Evaluating Performance...\n",
            "pass 18770, training loss 9.03353500366211\n",
            "Evaluating Performance...\n",
            "pass 18780, training loss 12.480648040771484\n",
            "Evaluating Performance...\n",
            "pass 18790, training loss 12.513575553894043\n",
            "Evaluating Performance...\n",
            "pass 18800, training loss 13.52364444732666\n",
            "Evaluating Performance...\n",
            "pass 18810, training loss 9.920145988464355\n",
            "Evaluating Performance...\n",
            "pass 18820, training loss 13.652908325195312\n",
            "Evaluating Performance...\n",
            "pass 18830, training loss 10.948859214782715\n",
            "Evaluating Performance...\n",
            "pass 18840, training loss 9.734087944030762\n",
            "Evaluating Performance...\n",
            "pass 18850, training loss 9.701679229736328\n",
            "Evaluating Performance...\n",
            "pass 18860, training loss 10.32868766784668\n",
            "Evaluating Performance...\n",
            "pass 18870, training loss 9.35309886932373\n",
            "Evaluating Performance...\n",
            "pass 18880, training loss 11.53917121887207\n",
            "Evaluating Performance...\n",
            "pass 18890, training loss 8.350862503051758\n",
            "Evaluating Performance...\n",
            "pass 18900, training loss 12.160287857055664\n",
            "Evaluating Performance...\n",
            "pass 18910, training loss 9.12607479095459\n",
            "Evaluating Performance...\n",
            "pass 18920, training loss 10.789412498474121\n",
            "Evaluating Performance...\n",
            "pass 18930, training loss 9.677934646606445\n",
            "Evaluating Performance...\n",
            "pass 18940, training loss 11.239067077636719\n",
            "Evaluating Performance...\n",
            "pass 18950, training loss 10.455297470092773\n",
            "Evaluating Performance...\n",
            "pass 18960, training loss 13.246723175048828\n",
            "Evaluating Performance...\n",
            "pass 18970, training loss 10.221911430358887\n",
            "Evaluating Performance...\n",
            "pass 18980, training loss 10.731289863586426\n",
            "Evaluating Performance...\n",
            "pass 18990, training loss 9.251853942871094\n",
            "Evaluating Performance...\n",
            "pass 19000, training loss 9.307699203491211\n",
            "Evaluating Performance...\n",
            "pass 19010, training loss 8.177641868591309\n",
            "Evaluating Performance...\n",
            "pass 19020, training loss 9.505194664001465\n",
            "Evaluating Performance...\n",
            "pass 19030, training loss 11.753918647766113\n",
            "Evaluating Performance...\n",
            "pass 19040, training loss 11.726082801818848\n",
            "Evaluating Performance...\n",
            "pass 19050, training loss 10.806753158569336\n",
            "Evaluating Performance...\n",
            "pass 19060, training loss 11.414233207702637\n",
            "Evaluating Performance...\n",
            "pass 19070, training loss 8.69870662689209\n",
            "Evaluating Performance...\n",
            "pass 19080, training loss 12.861988067626953\n",
            "Evaluating Performance...\n",
            "pass 19090, training loss 12.90943717956543\n",
            "Evaluating Performance...\n",
            "pass 19100, training loss 10.998676300048828\n",
            "Evaluating Performance...\n",
            "pass 19110, training loss 11.639599800109863\n",
            "Evaluating Performance...\n",
            "pass 19120, training loss 10.460003852844238\n",
            "Evaluating Performance...\n",
            "pass 19130, training loss 10.803409576416016\n",
            "Evaluating Performance...\n",
            "pass 19140, training loss 10.304699897766113\n",
            "Evaluating Performance...\n",
            "pass 19150, training loss 8.314626693725586\n",
            "Evaluating Performance...\n",
            "pass 19160, training loss 10.637935638427734\n",
            "Evaluating Performance...\n",
            "pass 19170, training loss 8.68942928314209\n",
            "Evaluating Performance...\n",
            "pass 19180, training loss 10.110467910766602\n",
            "Evaluating Performance...\n",
            "pass 19190, training loss 11.085280418395996\n",
            "Evaluating Performance...\n",
            "pass 19200, training loss 10.007345199584961\n",
            "Evaluating Performance...\n",
            "pass 19210, training loss 10.763925552368164\n",
            "Evaluating Performance...\n",
            "pass 19220, training loss 9.241682052612305\n",
            "Evaluating Performance...\n",
            "pass 19230, training loss 9.488004684448242\n",
            "Evaluating Performance...\n",
            "pass 19240, training loss 9.322402000427246\n",
            "Evaluating Performance...\n",
            "pass 19250, training loss 10.526384353637695\n",
            "Evaluating Performance...\n",
            "pass 19260, training loss 11.597923278808594\n",
            "Evaluating Performance...\n",
            "pass 19270, training loss 11.57213020324707\n",
            "Evaluating Performance...\n",
            "pass 19280, training loss 10.324090003967285\n",
            "Evaluating Performance...\n",
            "pass 19290, training loss 10.95322036743164\n",
            "Evaluating Performance...\n",
            "pass 19300, training loss 10.949338912963867\n",
            "Evaluating Performance...\n",
            "pass 19310, training loss 10.732108116149902\n",
            "Evaluating Performance...\n",
            "pass 19320, training loss 8.647811889648438\n",
            "Evaluating Performance...\n",
            "pass 19330, training loss 8.756880760192871\n",
            "Evaluating Performance...\n",
            "pass 19340, training loss 8.486899375915527\n",
            "Evaluating Performance...\n",
            "pass 19350, training loss 11.90921688079834\n",
            "Evaluating Performance...\n",
            "pass 19360, training loss 10.204408645629883\n",
            "Evaluating Performance...\n",
            "pass 19370, training loss 10.45654296875\n",
            "Evaluating Performance...\n",
            "pass 19380, training loss 7.506843090057373\n",
            "Evaluating Performance...\n",
            "pass 19390, training loss 10.759191513061523\n",
            "Evaluating Performance...\n",
            "pass 19400, training loss 12.456168174743652\n",
            "Evaluating Performance...\n",
            "pass 19410, training loss 8.667205810546875\n",
            "Evaluating Performance...\n",
            "pass 19420, training loss 9.951546669006348\n",
            "Evaluating Performance...\n",
            "pass 19430, training loss 9.327561378479004\n",
            "Evaluating Performance...\n",
            "pass 19440, training loss 10.636301040649414\n",
            "Evaluating Performance...\n",
            "pass 19450, training loss 11.864730834960938\n",
            "Evaluating Performance...\n",
            "pass 19460, training loss 8.66721248626709\n",
            "Evaluating Performance...\n",
            "pass 19470, training loss 10.303553581237793\n",
            "Evaluating Performance...\n",
            "pass 19480, training loss 9.694296836853027\n",
            "Evaluating Performance...\n",
            "pass 19490, training loss 11.70301628112793\n",
            "Evaluating Performance...\n",
            "pass 19500, training loss 11.271249771118164\n",
            "Evaluating Performance...\n",
            "pass 19510, training loss 8.352621078491211\n",
            "Evaluating Performance...\n",
            "pass 19520, training loss 10.360039710998535\n",
            "Evaluating Performance...\n",
            "pass 19530, training loss 10.325546264648438\n",
            "Evaluating Performance...\n",
            "pass 19540, training loss 11.209020614624023\n",
            "Evaluating Performance...\n",
            "pass 19550, training loss 10.326926231384277\n",
            "Evaluating Performance...\n",
            "pass 19560, training loss 10.431836128234863\n",
            "Evaluating Performance...\n",
            "pass 19570, training loss 10.028162956237793\n",
            "Evaluating Performance...\n",
            "pass 19580, training loss 9.664558410644531\n",
            "Evaluating Performance...\n",
            "pass 19590, training loss 10.013285636901855\n",
            "Evaluating Performance...\n",
            "pass 19600, training loss 11.672675132751465\n",
            "Evaluating Performance...\n",
            "pass 19610, training loss 9.181025505065918\n",
            "Evaluating Performance...\n",
            "pass 19620, training loss 11.405706405639648\n",
            "Evaluating Performance...\n",
            "pass 19630, training loss 7.984142303466797\n",
            "Evaluating Performance...\n",
            "pass 19640, training loss 11.683943748474121\n",
            "Evaluating Performance...\n",
            "pass 19650, training loss 9.461957931518555\n",
            "Evaluating Performance...\n",
            "pass 19660, training loss 10.892691612243652\n",
            "Evaluating Performance...\n",
            "pass 19670, training loss 10.104642868041992\n",
            "Evaluating Performance...\n",
            "pass 19680, training loss 9.623235702514648\n",
            "Evaluating Performance...\n",
            "pass 19690, training loss 9.067509651184082\n",
            "Evaluating Performance...\n",
            "pass 19700, training loss 11.742860794067383\n",
            "Evaluating Performance...\n",
            "pass 19710, training loss 10.309541702270508\n",
            "Evaluating Performance...\n",
            "pass 19720, training loss 9.258991241455078\n",
            "Evaluating Performance...\n",
            "pass 19730, training loss 9.745755195617676\n",
            "Evaluating Performance...\n",
            "pass 19740, training loss 9.730185508728027\n",
            "Evaluating Performance...\n",
            "pass 19750, training loss 10.549118041992188\n",
            "Evaluating Performance...\n",
            "pass 19760, training loss 10.064606666564941\n",
            "Evaluating Performance...\n",
            "pass 19770, training loss 9.470389366149902\n",
            "Evaluating Performance...\n",
            "pass 19780, training loss 11.348564147949219\n",
            "Evaluating Performance...\n",
            "pass 19790, training loss 9.946340560913086\n",
            "Evaluating Performance...\n",
            "pass 19800, training loss 15.30133056640625\n",
            "Evaluating Performance...\n",
            "pass 19810, training loss 9.265626907348633\n",
            "Evaluating Performance...\n",
            "pass 19820, training loss 7.17069673538208\n",
            "Evaluating Performance...\n",
            "pass 19830, training loss 9.062848091125488\n",
            "Evaluating Performance...\n",
            "pass 19840, training loss 10.321966171264648\n",
            "Evaluating Performance...\n",
            "pass 19850, training loss 9.681693077087402\n",
            "Evaluating Performance...\n",
            "pass 19860, training loss 9.729862213134766\n",
            "Evaluating Performance...\n",
            "pass 19870, training loss 9.539710998535156\n",
            "Evaluating Performance...\n",
            "pass 19880, training loss 11.320270538330078\n",
            "Evaluating Performance...\n",
            "pass 19890, training loss 11.21422290802002\n",
            "Evaluating Performance...\n",
            "pass 19900, training loss 11.249608993530273\n",
            "Evaluating Performance...\n",
            "pass 19910, training loss 11.535526275634766\n",
            "Evaluating Performance...\n",
            "pass 19920, training loss 10.385760307312012\n",
            "Evaluating Performance...\n",
            "pass 19930, training loss 10.810652732849121\n",
            "Evaluating Performance...\n",
            "pass 19940, training loss 10.997632026672363\n",
            "Evaluating Performance...\n",
            "pass 19950, training loss 9.734439849853516\n",
            "Evaluating Performance...\n",
            "pass 19960, training loss 10.313844680786133\n",
            "Evaluating Performance...\n",
            "pass 19970, training loss 10.54104995727539\n",
            "Evaluating Performance...\n",
            "pass 19980, training loss 8.461516380310059\n",
            "Evaluating Performance...\n",
            "pass 19990, training loss 8.481768608093262\n",
            "Evaluating Performance...\n",
            "pass 20000, training loss 11.648308753967285\n",
            "Saving Checkpoint...\n",
            "checkpoint saved\n",
            "Evaluating Performance...\n",
            "pass 20010, training loss 9.1705904006958\n",
            "Evaluating Performance...\n",
            "pass 20020, training loss 14.139012336730957\n",
            "Evaluating Performance...\n",
            "pass 20030, training loss 12.17686653137207\n",
            "Evaluating Performance...\n",
            "pass 20040, training loss 8.960692405700684\n",
            "Evaluating Performance...\n",
            "pass 20050, training loss 9.495434761047363\n",
            "Evaluating Performance...\n",
            "pass 20060, training loss 9.90051555633545\n",
            "Evaluating Performance...\n",
            "pass 20070, training loss 10.91411018371582\n",
            "Evaluating Performance...\n",
            "pass 20080, training loss 9.847685813903809\n",
            "Evaluating Performance...\n",
            "pass 20090, training loss 12.383122444152832\n",
            "Evaluating Performance...\n",
            "pass 20100, training loss 8.799798965454102\n",
            "Evaluating Performance...\n",
            "pass 20110, training loss 9.511407852172852\n",
            "Evaluating Performance...\n",
            "pass 20120, training loss 10.243705749511719\n",
            "Evaluating Performance...\n",
            "pass 20130, training loss 10.557138442993164\n",
            "Evaluating Performance...\n",
            "pass 20140, training loss 9.778029441833496\n",
            "Evaluating Performance...\n",
            "pass 20150, training loss 12.072123527526855\n",
            "Evaluating Performance...\n",
            "pass 20160, training loss 10.24169635772705\n",
            "Evaluating Performance...\n",
            "pass 20170, training loss 9.428403854370117\n",
            "Evaluating Performance...\n",
            "pass 20180, training loss 9.716676712036133\n",
            "Evaluating Performance...\n",
            "pass 20190, training loss 8.832406044006348\n",
            "Evaluating Performance...\n",
            "pass 20200, training loss 9.41756820678711\n",
            "Evaluating Performance...\n",
            "pass 20210, training loss 10.929342269897461\n",
            "Evaluating Performance...\n",
            "pass 20220, training loss 7.998169422149658\n",
            "Evaluating Performance...\n",
            "pass 20230, training loss 10.697317123413086\n",
            "Evaluating Performance...\n",
            "pass 20240, training loss 8.254734992980957\n",
            "Evaluating Performance...\n",
            "pass 20250, training loss 11.437443733215332\n",
            "Evaluating Performance...\n",
            "pass 20260, training loss 7.868803977966309\n",
            "Evaluating Performance...\n",
            "pass 20270, training loss 10.443086624145508\n",
            "Evaluating Performance...\n",
            "pass 20280, training loss 11.522549629211426\n",
            "Evaluating Performance...\n",
            "pass 20290, training loss 10.995633125305176\n",
            "Evaluating Performance...\n",
            "pass 20300, training loss 9.65790843963623\n",
            "Evaluating Performance...\n",
            "pass 20310, training loss 10.298693656921387\n",
            "Evaluating Performance...\n",
            "pass 20320, training loss 10.768171310424805\n",
            "Evaluating Performance...\n",
            "pass 20330, training loss 10.94018268585205\n",
            "Evaluating Performance...\n",
            "pass 20340, training loss 13.200113296508789\n",
            "Evaluating Performance...\n",
            "pass 20350, training loss 9.82032585144043\n",
            "Evaluating Performance...\n",
            "pass 20360, training loss 9.756978034973145\n",
            "Evaluating Performance...\n",
            "pass 20370, training loss 9.52247142791748\n",
            "Evaluating Performance...\n",
            "pass 20380, training loss 9.891444206237793\n",
            "Evaluating Performance...\n",
            "pass 20390, training loss 9.802850723266602\n",
            "Evaluating Performance...\n",
            "pass 20400, training loss 10.486075401306152\n",
            "Evaluating Performance...\n",
            "pass 20410, training loss 11.688742637634277\n",
            "Evaluating Performance...\n",
            "pass 20420, training loss 8.701173782348633\n",
            "Evaluating Performance...\n",
            "pass 20430, training loss 9.03305721282959\n",
            "Evaluating Performance...\n",
            "pass 20440, training loss 11.22700023651123\n",
            "Evaluating Performance...\n",
            "pass 20450, training loss 8.209673881530762\n",
            "Evaluating Performance...\n",
            "pass 20460, training loss 7.748064994812012\n",
            "Evaluating Performance...\n",
            "pass 20470, training loss 9.6804780960083\n",
            "Evaluating Performance...\n",
            "pass 20480, training loss 11.272497177124023\n",
            "Evaluating Performance...\n",
            "pass 20490, training loss 11.00539493560791\n",
            "Evaluating Performance...\n",
            "pass 20500, training loss 8.621153831481934\n",
            "Evaluating Performance...\n",
            "pass 20510, training loss 10.58799934387207\n",
            "Evaluating Performance...\n",
            "pass 20520, training loss 11.366873741149902\n",
            "Evaluating Performance...\n",
            "pass 20530, training loss 12.054779052734375\n",
            "Evaluating Performance...\n",
            "pass 20540, training loss 9.855450630187988\n",
            "Evaluating Performance...\n",
            "pass 20550, training loss 8.34870433807373\n",
            "Evaluating Performance...\n",
            "pass 20560, training loss 8.931001663208008\n",
            "Evaluating Performance...\n",
            "pass 20570, training loss 10.077202796936035\n",
            "Evaluating Performance...\n",
            "pass 20580, training loss 11.473787307739258\n",
            "Evaluating Performance...\n",
            "pass 20590, training loss 10.662484169006348\n",
            "Evaluating Performance...\n",
            "pass 20600, training loss 11.788914680480957\n",
            "Evaluating Performance...\n",
            "pass 20610, training loss 8.293001174926758\n",
            "Evaluating Performance...\n",
            "pass 20620, training loss 11.988887786865234\n",
            "Evaluating Performance...\n",
            "pass 20630, training loss 8.67231559753418\n",
            "Evaluating Performance...\n",
            "pass 20640, training loss 8.223706245422363\n",
            "Evaluating Performance...\n",
            "pass 20650, training loss 9.705716133117676\n",
            "Evaluating Performance...\n",
            "pass 20660, training loss 12.849039077758789\n",
            "Evaluating Performance...\n",
            "pass 20670, training loss 10.005812644958496\n",
            "Evaluating Performance...\n",
            "pass 20680, training loss 8.913392066955566\n",
            "Evaluating Performance...\n",
            "pass 20690, training loss 8.940443992614746\n",
            "Evaluating Performance...\n",
            "pass 20700, training loss 11.214561462402344\n",
            "Evaluating Performance...\n",
            "pass 20710, training loss 10.79100227355957\n",
            "Evaluating Performance...\n",
            "pass 20720, training loss 7.923261642456055\n",
            "Evaluating Performance...\n",
            "pass 20730, training loss 13.091397285461426\n",
            "Evaluating Performance...\n",
            "pass 20740, training loss 10.967486381530762\n",
            "Evaluating Performance...\n",
            "pass 20750, training loss 9.132088661193848\n",
            "Evaluating Performance...\n",
            "pass 20760, training loss 9.420340538024902\n",
            "Evaluating Performance...\n",
            "pass 20770, training loss 8.379941940307617\n",
            "Evaluating Performance...\n",
            "pass 20780, training loss 9.834866523742676\n",
            "Evaluating Performance...\n",
            "pass 20790, training loss 9.196122169494629\n",
            "Evaluating Performance...\n",
            "pass 20800, training loss 9.895743370056152\n",
            "Evaluating Performance...\n",
            "pass 20810, training loss 12.488090515136719\n",
            "Evaluating Performance...\n",
            "pass 20820, training loss 9.72292423248291\n",
            "Evaluating Performance...\n",
            "pass 20830, training loss 8.996017456054688\n",
            "Evaluating Performance...\n",
            "pass 20840, training loss 10.530832290649414\n",
            "Evaluating Performance...\n",
            "pass 20850, training loss 9.03958797454834\n",
            "Evaluating Performance...\n",
            "pass 20860, training loss 10.941568374633789\n",
            "Evaluating Performance...\n",
            "pass 20870, training loss 11.776350975036621\n",
            "Evaluating Performance...\n",
            "pass 20880, training loss 11.543174743652344\n",
            "Evaluating Performance...\n",
            "pass 20890, training loss 11.316176414489746\n",
            "Evaluating Performance...\n",
            "pass 20900, training loss 8.43488597869873\n",
            "Evaluating Performance...\n",
            "pass 20910, training loss 11.144241333007812\n",
            "Evaluating Performance...\n",
            "pass 20920, training loss 9.336385726928711\n",
            "Evaluating Performance...\n",
            "pass 20930, training loss 11.100207328796387\n",
            "Evaluating Performance...\n",
            "pass 20940, training loss 9.648078918457031\n",
            "Evaluating Performance...\n",
            "pass 20950, training loss 10.532829284667969\n",
            "Evaluating Performance...\n",
            "pass 20960, training loss 10.931916236877441\n",
            "Evaluating Performance...\n",
            "pass 20970, training loss 11.23658275604248\n",
            "Evaluating Performance...\n",
            "pass 20980, training loss 10.392518997192383\n",
            "Evaluating Performance...\n",
            "pass 20990, training loss 9.543642044067383\n",
            "Evaluating Performance...\n",
            "pass 21000, training loss 9.278764724731445\n",
            "Evaluating Performance...\n",
            "pass 21010, training loss 8.541844367980957\n",
            "Evaluating Performance...\n",
            "pass 21020, training loss 8.998214721679688\n",
            "Evaluating Performance...\n",
            "pass 21030, training loss 9.883780479431152\n",
            "Evaluating Performance...\n",
            "pass 21040, training loss 11.589298248291016\n",
            "Evaluating Performance...\n",
            "pass 21050, training loss 8.86539363861084\n",
            "Evaluating Performance...\n",
            "pass 21060, training loss 10.49831485748291\n",
            "Evaluating Performance...\n",
            "pass 21070, training loss 9.127826690673828\n",
            "Evaluating Performance...\n",
            "pass 21080, training loss 9.333806037902832\n",
            "Evaluating Performance...\n",
            "pass 21090, training loss 7.4701690673828125\n",
            "Evaluating Performance...\n",
            "pass 21100, training loss 8.956499099731445\n",
            "Evaluating Performance...\n",
            "pass 21110, training loss 11.132373809814453\n",
            "Evaluating Performance...\n",
            "pass 21120, training loss 10.175554275512695\n",
            "Evaluating Performance...\n",
            "pass 21130, training loss 12.190022468566895\n",
            "Evaluating Performance...\n",
            "pass 21140, training loss 11.293327331542969\n",
            "Evaluating Performance...\n",
            "pass 21150, training loss 10.657964706420898\n",
            "Evaluating Performance...\n",
            "pass 21160, training loss 9.937238693237305\n",
            "Evaluating Performance...\n",
            "pass 21170, training loss 12.08682918548584\n",
            "Evaluating Performance...\n",
            "pass 21180, training loss 8.973810195922852\n",
            "Evaluating Performance...\n",
            "pass 21190, training loss 10.11166763305664\n",
            "Evaluating Performance...\n",
            "pass 21200, training loss 12.119149208068848\n",
            "Evaluating Performance...\n",
            "pass 21210, training loss 9.982464790344238\n",
            "Evaluating Performance...\n",
            "pass 21220, training loss 10.697113990783691\n",
            "Evaluating Performance...\n",
            "pass 21230, training loss 10.890385627746582\n",
            "Evaluating Performance...\n",
            "pass 21240, training loss 9.833515167236328\n",
            "Evaluating Performance...\n",
            "pass 21250, training loss 10.465925216674805\n",
            "Evaluating Performance...\n",
            "pass 21260, training loss 10.196812629699707\n",
            "Evaluating Performance...\n",
            "pass 21270, training loss 10.318729400634766\n",
            "Evaluating Performance...\n",
            "pass 21280, training loss 9.850668907165527\n",
            "Evaluating Performance...\n",
            "pass 21290, training loss 7.4170074462890625\n",
            "Evaluating Performance...\n",
            "pass 21300, training loss 9.494415283203125\n",
            "Evaluating Performance...\n",
            "pass 21310, training loss 10.45705509185791\n",
            "Evaluating Performance...\n",
            "pass 21320, training loss 10.146530151367188\n",
            "Evaluating Performance...\n",
            "pass 21330, training loss 11.101982116699219\n",
            "Evaluating Performance...\n",
            "pass 21340, training loss 8.459939002990723\n",
            "Evaluating Performance...\n",
            "pass 21350, training loss 10.766694068908691\n",
            "Evaluating Performance...\n",
            "pass 21360, training loss 8.853628158569336\n",
            "Evaluating Performance...\n",
            "pass 21370, training loss 10.97120189666748\n",
            "Evaluating Performance...\n",
            "pass 21380, training loss 11.770869255065918\n",
            "Evaluating Performance...\n",
            "pass 21390, training loss 9.223845481872559\n",
            "Evaluating Performance...\n",
            "pass 21400, training loss 12.992973327636719\n",
            "Evaluating Performance...\n",
            "pass 21410, training loss 10.552913665771484\n",
            "Evaluating Performance...\n",
            "pass 21420, training loss 11.611425399780273\n",
            "Evaluating Performance...\n",
            "pass 21430, training loss 11.613091468811035\n",
            "Evaluating Performance...\n",
            "pass 21440, training loss 12.106538772583008\n",
            "Evaluating Performance...\n",
            "pass 21450, training loss 11.534965515136719\n",
            "Evaluating Performance...\n",
            "pass 21460, training loss 8.807210922241211\n",
            "Evaluating Performance...\n",
            "pass 21470, training loss 8.621465682983398\n",
            "Evaluating Performance...\n",
            "pass 21480, training loss 11.270834922790527\n",
            "Evaluating Performance...\n",
            "pass 21490, training loss 9.156652450561523\n",
            "Evaluating Performance...\n",
            "pass 21500, training loss 9.506815910339355\n",
            "Evaluating Performance...\n",
            "pass 21510, training loss 10.623985290527344\n",
            "Evaluating Performance...\n",
            "pass 21520, training loss 11.225844383239746\n",
            "Evaluating Performance...\n",
            "pass 21530, training loss 7.87819766998291\n",
            "Evaluating Performance...\n",
            "pass 21540, training loss 11.285000801086426\n",
            "Evaluating Performance...\n",
            "pass 21550, training loss 10.817358016967773\n",
            "Evaluating Performance...\n",
            "pass 21560, training loss 8.590413093566895\n",
            "Evaluating Performance...\n",
            "pass 21570, training loss 10.354804039001465\n",
            "Evaluating Performance...\n",
            "pass 21580, training loss 12.821086883544922\n",
            "Evaluating Performance...\n",
            "pass 21590, training loss 11.192999839782715\n",
            "Evaluating Performance...\n",
            "pass 21600, training loss 8.347021102905273\n",
            "Evaluating Performance...\n",
            "pass 21610, training loss 8.711864471435547\n",
            "Evaluating Performance...\n",
            "pass 21620, training loss 9.715079307556152\n",
            "Evaluating Performance...\n",
            "pass 21630, training loss 10.949041366577148\n",
            "Evaluating Performance...\n",
            "pass 21640, training loss 11.032391548156738\n",
            "Evaluating Performance...\n",
            "pass 21650, training loss 9.48992919921875\n",
            "Evaluating Performance...\n",
            "pass 21660, training loss 11.516202926635742\n",
            "Evaluating Performance...\n",
            "pass 21670, training loss 10.149106979370117\n",
            "Evaluating Performance...\n",
            "pass 21680, training loss 9.508626937866211\n",
            "Evaluating Performance...\n",
            "pass 21690, training loss 10.639483451843262\n",
            "Evaluating Performance...\n",
            "pass 21700, training loss 10.556364059448242\n",
            "Evaluating Performance...\n",
            "pass 21710, training loss 11.49116325378418\n",
            "Evaluating Performance...\n",
            "pass 21720, training loss 9.991585731506348\n",
            "Evaluating Performance...\n",
            "pass 21730, training loss 9.707925796508789\n",
            "Evaluating Performance...\n",
            "pass 21740, training loss 9.447331428527832\n",
            "Evaluating Performance...\n",
            "pass 21750, training loss 11.508537292480469\n",
            "Evaluating Performance...\n",
            "pass 21760, training loss 10.917040824890137\n",
            "Evaluating Performance...\n",
            "pass 21770, training loss 11.992388725280762\n",
            "Evaluating Performance...\n",
            "pass 21780, training loss 12.817174911499023\n",
            "Evaluating Performance...\n",
            "pass 21790, training loss 7.389801502227783\n",
            "Evaluating Performance...\n",
            "pass 21800, training loss 8.496665000915527\n",
            "Evaluating Performance...\n",
            "pass 21810, training loss 9.287999153137207\n",
            "Evaluating Performance...\n",
            "pass 21820, training loss 9.870512008666992\n",
            "Evaluating Performance...\n",
            "pass 21830, training loss 9.980552673339844\n",
            "Evaluating Performance...\n",
            "pass 21840, training loss 9.966617584228516\n",
            "Evaluating Performance...\n",
            "pass 21850, training loss 8.967669486999512\n",
            "Evaluating Performance...\n",
            "pass 21860, training loss 8.899330139160156\n",
            "Evaluating Performance...\n",
            "pass 21870, training loss 10.373891830444336\n",
            "Evaluating Performance...\n",
            "pass 21880, training loss 13.069960594177246\n",
            "Evaluating Performance...\n",
            "pass 21890, training loss 10.481467247009277\n",
            "Evaluating Performance...\n",
            "pass 21900, training loss 8.425057411193848\n",
            "Evaluating Performance...\n",
            "pass 21910, training loss 12.138193130493164\n",
            "Evaluating Performance...\n",
            "pass 21920, training loss 10.478388786315918\n",
            "Evaluating Performance...\n",
            "pass 21930, training loss 9.915177345275879\n",
            "Evaluating Performance...\n",
            "pass 21940, training loss 12.266372680664062\n",
            "Evaluating Performance...\n",
            "pass 21950, training loss 13.780637741088867\n",
            "Evaluating Performance...\n",
            "pass 21960, training loss 8.507464408874512\n",
            "Evaluating Performance...\n",
            "pass 21970, training loss 7.846897602081299\n",
            "Evaluating Performance...\n",
            "pass 21980, training loss 10.455363273620605\n",
            "Evaluating Performance...\n",
            "pass 21990, training loss 11.431501388549805\n",
            "Evaluating Performance...\n",
            "pass 22000, training loss 10.391891479492188\n",
            "Evaluating Performance...\n",
            "pass 22010, training loss 11.68913745880127\n",
            "Evaluating Performance...\n",
            "pass 22020, training loss 11.06631851196289\n",
            "Evaluating Performance...\n",
            "pass 22030, training loss 11.673040390014648\n",
            "Evaluating Performance...\n",
            "pass 22040, training loss 9.422012329101562\n",
            "Evaluating Performance...\n",
            "pass 22050, training loss 10.94633674621582\n",
            "Evaluating Performance...\n",
            "pass 22060, training loss 8.035738945007324\n",
            "Evaluating Performance...\n",
            "pass 22070, training loss 12.30898666381836\n",
            "Evaluating Performance...\n",
            "pass 22080, training loss 9.857328414916992\n",
            "Evaluating Performance...\n",
            "pass 22090, training loss 10.09447193145752\n",
            "Evaluating Performance...\n",
            "pass 22100, training loss 11.356552124023438\n",
            "Evaluating Performance...\n",
            "pass 22110, training loss 12.496156692504883\n",
            "Evaluating Performance...\n",
            "pass 22120, training loss 8.844172477722168\n",
            "Evaluating Performance...\n",
            "pass 22130, training loss 10.645139694213867\n",
            "Evaluating Performance...\n",
            "pass 22140, training loss 9.237934112548828\n",
            "Evaluating Performance...\n",
            "pass 22150, training loss 9.838238716125488\n",
            "Evaluating Performance...\n",
            "pass 22160, training loss 9.63671588897705\n",
            "Evaluating Performance...\n",
            "pass 22170, training loss 10.000938415527344\n",
            "Evaluating Performance...\n",
            "pass 22180, training loss 9.622594833374023\n",
            "Evaluating Performance...\n",
            "pass 22190, training loss 9.479400634765625\n",
            "Evaluating Performance...\n",
            "pass 22200, training loss 10.001312255859375\n",
            "Evaluating Performance...\n",
            "pass 22210, training loss 8.882125854492188\n",
            "Evaluating Performance...\n",
            "pass 22220, training loss 8.368391990661621\n",
            "Evaluating Performance...\n",
            "pass 22230, training loss 10.87957763671875\n",
            "Evaluating Performance...\n",
            "pass 22240, training loss 10.173848152160645\n",
            "Evaluating Performance...\n",
            "pass 22250, training loss 9.771161079406738\n",
            "Evaluating Performance...\n",
            "pass 22260, training loss 7.355940341949463\n",
            "Evaluating Performance...\n",
            "pass 22270, training loss 8.986233711242676\n",
            "Evaluating Performance...\n",
            "pass 22280, training loss 10.511679649353027\n",
            "Evaluating Performance...\n",
            "pass 22290, training loss 11.247055053710938\n",
            "Evaluating Performance...\n",
            "pass 22300, training loss 9.841460227966309\n",
            "Evaluating Performance...\n",
            "pass 22310, training loss 9.943092346191406\n",
            "Evaluating Performance...\n",
            "pass 22320, training loss 10.458284378051758\n",
            "Evaluating Performance...\n",
            "pass 22330, training loss 9.055353164672852\n",
            "Evaluating Performance...\n",
            "pass 22340, training loss 8.930487632751465\n",
            "Evaluating Performance...\n",
            "pass 22350, training loss 11.248308181762695\n",
            "Evaluating Performance...\n",
            "pass 22360, training loss 11.406685829162598\n",
            "Evaluating Performance...\n",
            "pass 22370, training loss 10.662396430969238\n",
            "Evaluating Performance...\n",
            "pass 22380, training loss 10.969684600830078\n",
            "Evaluating Performance...\n",
            "pass 22390, training loss 10.21779727935791\n",
            "Evaluating Performance...\n",
            "pass 22400, training loss 9.52292537689209\n",
            "Evaluating Performance...\n",
            "pass 22410, training loss 9.006965637207031\n",
            "Evaluating Performance...\n",
            "pass 22420, training loss 9.551441192626953\n",
            "Evaluating Performance...\n",
            "pass 22430, training loss 9.372486114501953\n",
            "Evaluating Performance...\n",
            "pass 22440, training loss 9.298238754272461\n",
            "Evaluating Performance...\n",
            "pass 22450, training loss 10.195441246032715\n",
            "Evaluating Performance...\n",
            "pass 22460, training loss 11.502961158752441\n",
            "Evaluating Performance...\n",
            "pass 22470, training loss 10.298151969909668\n",
            "Evaluating Performance...\n",
            "pass 22480, training loss 7.986599445343018\n",
            "Evaluating Performance...\n",
            "pass 22490, training loss 11.517191886901855\n",
            "Evaluating Performance...\n",
            "pass 22500, training loss 11.44881534576416\n",
            "Evaluating Performance...\n",
            "pass 22510, training loss 10.538336753845215\n",
            "Evaluating Performance...\n",
            "pass 22520, training loss 11.43545150756836\n",
            "Evaluating Performance...\n",
            "pass 22530, training loss 8.596699714660645\n",
            "Evaluating Performance...\n",
            "pass 22540, training loss 10.206988334655762\n",
            "Evaluating Performance...\n",
            "pass 22550, training loss 8.656119346618652\n",
            "Evaluating Performance...\n",
            "pass 22560, training loss 9.385115623474121\n",
            "Evaluating Performance...\n",
            "pass 22570, training loss 10.160212516784668\n",
            "Evaluating Performance...\n",
            "pass 22580, training loss 9.879141807556152\n",
            "Evaluating Performance...\n",
            "pass 22590, training loss 12.24155044555664\n",
            "Evaluating Performance...\n",
            "pass 22600, training loss 9.746672630310059\n",
            "Evaluating Performance...\n",
            "pass 22610, training loss 8.512643814086914\n",
            "Evaluating Performance...\n",
            "pass 22620, training loss 11.835100173950195\n",
            "Evaluating Performance...\n",
            "pass 22630, training loss 9.496651649475098\n",
            "Evaluating Performance...\n",
            "pass 22640, training loss 7.926161289215088\n",
            "Evaluating Performance...\n",
            "pass 22650, training loss 9.96136474609375\n",
            "Evaluating Performance...\n",
            "pass 22660, training loss 11.550311088562012\n",
            "Evaluating Performance...\n",
            "pass 22670, training loss 6.991177082061768\n",
            "Evaluating Performance...\n",
            "pass 22680, training loss 9.04587459564209\n",
            "Evaluating Performance...\n",
            "pass 22690, training loss 9.737329483032227\n",
            "Evaluating Performance...\n",
            "pass 22700, training loss 11.008686065673828\n",
            "Evaluating Performance...\n",
            "pass 22710, training loss 11.637908935546875\n",
            "Evaluating Performance...\n",
            "pass 22720, training loss 9.19787883758545\n",
            "Evaluating Performance...\n",
            "pass 22730, training loss 10.358969688415527\n",
            "Evaluating Performance...\n",
            "pass 22740, training loss 9.964269638061523\n",
            "Evaluating Performance...\n",
            "pass 22750, training loss 8.876958847045898\n",
            "Evaluating Performance...\n",
            "pass 22760, training loss 11.361825942993164\n",
            "Evaluating Performance...\n",
            "pass 22770, training loss 10.259530067443848\n",
            "Evaluating Performance...\n",
            "pass 22780, training loss 10.826961517333984\n",
            "Evaluating Performance...\n",
            "pass 22790, training loss 10.78000545501709\n",
            "Evaluating Performance...\n",
            "pass 22800, training loss 13.344966888427734\n",
            "Evaluating Performance...\n",
            "pass 22810, training loss 11.440569877624512\n",
            "Evaluating Performance...\n",
            "pass 22820, training loss 8.726117134094238\n",
            "Evaluating Performance...\n",
            "pass 22830, training loss 9.779434204101562\n",
            "Evaluating Performance...\n",
            "pass 22840, training loss 9.270206451416016\n",
            "Evaluating Performance...\n",
            "pass 22850, training loss 8.109225273132324\n",
            "Evaluating Performance...\n",
            "pass 22860, training loss 12.013111114501953\n",
            "Evaluating Performance...\n",
            "pass 22870, training loss 9.526811599731445\n",
            "Evaluating Performance...\n",
            "pass 22880, training loss 11.067437171936035\n",
            "Evaluating Performance...\n",
            "pass 22890, training loss 11.697461128234863\n",
            "Evaluating Performance...\n",
            "pass 22900, training loss 9.39365291595459\n",
            "Evaluating Performance...\n",
            "pass 22910, training loss 9.657574653625488\n",
            "Evaluating Performance...\n",
            "pass 22920, training loss 7.791833877563477\n",
            "Evaluating Performance...\n",
            "pass 22930, training loss 9.454744338989258\n",
            "Evaluating Performance...\n",
            "pass 22940, training loss 10.333683967590332\n",
            "Evaluating Performance...\n",
            "pass 22950, training loss 7.290801048278809\n",
            "Evaluating Performance...\n",
            "pass 22960, training loss 8.121848106384277\n",
            "Evaluating Performance...\n",
            "pass 22970, training loss 9.903081893920898\n",
            "Evaluating Performance...\n",
            "pass 22980, training loss 10.451165199279785\n",
            "Evaluating Performance...\n",
            "pass 22990, training loss 9.489139556884766\n",
            "Evaluating Performance...\n",
            "pass 23000, training loss 10.299479484558105\n",
            "Evaluating Performance...\n",
            "pass 23010, training loss 10.033108711242676\n",
            "Evaluating Performance...\n",
            "pass 23020, training loss 9.292366027832031\n",
            "Evaluating Performance...\n",
            "pass 23030, training loss 9.326495170593262\n",
            "Evaluating Performance...\n",
            "pass 23040, training loss 10.520893096923828\n",
            "Evaluating Performance...\n",
            "pass 23050, training loss 11.159260749816895\n",
            "Evaluating Performance...\n",
            "pass 23060, training loss 9.15325927734375\n",
            "Evaluating Performance...\n",
            "pass 23070, training loss 10.39842414855957\n",
            "Evaluating Performance...\n",
            "pass 23080, training loss 10.26857852935791\n",
            "Evaluating Performance...\n",
            "pass 23090, training loss 8.030577659606934\n",
            "Evaluating Performance...\n",
            "pass 23100, training loss 10.17345905303955\n",
            "Evaluating Performance...\n",
            "pass 23110, training loss 10.528740882873535\n",
            "Evaluating Performance...\n",
            "pass 23120, training loss 9.025309562683105\n",
            "Evaluating Performance...\n",
            "pass 23130, training loss 10.077314376831055\n",
            "Evaluating Performance...\n",
            "pass 23140, training loss 7.521920680999756\n",
            "Evaluating Performance...\n",
            "pass 23150, training loss 9.95820426940918\n",
            "Evaluating Performance...\n",
            "pass 23160, training loss 9.102282524108887\n",
            "Evaluating Performance...\n",
            "pass 23170, training loss 11.316706657409668\n",
            "Evaluating Performance...\n",
            "pass 23180, training loss 9.678656578063965\n",
            "Evaluating Performance...\n",
            "pass 23190, training loss 6.9446539878845215\n",
            "Evaluating Performance...\n",
            "pass 23200, training loss 9.29720687866211\n",
            "Evaluating Performance...\n",
            "pass 23210, training loss 8.827374458312988\n",
            "Evaluating Performance...\n",
            "pass 23220, training loss 10.021045684814453\n",
            "Evaluating Performance...\n",
            "pass 23230, training loss 10.436593055725098\n",
            "Evaluating Performance...\n",
            "pass 23240, training loss 13.059234619140625\n",
            "Evaluating Performance...\n",
            "pass 23250, training loss 8.712377548217773\n",
            "Evaluating Performance...\n",
            "pass 23260, training loss 9.087902069091797\n",
            "Evaluating Performance...\n",
            "pass 23270, training loss 8.745556831359863\n",
            "Evaluating Performance...\n",
            "pass 23280, training loss 8.404183387756348\n",
            "Evaluating Performance...\n",
            "pass 23290, training loss 9.872896194458008\n",
            "Evaluating Performance...\n",
            "pass 23300, training loss 9.252257347106934\n",
            "Evaluating Performance...\n",
            "pass 23310, training loss 9.52774715423584\n",
            "Evaluating Performance...\n",
            "pass 23320, training loss 11.192009925842285\n",
            "Evaluating Performance...\n",
            "pass 23330, training loss 9.030832290649414\n",
            "Evaluating Performance...\n",
            "pass 23340, training loss 10.687196731567383\n",
            "Evaluating Performance...\n",
            "pass 23350, training loss 8.737624168395996\n",
            "Evaluating Performance...\n",
            "pass 23360, training loss 13.658597946166992\n",
            "Evaluating Performance...\n",
            "pass 23370, training loss 9.103522300720215\n",
            "Evaluating Performance...\n",
            "pass 23380, training loss 9.883698463439941\n",
            "Evaluating Performance...\n",
            "pass 23390, training loss 8.817849159240723\n",
            "Evaluating Performance...\n",
            "pass 23400, training loss 9.856438636779785\n",
            "Evaluating Performance...\n",
            "pass 23410, training loss 8.206677436828613\n",
            "Evaluating Performance...\n",
            "pass 23420, training loss 8.822830200195312\n",
            "Evaluating Performance...\n",
            "pass 23430, training loss 10.540739059448242\n",
            "Evaluating Performance...\n",
            "pass 23440, training loss 7.719701766967773\n",
            "Evaluating Performance...\n",
            "pass 23450, training loss 8.307963371276855\n",
            "Evaluating Performance...\n",
            "pass 23460, training loss 10.763664245605469\n",
            "Evaluating Performance...\n",
            "pass 23470, training loss 9.790349006652832\n",
            "Evaluating Performance...\n",
            "pass 23480, training loss 8.856066703796387\n",
            "Evaluating Performance...\n",
            "pass 23490, training loss 11.17065143585205\n",
            "Evaluating Performance...\n",
            "pass 23500, training loss 8.378825187683105\n",
            "Evaluating Performance...\n",
            "pass 23510, training loss 7.917394161224365\n",
            "Evaluating Performance...\n",
            "pass 23520, training loss 8.542304039001465\n",
            "Evaluating Performance...\n",
            "pass 23530, training loss 9.284789085388184\n",
            "Evaluating Performance...\n",
            "pass 23540, training loss 8.541276931762695\n",
            "Evaluating Performance...\n",
            "pass 23550, training loss 7.174656391143799\n",
            "Evaluating Performance...\n",
            "pass 23560, training loss 6.463715076446533\n",
            "Evaluating Performance...\n",
            "pass 23570, training loss 11.142560005187988\n",
            "Evaluating Performance...\n",
            "pass 23580, training loss 8.296359062194824\n",
            "Evaluating Performance...\n",
            "pass 23590, training loss 8.71964168548584\n",
            "Evaluating Performance...\n",
            "pass 23600, training loss 10.952088356018066\n",
            "Evaluating Performance...\n",
            "pass 23610, training loss 8.192300796508789\n",
            "Evaluating Performance...\n",
            "pass 23620, training loss 9.35712718963623\n",
            "Evaluating Performance...\n",
            "pass 23630, training loss 11.949800491333008\n",
            "Evaluating Performance...\n",
            "pass 23640, training loss 10.378310203552246\n",
            "Evaluating Performance...\n",
            "pass 23650, training loss 10.882128715515137\n",
            "Evaluating Performance...\n",
            "pass 23660, training loss 9.36155891418457\n",
            "Evaluating Performance...\n",
            "pass 23670, training loss 7.685144901275635\n",
            "Evaluating Performance...\n",
            "pass 23680, training loss 7.984653949737549\n",
            "Evaluating Performance...\n",
            "pass 23690, training loss 10.732912063598633\n",
            "Evaluating Performance...\n",
            "pass 23700, training loss 8.809531211853027\n",
            "Evaluating Performance...\n",
            "pass 23710, training loss 9.926345825195312\n",
            "Evaluating Performance...\n",
            "pass 23720, training loss 8.581719398498535\n",
            "Evaluating Performance...\n",
            "pass 23730, training loss 9.860724449157715\n",
            "Evaluating Performance...\n",
            "pass 23740, training loss 9.58825397491455\n",
            "Evaluating Performance...\n",
            "pass 23750, training loss 8.880463600158691\n",
            "Evaluating Performance...\n",
            "pass 23760, training loss 11.433528900146484\n",
            "Evaluating Performance...\n",
            "pass 23770, training loss 6.802057266235352\n",
            "Evaluating Performance...\n",
            "pass 23780, training loss 8.875432968139648\n",
            "Evaluating Performance...\n",
            "pass 23790, training loss 9.984713554382324\n",
            "Evaluating Performance...\n",
            "pass 23800, training loss 11.809492111206055\n",
            "Evaluating Performance...\n",
            "pass 23810, training loss 7.804898738861084\n",
            "Evaluating Performance...\n",
            "pass 23820, training loss 9.828582763671875\n",
            "Evaluating Performance...\n",
            "pass 23830, training loss 7.683531284332275\n",
            "Evaluating Performance...\n",
            "pass 23840, training loss 7.989659786224365\n",
            "Evaluating Performance...\n",
            "pass 23850, training loss 7.353679656982422\n",
            "Evaluating Performance...\n",
            "pass 23860, training loss 9.200227737426758\n",
            "Evaluating Performance...\n",
            "pass 23870, training loss 9.702531814575195\n",
            "Evaluating Performance...\n",
            "pass 23880, training loss 10.074540138244629\n",
            "Evaluating Performance...\n",
            "pass 23890, training loss 11.311625480651855\n",
            "Evaluating Performance...\n",
            "pass 23900, training loss 11.873621940612793\n",
            "Evaluating Performance...\n",
            "pass 23910, training loss 8.34561824798584\n",
            "Evaluating Performance...\n",
            "pass 23920, training loss 9.388673782348633\n",
            "Evaluating Performance...\n",
            "pass 23930, training loss 7.6053996086120605\n",
            "Evaluating Performance...\n",
            "pass 23940, training loss 9.817447662353516\n",
            "Evaluating Performance...\n",
            "pass 23950, training loss 8.701132774353027\n",
            "Evaluating Performance...\n",
            "pass 23960, training loss 8.008904457092285\n",
            "Evaluating Performance...\n",
            "pass 23970, training loss 8.52042293548584\n",
            "Evaluating Performance...\n",
            "pass 23980, training loss 8.748953819274902\n",
            "Evaluating Performance...\n",
            "pass 23990, training loss 8.420477867126465\n",
            "Evaluating Performance...\n",
            "pass 24000, training loss 9.555986404418945\n",
            "Evaluating Performance...\n",
            "pass 24010, training loss 8.019749641418457\n",
            "Evaluating Performance...\n",
            "pass 24020, training loss 7.746135711669922\n",
            "Evaluating Performance...\n",
            "pass 24030, training loss 7.581578254699707\n",
            "Evaluating Performance...\n",
            "pass 24040, training loss 8.21261978149414\n",
            "Evaluating Performance...\n",
            "pass 24050, training loss 9.4258394241333\n",
            "Evaluating Performance...\n",
            "pass 24060, training loss 8.926658630371094\n",
            "Evaluating Performance...\n",
            "pass 24070, training loss 8.75894546508789\n",
            "Evaluating Performance...\n",
            "pass 24080, training loss 9.30948257446289\n",
            "Evaluating Performance...\n",
            "pass 24090, training loss 10.320793151855469\n",
            "Evaluating Performance...\n",
            "pass 24100, training loss 8.439257621765137\n",
            "Evaluating Performance...\n",
            "pass 24110, training loss 8.156425476074219\n",
            "Evaluating Performance...\n",
            "pass 24120, training loss 11.228012084960938\n",
            "Evaluating Performance...\n",
            "pass 24130, training loss 10.135971069335938\n",
            "Evaluating Performance...\n",
            "pass 24140, training loss 7.7429399490356445\n",
            "Evaluating Performance...\n",
            "pass 24150, training loss 8.42236328125\n",
            "Evaluating Performance...\n",
            "pass 24160, training loss 8.38243579864502\n",
            "Evaluating Performance...\n",
            "pass 24170, training loss 7.901017665863037\n",
            "Evaluating Performance...\n",
            "pass 24180, training loss 7.36409854888916\n",
            "Evaluating Performance...\n",
            "pass 24190, training loss 7.570515155792236\n",
            "Evaluating Performance...\n",
            "pass 24200, training loss 9.999783515930176\n",
            "Evaluating Performance...\n",
            "pass 24210, training loss 7.299505233764648\n",
            "Evaluating Performance...\n",
            "pass 24220, training loss 8.85435676574707\n",
            "Evaluating Performance...\n",
            "pass 24230, training loss 7.275168418884277\n",
            "Evaluating Performance...\n",
            "pass 24240, training loss 9.375246047973633\n",
            "Evaluating Performance...\n",
            "pass 24250, training loss 7.7855544090271\n",
            "Evaluating Performance...\n",
            "pass 24260, training loss 9.058353424072266\n",
            "Evaluating Performance...\n",
            "pass 24270, training loss 10.092846870422363\n",
            "Evaluating Performance...\n",
            "pass 24280, training loss 8.6873140335083\n",
            "Evaluating Performance...\n",
            "pass 24290, training loss 8.524683952331543\n",
            "Evaluating Performance...\n",
            "pass 24300, training loss 9.76589584350586\n",
            "Evaluating Performance...\n",
            "pass 24310, training loss 9.367266654968262\n",
            "Evaluating Performance...\n",
            "pass 24320, training loss 7.48160982131958\n",
            "Evaluating Performance...\n",
            "pass 24330, training loss 8.98998737335205\n",
            "Evaluating Performance...\n",
            "pass 24340, training loss 9.636834144592285\n",
            "Evaluating Performance...\n",
            "pass 24350, training loss 11.331435203552246\n",
            "Evaluating Performance...\n",
            "pass 24360, training loss 9.132073402404785\n",
            "Evaluating Performance...\n",
            "pass 24370, training loss 9.595403671264648\n",
            "Evaluating Performance...\n",
            "pass 24380, training loss 8.163718223571777\n",
            "Evaluating Performance...\n",
            "pass 24390, training loss 9.870908737182617\n",
            "Evaluating Performance...\n",
            "pass 24400, training loss 7.544617652893066\n",
            "Evaluating Performance...\n",
            "pass 24410, training loss 8.628704071044922\n",
            "Evaluating Performance...\n",
            "pass 24420, training loss 11.803675651550293\n",
            "Evaluating Performance...\n",
            "pass 24430, training loss 10.026541709899902\n",
            "Evaluating Performance...\n",
            "pass 24440, training loss 10.892601013183594\n",
            "Evaluating Performance...\n",
            "pass 24450, training loss 8.10931396484375\n",
            "Evaluating Performance...\n",
            "pass 24460, training loss 8.228193283081055\n",
            "Evaluating Performance...\n",
            "pass 24470, training loss 10.384963035583496\n",
            "Evaluating Performance...\n",
            "pass 24480, training loss 7.642801761627197\n",
            "Evaluating Performance...\n",
            "pass 24490, training loss 10.096827507019043\n",
            "Evaluating Performance...\n",
            "pass 24500, training loss 12.56822681427002\n",
            "Evaluating Performance...\n",
            "pass 24510, training loss 9.01639175415039\n",
            "Evaluating Performance...\n",
            "pass 24520, training loss 10.781834602355957\n",
            "Evaluating Performance...\n",
            "pass 24530, training loss 8.661428451538086\n",
            "Evaluating Performance...\n",
            "pass 24540, training loss 7.514555931091309\n",
            "Evaluating Performance...\n",
            "pass 24550, training loss 7.630219459533691\n",
            "Evaluating Performance...\n",
            "pass 24560, training loss 8.548161506652832\n",
            "Evaluating Performance...\n",
            "pass 24570, training loss 10.729327201843262\n",
            "Evaluating Performance...\n",
            "pass 24580, training loss 9.262819290161133\n",
            "Evaluating Performance...\n",
            "pass 24590, training loss 8.661794662475586\n",
            "Evaluating Performance...\n",
            "pass 24600, training loss 11.061117172241211\n",
            "Evaluating Performance...\n",
            "pass 24610, training loss 10.884024620056152\n",
            "Evaluating Performance...\n",
            "pass 24620, training loss 8.22844123840332\n",
            "Evaluating Performance...\n",
            "pass 24630, training loss 9.608282089233398\n",
            "Evaluating Performance...\n",
            "pass 24640, training loss 8.052495956420898\n",
            "Evaluating Performance...\n",
            "pass 24650, training loss 8.602266311645508\n",
            "Evaluating Performance...\n",
            "pass 24660, training loss 10.330249786376953\n",
            "Evaluating Performance...\n",
            "pass 24670, training loss 10.55426025390625\n",
            "Evaluating Performance...\n",
            "pass 24680, training loss 11.039826393127441\n",
            "Evaluating Performance...\n",
            "pass 24690, training loss 10.10283374786377\n",
            "Evaluating Performance...\n",
            "pass 24700, training loss 11.2669038772583\n",
            "Evaluating Performance...\n",
            "pass 24710, training loss 11.08323860168457\n",
            "Evaluating Performance...\n",
            "pass 24720, training loss 8.884255409240723\n",
            "Evaluating Performance...\n",
            "pass 24730, training loss 11.53329849243164\n",
            "Evaluating Performance...\n",
            "pass 24740, training loss 8.2698974609375\n",
            "Evaluating Performance...\n",
            "pass 24750, training loss 8.816879272460938\n",
            "Evaluating Performance...\n",
            "pass 24760, training loss 9.65393352508545\n",
            "Evaluating Performance...\n",
            "pass 24770, training loss 9.678220748901367\n",
            "Evaluating Performance...\n",
            "pass 24780, training loss 8.24805736541748\n",
            "Evaluating Performance...\n",
            "pass 24790, training loss 10.450096130371094\n",
            "Evaluating Performance...\n",
            "pass 24800, training loss 7.741994380950928\n",
            "Evaluating Performance...\n",
            "pass 24810, training loss 8.176605224609375\n",
            "Evaluating Performance...\n",
            "pass 24820, training loss 9.672605514526367\n",
            "Evaluating Performance...\n",
            "pass 24830, training loss 9.6215238571167\n",
            "Evaluating Performance...\n",
            "pass 24840, training loss 10.091273307800293\n",
            "Evaluating Performance...\n",
            "pass 24850, training loss 9.626444816589355\n",
            "Evaluating Performance...\n",
            "pass 24860, training loss 9.145869255065918\n",
            "Evaluating Performance...\n",
            "pass 24870, training loss 7.792445659637451\n",
            "Evaluating Performance...\n",
            "pass 24880, training loss 10.0289888381958\n",
            "Evaluating Performance...\n",
            "pass 24890, training loss 7.066348075866699\n",
            "Evaluating Performance...\n",
            "pass 24900, training loss 9.335442543029785\n",
            "Evaluating Performance...\n",
            "pass 24910, training loss 9.16628360748291\n",
            "Evaluating Performance...\n",
            "pass 24920, training loss 9.692571640014648\n",
            "Evaluating Performance...\n",
            "pass 24930, training loss 7.4558305740356445\n",
            "Evaluating Performance...\n",
            "pass 24940, training loss 9.585156440734863\n",
            "Evaluating Performance...\n",
            "pass 24950, training loss 8.588583946228027\n",
            "Evaluating Performance...\n",
            "pass 24960, training loss 11.012495994567871\n",
            "Evaluating Performance...\n",
            "pass 24970, training loss 9.408975601196289\n",
            "Evaluating Performance...\n",
            "pass 24980, training loss 9.913994789123535\n",
            "Evaluating Performance...\n",
            "pass 24990, training loss 8.82674503326416\n",
            "Evaluating Performance...\n",
            "pass 25000, training loss 10.438372611999512\n",
            "Evaluating Performance...\n",
            "pass 25010, training loss 11.828737258911133\n",
            "Evaluating Performance...\n",
            "pass 25020, training loss 9.050130844116211\n",
            "Evaluating Performance...\n",
            "pass 25030, training loss 10.046774864196777\n",
            "Evaluating Performance...\n",
            "pass 25040, training loss 10.234443664550781\n",
            "Evaluating Performance...\n",
            "pass 25050, training loss 9.745461463928223\n",
            "Evaluating Performance...\n",
            "pass 25060, training loss 9.073594093322754\n",
            "Evaluating Performance...\n",
            "pass 25070, training loss 8.312958717346191\n",
            "Evaluating Performance...\n",
            "pass 25080, training loss 9.427154541015625\n",
            "Evaluating Performance...\n",
            "pass 25090, training loss 8.509052276611328\n",
            "Evaluating Performance...\n",
            "pass 25100, training loss 10.054774284362793\n",
            "Evaluating Performance...\n",
            "pass 25110, training loss 8.062478065490723\n",
            "Evaluating Performance...\n",
            "pass 25120, training loss 11.43024730682373\n",
            "Evaluating Performance...\n",
            "pass 25130, training loss 8.983718872070312\n",
            "Evaluating Performance...\n",
            "pass 25140, training loss 9.005943298339844\n",
            "Evaluating Performance...\n",
            "pass 25150, training loss 8.659505844116211\n",
            "Evaluating Performance...\n",
            "pass 25160, training loss 8.510279655456543\n",
            "Evaluating Performance...\n",
            "pass 25170, training loss 9.44723892211914\n",
            "Evaluating Performance...\n",
            "pass 25180, training loss 8.746668815612793\n",
            "Evaluating Performance...\n",
            "pass 25190, training loss 9.008891105651855\n",
            "Evaluating Performance...\n",
            "pass 25200, training loss 8.306532859802246\n",
            "Evaluating Performance...\n",
            "pass 25210, training loss 9.005470275878906\n",
            "Evaluating Performance...\n",
            "pass 25220, training loss 8.646660804748535\n",
            "Evaluating Performance...\n",
            "pass 25230, training loss 9.324214935302734\n",
            "Evaluating Performance...\n",
            "pass 25240, training loss 7.14855432510376\n",
            "Evaluating Performance...\n",
            "pass 25250, training loss 10.287442207336426\n",
            "Evaluating Performance...\n",
            "pass 25260, training loss 12.126007080078125\n",
            "Evaluating Performance...\n",
            "pass 25270, training loss 7.2838873863220215\n",
            "Evaluating Performance...\n",
            "pass 25280, training loss 9.078478813171387\n",
            "Evaluating Performance...\n",
            "pass 25290, training loss 8.228545188903809\n",
            "Evaluating Performance...\n",
            "pass 25300, training loss 11.259260177612305\n",
            "Evaluating Performance...\n",
            "pass 25310, training loss 10.156938552856445\n",
            "Evaluating Performance...\n",
            "pass 25320, training loss 8.903938293457031\n",
            "Evaluating Performance...\n",
            "pass 25330, training loss 7.879329204559326\n",
            "Evaluating Performance...\n",
            "pass 25340, training loss 12.87562084197998\n",
            "Evaluating Performance...\n",
            "pass 25350, training loss 8.644227027893066\n",
            "Evaluating Performance...\n",
            "pass 25360, training loss 10.73072624206543\n",
            "Evaluating Performance...\n",
            "pass 25370, training loss 10.39468002319336\n",
            "Evaluating Performance...\n",
            "pass 25380, training loss 9.685558319091797\n",
            "Evaluating Performance...\n",
            "pass 25390, training loss 8.978384017944336\n",
            "Evaluating Performance...\n",
            "pass 25400, training loss 6.452062129974365\n",
            "Evaluating Performance...\n",
            "pass 25410, training loss 6.5743889808654785\n",
            "Evaluating Performance...\n",
            "pass 25420, training loss 9.419607162475586\n",
            "Evaluating Performance...\n",
            "pass 25430, training loss 8.144487380981445\n",
            "Evaluating Performance...\n",
            "pass 25440, training loss 9.439249992370605\n",
            "Evaluating Performance...\n",
            "pass 25450, training loss 9.208992958068848\n",
            "Evaluating Performance...\n",
            "pass 25460, training loss 6.961585521697998\n",
            "Evaluating Performance...\n",
            "pass 25470, training loss 7.509241580963135\n",
            "Evaluating Performance...\n",
            "pass 25480, training loss 8.457499504089355\n",
            "Evaluating Performance...\n",
            "pass 25490, training loss 10.877900123596191\n",
            "Evaluating Performance...\n",
            "pass 25500, training loss 6.87620210647583\n",
            "Evaluating Performance...\n",
            "pass 25510, training loss 7.70060396194458\n",
            "Evaluating Performance...\n",
            "pass 25520, training loss 9.165535926818848\n",
            "Evaluating Performance...\n",
            "pass 25530, training loss 10.909987449645996\n",
            "Evaluating Performance...\n",
            "pass 25540, training loss 11.61136531829834\n",
            "Evaluating Performance...\n",
            "pass 25550, training loss 8.3128080368042\n",
            "Evaluating Performance...\n",
            "pass 25560, training loss 8.770459175109863\n",
            "Evaluating Performance...\n",
            "pass 25570, training loss 6.737192153930664\n",
            "Evaluating Performance...\n",
            "pass 25580, training loss 11.223320007324219\n",
            "Evaluating Performance...\n",
            "pass 25590, training loss 8.935410499572754\n",
            "Evaluating Performance...\n",
            "pass 25600, training loss 7.33035135269165\n",
            "Evaluating Performance...\n",
            "pass 25610, training loss 8.063165664672852\n",
            "Evaluating Performance...\n",
            "pass 25620, training loss 11.54042911529541\n",
            "Evaluating Performance...\n",
            "pass 25630, training loss 9.144363403320312\n",
            "Evaluating Performance...\n",
            "pass 25640, training loss 7.488361835479736\n",
            "Evaluating Performance...\n",
            "pass 25650, training loss 9.824860572814941\n",
            "Evaluating Performance...\n",
            "pass 25660, training loss 7.546050071716309\n",
            "Evaluating Performance...\n",
            "pass 25670, training loss 11.52051830291748\n",
            "Evaluating Performance...\n",
            "pass 25680, training loss 10.156766891479492\n",
            "Evaluating Performance...\n",
            "pass 25690, training loss 9.671414375305176\n",
            "Evaluating Performance...\n",
            "pass 25700, training loss 7.5048346519470215\n",
            "Evaluating Performance...\n",
            "pass 25710, training loss 10.773577690124512\n",
            "Evaluating Performance...\n",
            "pass 25720, training loss 8.69056224822998\n",
            "Evaluating Performance...\n",
            "pass 25730, training loss 9.79874324798584\n",
            "Evaluating Performance...\n",
            "pass 25740, training loss 8.275583267211914\n",
            "Evaluating Performance...\n",
            "pass 25750, training loss 9.901986122131348\n",
            "Evaluating Performance...\n",
            "pass 25760, training loss 9.328156471252441\n",
            "Evaluating Performance...\n",
            "pass 25770, training loss 9.256857872009277\n",
            "Evaluating Performance...\n",
            "pass 25780, training loss 10.624661445617676\n",
            "Evaluating Performance...\n",
            "pass 25790, training loss 11.996965408325195\n",
            "Evaluating Performance...\n",
            "pass 25800, training loss 9.51342487335205\n",
            "Evaluating Performance...\n",
            "pass 25810, training loss 9.719596862792969\n",
            "Evaluating Performance...\n",
            "pass 25820, training loss 9.486223220825195\n",
            "Evaluating Performance...\n",
            "pass 25830, training loss 8.008910179138184\n",
            "Evaluating Performance...\n",
            "pass 25840, training loss 10.524328231811523\n",
            "Evaluating Performance...\n",
            "pass 25850, training loss 7.668084144592285\n",
            "Evaluating Performance...\n",
            "pass 25860, training loss 8.091275215148926\n",
            "Evaluating Performance...\n",
            "pass 25870, training loss 10.81607723236084\n",
            "Evaluating Performance...\n",
            "pass 25880, training loss 9.040700912475586\n",
            "Evaluating Performance...\n",
            "pass 25890, training loss 7.531418800354004\n",
            "Evaluating Performance...\n",
            "pass 25900, training loss 8.370925903320312\n",
            "Evaluating Performance...\n",
            "pass 25910, training loss 9.380066871643066\n",
            "Evaluating Performance...\n",
            "pass 25920, training loss 8.10306453704834\n",
            "Evaluating Performance...\n",
            "pass 25930, training loss 7.986377239227295\n",
            "Evaluating Performance...\n",
            "pass 25940, training loss 8.494720458984375\n",
            "Evaluating Performance...\n",
            "pass 25950, training loss 8.565272331237793\n",
            "Evaluating Performance...\n",
            "pass 25960, training loss 9.352775573730469\n",
            "Evaluating Performance...\n",
            "pass 25970, training loss 8.696221351623535\n",
            "Evaluating Performance...\n",
            "pass 25980, training loss 9.0672607421875\n",
            "Evaluating Performance...\n",
            "pass 25990, training loss 9.696925163269043\n",
            "Evaluating Performance...\n",
            "pass 26000, training loss 8.34719181060791\n",
            "Evaluating Performance...\n",
            "pass 26010, training loss 11.032240867614746\n",
            "Evaluating Performance...\n",
            "pass 26020, training loss 10.749082565307617\n",
            "Evaluating Performance...\n",
            "pass 26030, training loss 7.466207027435303\n",
            "Evaluating Performance...\n",
            "pass 26040, training loss 9.306352615356445\n",
            "Evaluating Performance...\n",
            "pass 26050, training loss 8.105772972106934\n",
            "Evaluating Performance...\n",
            "pass 26060, training loss 9.430843353271484\n",
            "Evaluating Performance...\n",
            "pass 26070, training loss 9.035662651062012\n",
            "Evaluating Performance...\n",
            "pass 26080, training loss 8.285350799560547\n",
            "Evaluating Performance...\n",
            "pass 26090, training loss 8.856523513793945\n",
            "Evaluating Performance...\n",
            "pass 26100, training loss 10.369083404541016\n",
            "Evaluating Performance...\n",
            "pass 26110, training loss 9.926403999328613\n",
            "Evaluating Performance...\n",
            "pass 26120, training loss 8.142231941223145\n",
            "Evaluating Performance...\n",
            "pass 26130, training loss 10.641535758972168\n",
            "Evaluating Performance...\n",
            "pass 26140, training loss 9.38707447052002\n",
            "Evaluating Performance...\n",
            "pass 26150, training loss 10.69996166229248\n",
            "Evaluating Performance...\n",
            "pass 26160, training loss 10.785795211791992\n",
            "Evaluating Performance...\n",
            "pass 26170, training loss 9.365218162536621\n",
            "Evaluating Performance...\n",
            "pass 26180, training loss 7.705917835235596\n",
            "Evaluating Performance...\n",
            "pass 26190, training loss 11.490458488464355\n",
            "Evaluating Performance...\n",
            "pass 26200, training loss 10.679988861083984\n",
            "Evaluating Performance...\n",
            "pass 26210, training loss 9.752342224121094\n",
            "Evaluating Performance...\n",
            "pass 26220, training loss 7.732158660888672\n",
            "Evaluating Performance...\n",
            "pass 26230, training loss 8.2280855178833\n",
            "Evaluating Performance...\n",
            "pass 26240, training loss 8.52930736541748\n",
            "Evaluating Performance...\n",
            "pass 26250, training loss 10.950187683105469\n",
            "Evaluating Performance...\n",
            "pass 26260, training loss 9.692828178405762\n",
            "Evaluating Performance...\n",
            "pass 26270, training loss 9.335978507995605\n",
            "Evaluating Performance...\n",
            "pass 26280, training loss 8.426154136657715\n",
            "Evaluating Performance...\n",
            "pass 26290, training loss 5.941637992858887\n",
            "Evaluating Performance...\n",
            "pass 26300, training loss 9.018792152404785\n",
            "Evaluating Performance...\n",
            "pass 26310, training loss 10.474696159362793\n",
            "Evaluating Performance...\n",
            "pass 26320, training loss 9.417311668395996\n",
            "Evaluating Performance...\n",
            "pass 26330, training loss 8.504154205322266\n",
            "Evaluating Performance...\n",
            "pass 26340, training loss 8.58869457244873\n",
            "Evaluating Performance...\n",
            "pass 26350, training loss 7.773099422454834\n",
            "Evaluating Performance...\n",
            "pass 26360, training loss 9.731449127197266\n",
            "Evaluating Performance...\n",
            "pass 26370, training loss 8.934866905212402\n",
            "Evaluating Performance...\n",
            "pass 26380, training loss 8.258933067321777\n",
            "Evaluating Performance...\n",
            "pass 26390, training loss 8.706162452697754\n",
            "Evaluating Performance...\n",
            "pass 26400, training loss 9.775165557861328\n",
            "Evaluating Performance...\n",
            "pass 26410, training loss 8.274895668029785\n",
            "Evaluating Performance...\n",
            "pass 26420, training loss 13.490975379943848\n",
            "Evaluating Performance...\n",
            "pass 26430, training loss 11.526007652282715\n",
            "Evaluating Performance...\n",
            "pass 26440, training loss 11.401838302612305\n",
            "Evaluating Performance...\n",
            "pass 26450, training loss 9.691516876220703\n",
            "Evaluating Performance...\n",
            "pass 26460, training loss 8.634218215942383\n",
            "Evaluating Performance...\n",
            "pass 26470, training loss 11.099544525146484\n",
            "Evaluating Performance...\n",
            "pass 26480, training loss 7.983098983764648\n",
            "Evaluating Performance...\n",
            "pass 26490, training loss 10.035978317260742\n",
            "Evaluating Performance...\n",
            "pass 26500, training loss 10.347463607788086\n",
            "Evaluating Performance...\n",
            "pass 26510, training loss 8.118437767028809\n",
            "Evaluating Performance...\n",
            "pass 26520, training loss 8.801896095275879\n",
            "Evaluating Performance...\n",
            "pass 26530, training loss 7.861431121826172\n",
            "Evaluating Performance...\n",
            "pass 26540, training loss 8.918292999267578\n",
            "Evaluating Performance...\n",
            "pass 26550, training loss 9.549579620361328\n",
            "Evaluating Performance...\n",
            "pass 26560, training loss 7.708272457122803\n",
            "Evaluating Performance...\n",
            "pass 26570, training loss 7.88427209854126\n",
            "Evaluating Performance...\n",
            "pass 26580, training loss 10.773877143859863\n",
            "Evaluating Performance...\n",
            "pass 26590, training loss 8.992766380310059\n",
            "Evaluating Performance...\n",
            "pass 26600, training loss 9.189355850219727\n",
            "Evaluating Performance...\n",
            "pass 26610, training loss 10.090188026428223\n",
            "Evaluating Performance...\n",
            "pass 26620, training loss 9.085823059082031\n",
            "Evaluating Performance...\n",
            "pass 26630, training loss 10.470952033996582\n",
            "Evaluating Performance...\n",
            "pass 26640, training loss 8.275304794311523\n",
            "Evaluating Performance...\n",
            "pass 26650, training loss 8.284621238708496\n",
            "Evaluating Performance...\n",
            "pass 26660, training loss 10.938916206359863\n",
            "Evaluating Performance...\n",
            "pass 26670, training loss 8.912532806396484\n",
            "Evaluating Performance...\n",
            "pass 26680, training loss 8.76647663116455\n",
            "Evaluating Performance...\n",
            "pass 26690, training loss 10.44179630279541\n",
            "Evaluating Performance...\n",
            "pass 26700, training loss 9.879304885864258\n",
            "Evaluating Performance...\n",
            "pass 26710, training loss 8.87963581085205\n",
            "Evaluating Performance...\n",
            "pass 26720, training loss 9.349093437194824\n",
            "Evaluating Performance...\n",
            "pass 26730, training loss 7.740514278411865\n",
            "Evaluating Performance...\n",
            "pass 26740, training loss 8.582975387573242\n",
            "Evaluating Performance...\n",
            "pass 26750, training loss 9.877375602722168\n",
            "Evaluating Performance...\n",
            "pass 26760, training loss 9.627872467041016\n",
            "Evaluating Performance...\n",
            "pass 26770, training loss 7.830691337585449\n",
            "Evaluating Performance...\n",
            "pass 26780, training loss 7.216362953186035\n",
            "Evaluating Performance...\n",
            "pass 26790, training loss 9.738024711608887\n",
            "Evaluating Performance...\n",
            "pass 26800, training loss 8.437459945678711\n",
            "Evaluating Performance...\n",
            "pass 26810, training loss 12.08547592163086\n",
            "Evaluating Performance...\n",
            "pass 26820, training loss 8.879914283752441\n",
            "Evaluating Performance...\n",
            "pass 26830, training loss 8.949682235717773\n",
            "Evaluating Performance...\n",
            "pass 26840, training loss 7.685408592224121\n",
            "Evaluating Performance...\n",
            "pass 26850, training loss 6.867866516113281\n",
            "Evaluating Performance...\n",
            "pass 26860, training loss 7.140244007110596\n",
            "Evaluating Performance...\n",
            "pass 26870, training loss 10.03406810760498\n",
            "Evaluating Performance...\n",
            "pass 26880, training loss 12.22735595703125\n",
            "Evaluating Performance...\n",
            "pass 26890, training loss 9.705452919006348\n",
            "Evaluating Performance...\n",
            "pass 26900, training loss 8.69129753112793\n",
            "Evaluating Performance...\n",
            "pass 26910, training loss 8.26122760772705\n",
            "Evaluating Performance...\n",
            "pass 26920, training loss 8.179682731628418\n",
            "Evaluating Performance...\n",
            "pass 26930, training loss 9.748353004455566\n",
            "Evaluating Performance...\n",
            "pass 26940, training loss 9.248626708984375\n",
            "Evaluating Performance...\n",
            "pass 26950, training loss 8.96570873260498\n",
            "Evaluating Performance...\n",
            "pass 26960, training loss 9.331657409667969\n",
            "Evaluating Performance...\n",
            "pass 26970, training loss 8.58178424835205\n",
            "Evaluating Performance...\n",
            "pass 26980, training loss 8.313250541687012\n",
            "Evaluating Performance...\n",
            "pass 26990, training loss 8.64168930053711\n",
            "Evaluating Performance...\n",
            "pass 27000, training loss 9.29007339477539\n",
            "Evaluating Performance...\n",
            "pass 27010, training loss 8.875093460083008\n",
            "Evaluating Performance...\n",
            "pass 27020, training loss 8.746086120605469\n",
            "Evaluating Performance...\n",
            "pass 27030, training loss 8.290557861328125\n",
            "Evaluating Performance...\n",
            "pass 27040, training loss 10.099112510681152\n",
            "Evaluating Performance...\n",
            "pass 27050, training loss 7.440480709075928\n",
            "Evaluating Performance...\n",
            "pass 27060, training loss 8.460597038269043\n",
            "Evaluating Performance...\n",
            "pass 27070, training loss 9.189249992370605\n",
            "Evaluating Performance...\n",
            "pass 27080, training loss 8.37020206451416\n",
            "Evaluating Performance...\n",
            "pass 27090, training loss 8.438847541809082\n",
            "Evaluating Performance...\n",
            "pass 27100, training loss 10.233304023742676\n",
            "Evaluating Performance...\n",
            "pass 27110, training loss 8.569563865661621\n",
            "Evaluating Performance...\n",
            "pass 27120, training loss 10.204739570617676\n",
            "Evaluating Performance...\n",
            "pass 27130, training loss 8.621758460998535\n",
            "Evaluating Performance...\n",
            "pass 27140, training loss 9.184784889221191\n",
            "Evaluating Performance...\n",
            "pass 27150, training loss 7.5419182777404785\n",
            "Evaluating Performance...\n",
            "pass 27160, training loss 8.526365280151367\n",
            "Evaluating Performance...\n",
            "pass 27170, training loss 9.01165771484375\n",
            "Evaluating Performance...\n",
            "pass 27180, training loss 7.596733093261719\n",
            "Evaluating Performance...\n",
            "pass 27190, training loss 9.554959297180176\n",
            "Evaluating Performance...\n",
            "pass 27200, training loss 9.046809196472168\n",
            "Evaluating Performance...\n",
            "pass 27210, training loss 6.647478103637695\n",
            "Evaluating Performance...\n",
            "pass 27220, training loss 6.604120254516602\n",
            "Evaluating Performance...\n",
            "pass 27230, training loss 9.772314071655273\n",
            "Evaluating Performance...\n",
            "pass 27240, training loss 6.643050193786621\n",
            "Evaluating Performance...\n",
            "pass 27250, training loss 10.382528305053711\n",
            "Evaluating Performance...\n",
            "pass 27260, training loss 8.016825675964355\n",
            "Evaluating Performance...\n",
            "pass 27270, training loss 8.640826225280762\n",
            "Evaluating Performance...\n",
            "pass 27280, training loss 7.787211894989014\n",
            "Evaluating Performance...\n",
            "pass 27290, training loss 7.214483261108398\n",
            "Evaluating Performance...\n",
            "pass 27300, training loss 8.147456169128418\n",
            "Evaluating Performance...\n",
            "pass 27310, training loss 8.461597442626953\n",
            "Evaluating Performance...\n",
            "pass 27320, training loss 10.5615234375\n",
            "Evaluating Performance...\n",
            "pass 27330, training loss 9.840601921081543\n",
            "Evaluating Performance...\n",
            "pass 27340, training loss 7.300713539123535\n",
            "Evaluating Performance...\n",
            "pass 27350, training loss 7.876916885375977\n",
            "Evaluating Performance...\n",
            "pass 27360, training loss 8.738097190856934\n",
            "Evaluating Performance...\n",
            "pass 27370, training loss 5.738131999969482\n",
            "Evaluating Performance...\n",
            "pass 27380, training loss 7.512487411499023\n",
            "Evaluating Performance...\n",
            "pass 27390, training loss 9.193467140197754\n",
            "Evaluating Performance...\n",
            "pass 27400, training loss 12.259051322937012\n",
            "Evaluating Performance...\n",
            "pass 27410, training loss 8.81704330444336\n",
            "Evaluating Performance...\n",
            "pass 27420, training loss 7.593410491943359\n",
            "Evaluating Performance...\n",
            "pass 27430, training loss 8.975768089294434\n",
            "Evaluating Performance...\n",
            "pass 27440, training loss 6.765092849731445\n",
            "Evaluating Performance...\n",
            "pass 27450, training loss 8.14029598236084\n",
            "Evaluating Performance...\n",
            "pass 27460, training loss 8.99284839630127\n",
            "Evaluating Performance...\n",
            "pass 27470, training loss 7.758876800537109\n",
            "Evaluating Performance...\n",
            "pass 27480, training loss 7.945811748504639\n",
            "Evaluating Performance...\n",
            "pass 27490, training loss 8.693778038024902\n",
            "Evaluating Performance...\n",
            "pass 27500, training loss 10.862001419067383\n",
            "Evaluating Performance...\n",
            "pass 27510, training loss 10.131281852722168\n",
            "Evaluating Performance...\n",
            "pass 27520, training loss 7.273562908172607\n",
            "Evaluating Performance...\n",
            "pass 27530, training loss 6.680728435516357\n",
            "Evaluating Performance...\n",
            "pass 27540, training loss 7.02439546585083\n",
            "Evaluating Performance...\n",
            "pass 27550, training loss 8.439114570617676\n",
            "Evaluating Performance...\n",
            "pass 27560, training loss 9.002488136291504\n",
            "Evaluating Performance...\n",
            "pass 27570, training loss 6.21122407913208\n",
            "Evaluating Performance...\n",
            "pass 27580, training loss 9.330183982849121\n",
            "Evaluating Performance...\n",
            "pass 27590, training loss 7.896058082580566\n",
            "Evaluating Performance...\n",
            "pass 27600, training loss 7.564960956573486\n",
            "Evaluating Performance...\n",
            "pass 27610, training loss 8.499688148498535\n",
            "Evaluating Performance...\n",
            "pass 27620, training loss 9.43910026550293\n",
            "Evaluating Performance...\n",
            "pass 27630, training loss 6.730891704559326\n",
            "Evaluating Performance...\n",
            "pass 27640, training loss 9.880690574645996\n",
            "Evaluating Performance...\n",
            "pass 27650, training loss 9.66781234741211\n",
            "Evaluating Performance...\n",
            "pass 27660, training loss 8.14346694946289\n",
            "Evaluating Performance...\n",
            "pass 27670, training loss 7.585236072540283\n",
            "Evaluating Performance...\n",
            "pass 27680, training loss 8.87042236328125\n",
            "Evaluating Performance...\n",
            "pass 27690, training loss 6.76508903503418\n",
            "Evaluating Performance...\n",
            "pass 27700, training loss 9.31450080871582\n",
            "Evaluating Performance...\n",
            "pass 27710, training loss 10.748835563659668\n",
            "Evaluating Performance...\n",
            "pass 27720, training loss 8.931248664855957\n",
            "Evaluating Performance...\n",
            "pass 27730, training loss 7.257144451141357\n",
            "Evaluating Performance...\n",
            "pass 27740, training loss 6.5016093254089355\n",
            "Evaluating Performance...\n",
            "pass 27750, training loss 10.410670280456543\n",
            "Evaluating Performance...\n",
            "pass 27760, training loss 8.362998962402344\n",
            "Evaluating Performance...\n",
            "pass 27770, training loss 6.863857269287109\n",
            "Evaluating Performance...\n",
            "pass 27780, training loss 8.24930477142334\n",
            "Evaluating Performance...\n",
            "pass 27790, training loss 10.772604942321777\n",
            "Evaluating Performance...\n",
            "pass 27800, training loss 7.876408100128174\n",
            "Evaluating Performance...\n",
            "pass 27810, training loss 6.235227584838867\n",
            "Evaluating Performance...\n",
            "pass 27820, training loss 8.900572776794434\n",
            "Evaluating Performance...\n",
            "pass 27830, training loss 6.529916286468506\n",
            "Evaluating Performance...\n",
            "pass 27840, training loss 10.903568267822266\n",
            "Evaluating Performance...\n",
            "pass 27850, training loss 7.346166610717773\n",
            "Evaluating Performance...\n",
            "pass 27860, training loss 8.293499946594238\n",
            "Evaluating Performance...\n",
            "pass 27870, training loss 11.825651168823242\n",
            "Evaluating Performance...\n",
            "pass 27880, training loss 9.26570987701416\n",
            "Evaluating Performance...\n",
            "pass 27890, training loss 8.919649124145508\n",
            "Evaluating Performance...\n",
            "pass 27900, training loss 10.133203506469727\n",
            "Evaluating Performance...\n",
            "pass 27910, training loss 7.223862648010254\n",
            "Evaluating Performance...\n",
            "pass 27920, training loss 6.94887638092041\n",
            "Evaluating Performance...\n",
            "pass 27930, training loss 8.54659652709961\n",
            "Evaluating Performance...\n",
            "pass 27940, training loss 11.354217529296875\n",
            "Evaluating Performance...\n",
            "pass 27950, training loss 8.110636711120605\n",
            "Evaluating Performance...\n",
            "pass 27960, training loss 10.447443962097168\n",
            "Evaluating Performance...\n",
            "pass 27970, training loss 9.137089729309082\n",
            "Evaluating Performance...\n",
            "pass 27980, training loss 8.447625160217285\n",
            "Evaluating Performance...\n",
            "pass 27990, training loss 8.204569816589355\n",
            "Evaluating Performance...\n",
            "pass 28000, training loss 8.962410926818848\n",
            "Evaluating Performance...\n",
            "pass 28010, training loss 9.01439094543457\n",
            "Evaluating Performance...\n",
            "pass 28020, training loss 8.466711044311523\n",
            "Evaluating Performance...\n",
            "pass 28030, training loss 8.736465454101562\n",
            "Evaluating Performance...\n",
            "pass 28040, training loss 9.872562408447266\n",
            "Evaluating Performance...\n",
            "pass 28050, training loss 7.811092376708984\n",
            "Evaluating Performance...\n",
            "pass 28060, training loss 8.601241111755371\n",
            "Evaluating Performance...\n",
            "pass 28070, training loss 9.12951374053955\n",
            "Evaluating Performance...\n",
            "pass 28080, training loss 7.575510501861572\n",
            "Evaluating Performance...\n",
            "pass 28090, training loss 8.303376197814941\n",
            "Evaluating Performance...\n",
            "pass 28100, training loss 10.570094108581543\n",
            "Evaluating Performance...\n",
            "pass 28110, training loss 8.664412498474121\n",
            "Evaluating Performance...\n",
            "pass 28120, training loss 9.028438568115234\n",
            "Evaluating Performance...\n",
            "pass 28130, training loss 9.990999221801758\n",
            "Evaluating Performance...\n",
            "pass 28140, training loss 10.623233795166016\n",
            "Evaluating Performance...\n",
            "pass 28150, training loss 7.0755133628845215\n",
            "Evaluating Performance...\n",
            "pass 28160, training loss 8.020735740661621\n",
            "Evaluating Performance...\n",
            "pass 28170, training loss 7.795322895050049\n",
            "Evaluating Performance...\n",
            "pass 28180, training loss 9.647441864013672\n",
            "Evaluating Performance...\n",
            "pass 28190, training loss 9.22745418548584\n",
            "Evaluating Performance...\n",
            "pass 28200, training loss 9.660053253173828\n",
            "Evaluating Performance...\n",
            "pass 28210, training loss 8.367450714111328\n",
            "Evaluating Performance...\n",
            "pass 28220, training loss 9.756124496459961\n",
            "Evaluating Performance...\n",
            "pass 28230, training loss 9.811460494995117\n",
            "Evaluating Performance...\n",
            "pass 28240, training loss 9.956608772277832\n",
            "Evaluating Performance...\n",
            "pass 28250, training loss 11.562728881835938\n",
            "Evaluating Performance...\n",
            "pass 28260, training loss 11.010951042175293\n",
            "Evaluating Performance...\n",
            "pass 28270, training loss 7.860940933227539\n",
            "Evaluating Performance...\n",
            "pass 28280, training loss 8.926162719726562\n",
            "Evaluating Performance...\n",
            "pass 28290, training loss 10.005974769592285\n",
            "Evaluating Performance...\n",
            "pass 28300, training loss 7.257291793823242\n",
            "Evaluating Performance...\n",
            "pass 28310, training loss 8.603850364685059\n",
            "Evaluating Performance...\n",
            "pass 28320, training loss 8.72174072265625\n",
            "Evaluating Performance...\n",
            "pass 28330, training loss 9.501490592956543\n",
            "Evaluating Performance...\n",
            "pass 28340, training loss 10.185394287109375\n",
            "Evaluating Performance...\n",
            "pass 28350, training loss 10.830899238586426\n",
            "Evaluating Performance...\n",
            "pass 28360, training loss 9.497652053833008\n",
            "Evaluating Performance...\n",
            "pass 28370, training loss 8.4268159866333\n",
            "Evaluating Performance...\n",
            "pass 28380, training loss 9.795859336853027\n",
            "Evaluating Performance...\n",
            "pass 28390, training loss 8.932693481445312\n",
            "Evaluating Performance...\n",
            "pass 28400, training loss 8.502077102661133\n",
            "Evaluating Performance...\n",
            "pass 28410, training loss 8.846601486206055\n",
            "Evaluating Performance...\n",
            "pass 28420, training loss 8.080842971801758\n",
            "Evaluating Performance...\n",
            "pass 28430, training loss 8.488655090332031\n",
            "Evaluating Performance...\n",
            "pass 28440, training loss 9.50204086303711\n",
            "Evaluating Performance...\n",
            "pass 28450, training loss 9.69831657409668\n",
            "Evaluating Performance...\n",
            "pass 28460, training loss 8.607403755187988\n",
            "Evaluating Performance...\n",
            "pass 28470, training loss 7.5828776359558105\n",
            "Evaluating Performance...\n",
            "pass 28480, training loss 12.019627571105957\n",
            "Evaluating Performance...\n",
            "pass 28490, training loss 6.9713053703308105\n",
            "Evaluating Performance...\n",
            "pass 28500, training loss 7.65936803817749\n",
            "Evaluating Performance...\n",
            "pass 28510, training loss 9.698391914367676\n",
            "Evaluating Performance...\n",
            "pass 28520, training loss 8.181379318237305\n",
            "Evaluating Performance...\n",
            "pass 28530, training loss 10.017654418945312\n",
            "Evaluating Performance...\n",
            "pass 28540, training loss 6.877760887145996\n",
            "Evaluating Performance...\n",
            "pass 28550, training loss 9.27070140838623\n",
            "Evaluating Performance...\n",
            "pass 28560, training loss 9.062297821044922\n",
            "Evaluating Performance...\n",
            "pass 28570, training loss 7.754800796508789\n",
            "Evaluating Performance...\n",
            "pass 28580, training loss 7.721700191497803\n",
            "Evaluating Performance...\n",
            "pass 28590, training loss 8.924134254455566\n",
            "Evaluating Performance...\n",
            "pass 28600, training loss 7.476569175720215\n",
            "Evaluating Performance...\n",
            "pass 28610, training loss 8.154853820800781\n",
            "Evaluating Performance...\n",
            "pass 28620, training loss 10.623753547668457\n",
            "Evaluating Performance...\n",
            "pass 28630, training loss 8.059822082519531\n",
            "Evaluating Performance...\n",
            "pass 28640, training loss 7.947258949279785\n",
            "Evaluating Performance...\n",
            "pass 28650, training loss 7.130119323730469\n",
            "Evaluating Performance...\n",
            "pass 28660, training loss 8.556486129760742\n",
            "Evaluating Performance...\n",
            "pass 28670, training loss 6.333385944366455\n",
            "Evaluating Performance...\n",
            "pass 28680, training loss 8.523648262023926\n",
            "Evaluating Performance...\n",
            "pass 28690, training loss 8.670578002929688\n",
            "Evaluating Performance...\n",
            "pass 28700, training loss 8.926794052124023\n",
            "Evaluating Performance...\n",
            "pass 28710, training loss 8.802570343017578\n",
            "Evaluating Performance...\n",
            "pass 28720, training loss 7.509788513183594\n",
            "Evaluating Performance...\n",
            "pass 28730, training loss 7.435576438903809\n",
            "Evaluating Performance...\n",
            "pass 28740, training loss 8.681190490722656\n",
            "Evaluating Performance...\n",
            "pass 28750, training loss 7.937070369720459\n",
            "Evaluating Performance...\n",
            "pass 28760, training loss 8.304715156555176\n",
            "Evaluating Performance...\n",
            "pass 28770, training loss 7.994164943695068\n",
            "Evaluating Performance...\n",
            "pass 28780, training loss 8.44543170928955\n",
            "Evaluating Performance...\n",
            "pass 28790, training loss 8.54061222076416\n",
            "Evaluating Performance...\n",
            "pass 28800, training loss 10.081182479858398\n",
            "Evaluating Performance...\n",
            "pass 28810, training loss 8.619166374206543\n",
            "Evaluating Performance...\n",
            "pass 28820, training loss 10.127121925354004\n",
            "Evaluating Performance...\n",
            "pass 28830, training loss 9.477293968200684\n",
            "Evaluating Performance...\n",
            "pass 28840, training loss 8.912651062011719\n",
            "Evaluating Performance...\n",
            "pass 28850, training loss 8.547959327697754\n",
            "Evaluating Performance...\n",
            "pass 28860, training loss 9.021708488464355\n",
            "Evaluating Performance...\n",
            "pass 28870, training loss 8.36258316040039\n",
            "Evaluating Performance...\n",
            "pass 28880, training loss 8.09809684753418\n",
            "Evaluating Performance...\n",
            "pass 28890, training loss 10.13786506652832\n",
            "Evaluating Performance...\n",
            "pass 28900, training loss 8.459010124206543\n",
            "Evaluating Performance...\n",
            "pass 28910, training loss 7.224997520446777\n",
            "Evaluating Performance...\n",
            "pass 28920, training loss 8.585586547851562\n",
            "Evaluating Performance...\n",
            "pass 28930, training loss 6.613112449645996\n",
            "Evaluating Performance...\n",
            "pass 28940, training loss 10.711663246154785\n",
            "Evaluating Performance...\n",
            "pass 28950, training loss 5.997434616088867\n",
            "Evaluating Performance...\n",
            "pass 28960, training loss 9.376175880432129\n",
            "Evaluating Performance...\n",
            "pass 28970, training loss 8.357247352600098\n",
            "Evaluating Performance...\n",
            "pass 28980, training loss 7.042215824127197\n",
            "Evaluating Performance...\n",
            "pass 28990, training loss 8.665322303771973\n",
            "Evaluating Performance...\n",
            "pass 29000, training loss 8.078347206115723\n",
            "Evaluating Performance...\n",
            "pass 29010, training loss 10.247562408447266\n",
            "Evaluating Performance...\n",
            "pass 29020, training loss 8.344486236572266\n",
            "Evaluating Performance...\n",
            "pass 29030, training loss 7.852451801300049\n",
            "Evaluating Performance...\n",
            "pass 29040, training loss 6.423301696777344\n",
            "Evaluating Performance...\n",
            "pass 29050, training loss 6.829586505889893\n",
            "Evaluating Performance...\n",
            "pass 29060, training loss 6.828981399536133\n",
            "Evaluating Performance...\n",
            "pass 29070, training loss 10.62789249420166\n",
            "Evaluating Performance...\n",
            "pass 29080, training loss 9.00667667388916\n",
            "Evaluating Performance...\n",
            "pass 29090, training loss 8.815324783325195\n",
            "Evaluating Performance...\n",
            "pass 29100, training loss 10.317768096923828\n",
            "Evaluating Performance...\n",
            "pass 29110, training loss 8.0768461227417\n",
            "Evaluating Performance...\n",
            "pass 29120, training loss 9.09428596496582\n",
            "Evaluating Performance...\n",
            "pass 29130, training loss 9.283578872680664\n",
            "Evaluating Performance...\n",
            "pass 29140, training loss 9.128337860107422\n",
            "Evaluating Performance...\n",
            "pass 29150, training loss 9.673553466796875\n",
            "Evaluating Performance...\n",
            "pass 29160, training loss 7.41832971572876\n",
            "Evaluating Performance...\n",
            "pass 29170, training loss 9.706300735473633\n",
            "Evaluating Performance...\n",
            "pass 29180, training loss 9.413581848144531\n",
            "Evaluating Performance...\n",
            "pass 29190, training loss 10.388392448425293\n",
            "Evaluating Performance...\n",
            "pass 29200, training loss 6.340216636657715\n",
            "Evaluating Performance...\n",
            "pass 29210, training loss 8.759049415588379\n",
            "Evaluating Performance...\n",
            "pass 29220, training loss 9.097512245178223\n",
            "Evaluating Performance...\n",
            "pass 29230, training loss 9.078569412231445\n",
            "Evaluating Performance...\n",
            "pass 29240, training loss 11.507912635803223\n",
            "Evaluating Performance...\n",
            "pass 29250, training loss 7.90191125869751\n",
            "Evaluating Performance...\n",
            "pass 29260, training loss 8.2716646194458\n",
            "Evaluating Performance...\n",
            "pass 29270, training loss 6.055603504180908\n",
            "Evaluating Performance...\n",
            "pass 29280, training loss 8.063606262207031\n",
            "Evaluating Performance...\n",
            "pass 29290, training loss 7.999383449554443\n",
            "Evaluating Performance...\n",
            "pass 29300, training loss 7.993818759918213\n",
            "Evaluating Performance...\n",
            "pass 29310, training loss 7.146258354187012\n",
            "Evaluating Performance...\n",
            "pass 29320, training loss 7.112862586975098\n",
            "Evaluating Performance...\n",
            "pass 29330, training loss 8.309027671813965\n",
            "Evaluating Performance...\n",
            "pass 29340, training loss 5.530311584472656\n",
            "Evaluating Performance...\n",
            "pass 29350, training loss 8.597468376159668\n",
            "Evaluating Performance...\n",
            "pass 29360, training loss 8.233067512512207\n",
            "Evaluating Performance...\n",
            "pass 29370, training loss 8.524560928344727\n",
            "Evaluating Performance...\n",
            "pass 29380, training loss 12.252079963684082\n",
            "Evaluating Performance...\n",
            "pass 29390, training loss 10.304996490478516\n",
            "Evaluating Performance...\n",
            "pass 29400, training loss 8.151947021484375\n",
            "Evaluating Performance...\n",
            "pass 29410, training loss 10.343653678894043\n",
            "Evaluating Performance...\n",
            "pass 29420, training loss 9.586675643920898\n",
            "Evaluating Performance...\n",
            "pass 29430, training loss 7.66248893737793\n",
            "Evaluating Performance...\n",
            "pass 29440, training loss 7.788266658782959\n",
            "Evaluating Performance...\n",
            "pass 29450, training loss 9.268606185913086\n",
            "Evaluating Performance...\n",
            "pass 29460, training loss 8.694169044494629\n",
            "Evaluating Performance...\n",
            "pass 29470, training loss 6.938732147216797\n",
            "Evaluating Performance...\n",
            "pass 29480, training loss 7.415939807891846\n",
            "Evaluating Performance...\n",
            "pass 29490, training loss 11.741495132446289\n",
            "Evaluating Performance...\n",
            "pass 29500, training loss 6.851068019866943\n",
            "Evaluating Performance...\n",
            "pass 29510, training loss 8.416418075561523\n",
            "Evaluating Performance...\n",
            "pass 29520, training loss 9.896831512451172\n",
            "Evaluating Performance...\n",
            "pass 29530, training loss 9.642660140991211\n",
            "Evaluating Performance...\n",
            "pass 29540, training loss 8.735219955444336\n",
            "Evaluating Performance...\n",
            "pass 29550, training loss 7.520770072937012\n",
            "Evaluating Performance...\n",
            "pass 29560, training loss 8.02636432647705\n",
            "Evaluating Performance...\n",
            "pass 29570, training loss 10.509115219116211\n",
            "Evaluating Performance...\n",
            "pass 29580, training loss 10.58425521850586\n",
            "Evaluating Performance...\n",
            "pass 29590, training loss 7.126595973968506\n",
            "Evaluating Performance...\n",
            "pass 29600, training loss 7.211662292480469\n",
            "Evaluating Performance...\n",
            "pass 29610, training loss 7.2264933586120605\n",
            "Evaluating Performance...\n",
            "pass 29620, training loss 9.437542915344238\n",
            "Evaluating Performance...\n",
            "pass 29630, training loss 7.614236354827881\n",
            "Evaluating Performance...\n",
            "pass 29640, training loss 9.572030067443848\n",
            "Evaluating Performance...\n",
            "pass 29650, training loss 10.87213134765625\n",
            "Evaluating Performance...\n",
            "pass 29660, training loss 7.197774887084961\n",
            "Evaluating Performance...\n",
            "pass 29670, training loss 9.05044937133789\n",
            "Evaluating Performance...\n",
            "pass 29680, training loss 9.18826675415039\n",
            "Evaluating Performance...\n",
            "pass 29690, training loss 7.865830898284912\n",
            "Evaluating Performance...\n",
            "pass 29700, training loss 8.649445533752441\n",
            "Evaluating Performance...\n",
            "pass 29710, training loss 9.197859764099121\n",
            "Evaluating Performance...\n",
            "pass 29720, training loss 9.757229804992676\n",
            "Evaluating Performance...\n",
            "pass 29730, training loss 6.580662250518799\n",
            "Evaluating Performance...\n",
            "pass 29740, training loss 8.969807624816895\n",
            "Evaluating Performance...\n",
            "pass 29750, training loss 11.407106399536133\n",
            "Evaluating Performance...\n",
            "pass 29760, training loss 8.688469886779785\n",
            "Evaluating Performance...\n",
            "pass 29770, training loss 11.2261962890625\n",
            "Evaluating Performance...\n",
            "pass 29780, training loss 8.065049171447754\n",
            "Evaluating Performance...\n",
            "pass 29790, training loss 9.542914390563965\n",
            "Evaluating Performance...\n",
            "pass 29800, training loss 9.894039154052734\n",
            "Evaluating Performance...\n",
            "pass 29810, training loss 9.888738632202148\n",
            "Evaluating Performance...\n",
            "pass 29820, training loss 10.129681587219238\n",
            "Evaluating Performance...\n",
            "pass 29830, training loss 9.387420654296875\n",
            "Evaluating Performance...\n",
            "pass 29840, training loss 6.838492393493652\n",
            "Evaluating Performance...\n",
            "pass 29850, training loss 7.705964088439941\n",
            "Evaluating Performance...\n",
            "pass 29860, training loss 7.52474308013916\n",
            "Evaluating Performance...\n",
            "pass 29870, training loss 11.411858558654785\n",
            "Evaluating Performance...\n",
            "pass 29880, training loss 6.48838472366333\n",
            "Evaluating Performance...\n",
            "pass 29890, training loss 10.41441535949707\n",
            "Evaluating Performance...\n",
            "pass 29900, training loss 9.490347862243652\n",
            "Evaluating Performance...\n",
            "pass 29910, training loss 8.257458686828613\n",
            "Evaluating Performance...\n",
            "pass 29920, training loss 7.206772327423096\n",
            "Evaluating Performance...\n",
            "pass 29930, training loss 7.860085964202881\n",
            "Evaluating Performance...\n",
            "pass 29940, training loss 6.200675010681152\n",
            "Evaluating Performance...\n",
            "pass 29950, training loss 10.88453197479248\n",
            "Evaluating Performance...\n",
            "pass 29960, training loss 9.011373519897461\n",
            "Evaluating Performance...\n",
            "pass 29970, training loss 7.950375080108643\n",
            "Evaluating Performance...\n",
            "pass 29980, training loss 9.48803424835205\n",
            "Evaluating Performance...\n",
            "pass 29990, training loss 7.093563079833984\n",
            "Evaluating Performance...\n",
            "pass 30000, training loss 8.558053016662598\n",
            "Saving Checkpoint...\n",
            "checkpoint saved\n",
            "Evaluating Performance...\n",
            "pass 30010, training loss 7.113842010498047\n",
            "Evaluating Performance...\n",
            "pass 30020, training loss 8.655946731567383\n",
            "Evaluating Performance...\n",
            "pass 30030, training loss 7.993820667266846\n",
            "Evaluating Performance...\n",
            "pass 30040, training loss 7.5747270584106445\n",
            "Evaluating Performance...\n",
            "pass 30050, training loss 9.141834259033203\n",
            "Evaluating Performance...\n",
            "pass 30060, training loss 9.272841453552246\n",
            "Evaluating Performance...\n",
            "pass 30070, training loss 6.4431376457214355\n",
            "Evaluating Performance...\n",
            "pass 30080, training loss 6.340388774871826\n",
            "Evaluating Performance...\n",
            "pass 30090, training loss 6.63913631439209\n",
            "Evaluating Performance...\n",
            "pass 30100, training loss 9.260103225708008\n",
            "Evaluating Performance...\n",
            "pass 30110, training loss 8.61754322052002\n",
            "Evaluating Performance...\n",
            "pass 30120, training loss 8.015727043151855\n",
            "Evaluating Performance...\n",
            "pass 30130, training loss 10.390229225158691\n",
            "Evaluating Performance...\n",
            "pass 30140, training loss 8.112662315368652\n",
            "Evaluating Performance...\n",
            "pass 30150, training loss 6.819122314453125\n",
            "Evaluating Performance...\n",
            "pass 30160, training loss 7.855332851409912\n",
            "Evaluating Performance...\n",
            "pass 30170, training loss 8.39427661895752\n",
            "Evaluating Performance...\n",
            "pass 30180, training loss 6.370291709899902\n",
            "Evaluating Performance...\n",
            "pass 30190, training loss 9.206625938415527\n",
            "Evaluating Performance...\n",
            "pass 30200, training loss 7.6822967529296875\n",
            "Evaluating Performance...\n",
            "pass 30210, training loss 9.886314392089844\n",
            "Evaluating Performance...\n",
            "pass 30220, training loss 9.36349868774414\n",
            "Evaluating Performance...\n",
            "pass 30230, training loss 6.910404682159424\n",
            "Evaluating Performance...\n",
            "pass 30240, training loss 8.71485710144043\n",
            "Evaluating Performance...\n",
            "pass 30250, training loss 6.455486297607422\n",
            "Evaluating Performance...\n",
            "pass 30260, training loss 7.836325645446777\n",
            "Evaluating Performance...\n",
            "pass 30270, training loss 8.953063011169434\n",
            "Evaluating Performance...\n",
            "pass 30280, training loss 9.320958137512207\n",
            "Evaluating Performance...\n",
            "pass 30290, training loss 9.202874183654785\n",
            "Evaluating Performance...\n",
            "pass 30300, training loss 8.635804176330566\n",
            "Evaluating Performance...\n",
            "pass 30310, training loss 8.24238109588623\n",
            "Evaluating Performance...\n",
            "pass 30320, training loss 8.463104248046875\n",
            "Evaluating Performance...\n",
            "pass 30330, training loss 7.820180892944336\n",
            "Evaluating Performance...\n",
            "pass 30340, training loss 8.684442520141602\n",
            "Evaluating Performance...\n",
            "pass 30350, training loss 9.009439468383789\n",
            "Evaluating Performance...\n",
            "pass 30360, training loss 8.743929862976074\n",
            "Evaluating Performance...\n",
            "pass 30370, training loss 8.177288055419922\n",
            "Evaluating Performance...\n",
            "pass 30380, training loss 11.295449256896973\n",
            "Evaluating Performance...\n",
            "pass 30390, training loss 6.977522850036621\n",
            "Evaluating Performance...\n",
            "pass 30400, training loss 10.691951751708984\n",
            "Evaluating Performance...\n",
            "pass 30410, training loss 8.824377059936523\n",
            "Evaluating Performance...\n",
            "pass 30420, training loss 8.507759094238281\n",
            "Evaluating Performance...\n",
            "pass 30430, training loss 7.13600492477417\n",
            "Evaluating Performance...\n",
            "pass 30440, training loss 7.492891788482666\n",
            "Evaluating Performance...\n",
            "pass 30450, training loss 9.727129936218262\n",
            "Evaluating Performance...\n",
            "pass 30460, training loss 8.622650146484375\n",
            "Evaluating Performance...\n",
            "pass 30470, training loss 9.470255851745605\n",
            "Evaluating Performance...\n",
            "pass 30480, training loss 8.473726272583008\n",
            "Evaluating Performance...\n",
            "pass 30490, training loss 9.23844051361084\n",
            "Evaluating Performance...\n",
            "pass 30500, training loss 8.480839729309082\n",
            "Evaluating Performance...\n",
            "pass 30510, training loss 8.30310344696045\n",
            "Evaluating Performance...\n",
            "pass 30520, training loss 8.015175819396973\n",
            "Evaluating Performance...\n",
            "pass 30530, training loss 7.569462299346924\n",
            "Evaluating Performance...\n",
            "pass 30540, training loss 7.864090919494629\n",
            "Evaluating Performance...\n",
            "pass 30550, training loss 8.165111541748047\n",
            "Evaluating Performance...\n",
            "pass 30560, training loss 7.383305549621582\n",
            "Evaluating Performance...\n",
            "pass 30570, training loss 8.55394458770752\n",
            "Evaluating Performance...\n",
            "pass 30580, training loss 6.54710054397583\n",
            "Evaluating Performance...\n",
            "pass 30590, training loss 9.051072120666504\n",
            "Evaluating Performance...\n",
            "pass 30600, training loss 8.00715160369873\n",
            "Evaluating Performance...\n",
            "pass 30610, training loss 7.339159965515137\n",
            "Evaluating Performance...\n",
            "pass 30620, training loss 8.905475616455078\n",
            "Evaluating Performance...\n",
            "pass 30630, training loss 9.43641185760498\n",
            "Evaluating Performance...\n",
            "pass 30640, training loss 8.509769439697266\n",
            "Evaluating Performance...\n",
            "pass 30650, training loss 11.99356460571289\n",
            "Evaluating Performance...\n",
            "pass 30660, training loss 8.217005729675293\n",
            "Evaluating Performance...\n",
            "pass 30670, training loss 7.902066707611084\n",
            "Evaluating Performance...\n",
            "pass 30680, training loss 10.250636100769043\n",
            "Evaluating Performance...\n",
            "pass 30690, training loss 6.691842555999756\n",
            "Evaluating Performance...\n",
            "pass 30700, training loss 8.292585372924805\n",
            "Evaluating Performance...\n",
            "pass 30710, training loss 7.15134859085083\n",
            "Evaluating Performance...\n",
            "pass 30720, training loss 8.574470520019531\n",
            "Evaluating Performance...\n",
            "pass 30730, training loss 12.364246368408203\n",
            "Evaluating Performance...\n",
            "pass 30740, training loss 7.538289546966553\n",
            "Evaluating Performance...\n",
            "pass 30750, training loss 8.378162384033203\n",
            "Evaluating Performance...\n",
            "pass 30760, training loss 10.701409339904785\n",
            "Evaluating Performance...\n",
            "pass 30770, training loss 7.2848381996154785\n",
            "Evaluating Performance...\n",
            "pass 30780, training loss 8.197479248046875\n",
            "Evaluating Performance...\n",
            "pass 30790, training loss 8.818334579467773\n",
            "Evaluating Performance...\n",
            "pass 30800, training loss 7.974355697631836\n",
            "Evaluating Performance...\n",
            "pass 30810, training loss 9.660309791564941\n",
            "Evaluating Performance...\n",
            "pass 30820, training loss 8.357226371765137\n",
            "Evaluating Performance...\n",
            "pass 30830, training loss 7.097014427185059\n",
            "Evaluating Performance...\n",
            "pass 30840, training loss 8.562546730041504\n",
            "Evaluating Performance...\n",
            "pass 30850, training loss 10.870184898376465\n",
            "Evaluating Performance...\n",
            "pass 30860, training loss 8.366119384765625\n",
            "Evaluating Performance...\n",
            "pass 30870, training loss 7.399486064910889\n",
            "Evaluating Performance...\n",
            "pass 30880, training loss 9.402593612670898\n",
            "Evaluating Performance...\n",
            "pass 30890, training loss 8.992438316345215\n",
            "Evaluating Performance...\n",
            "pass 30900, training loss 7.21194314956665\n",
            "Evaluating Performance...\n",
            "pass 30910, training loss 9.840690612792969\n",
            "Evaluating Performance...\n",
            "pass 30920, training loss 7.4675517082214355\n",
            "Evaluating Performance...\n",
            "pass 30930, training loss 10.738809585571289\n",
            "Evaluating Performance...\n",
            "pass 30940, training loss 8.280052185058594\n",
            "Evaluating Performance...\n",
            "pass 30950, training loss 8.996795654296875\n",
            "Evaluating Performance...\n",
            "pass 30960, training loss 7.956437110900879\n",
            "Evaluating Performance...\n",
            "pass 30970, training loss 7.270388603210449\n",
            "Evaluating Performance...\n",
            "pass 30980, training loss 8.667362213134766\n",
            "Evaluating Performance...\n",
            "pass 30990, training loss 6.966096878051758\n",
            "Evaluating Performance...\n",
            "pass 31000, training loss 9.51045036315918\n",
            "Evaluating Performance...\n",
            "pass 31010, training loss 7.382948875427246\n",
            "Evaluating Performance...\n",
            "pass 31020, training loss 7.24592399597168\n",
            "Evaluating Performance...\n",
            "pass 31030, training loss 8.690197944641113\n",
            "Evaluating Performance...\n",
            "pass 31040, training loss 7.6653971672058105\n",
            "Evaluating Performance...\n",
            "pass 31050, training loss 10.315312385559082\n",
            "Evaluating Performance...\n",
            "pass 31060, training loss 8.947216987609863\n",
            "Evaluating Performance...\n",
            "pass 31070, training loss 8.018202781677246\n",
            "Evaluating Performance...\n",
            "pass 31080, training loss 8.466552734375\n",
            "Evaluating Performance...\n",
            "pass 31090, training loss 6.162825584411621\n",
            "Evaluating Performance...\n",
            "pass 31100, training loss 6.770766258239746\n",
            "Evaluating Performance...\n",
            "pass 31110, training loss 8.390048027038574\n",
            "Evaluating Performance...\n",
            "pass 31120, training loss 7.633090496063232\n",
            "Evaluating Performance...\n",
            "pass 31130, training loss 11.050491333007812\n",
            "Evaluating Performance...\n",
            "pass 31140, training loss 8.322588920593262\n",
            "Evaluating Performance...\n",
            "pass 31150, training loss 7.997862815856934\n",
            "Evaluating Performance...\n",
            "pass 31160, training loss 10.029980659484863\n",
            "Evaluating Performance...\n",
            "pass 31170, training loss 7.748787879943848\n",
            "Evaluating Performance...\n",
            "pass 31180, training loss 7.748958110809326\n",
            "Evaluating Performance...\n",
            "pass 31190, training loss 7.284908294677734\n",
            "Evaluating Performance...\n",
            "pass 31200, training loss 7.239131450653076\n",
            "Evaluating Performance...\n",
            "pass 31210, training loss 8.762228012084961\n",
            "Evaluating Performance...\n",
            "pass 31220, training loss 8.859910011291504\n",
            "Evaluating Performance...\n",
            "pass 31230, training loss 8.737096786499023\n",
            "Evaluating Performance...\n",
            "pass 31240, training loss 10.86408805847168\n",
            "Evaluating Performance...\n",
            "pass 31250, training loss 7.330672264099121\n",
            "Evaluating Performance...\n",
            "pass 31260, training loss 7.917031288146973\n",
            "Evaluating Performance...\n",
            "pass 31270, training loss 12.752397537231445\n",
            "Evaluating Performance...\n",
            "pass 31280, training loss 6.061105251312256\n",
            "Evaluating Performance...\n",
            "pass 31290, training loss 8.89234447479248\n",
            "Evaluating Performance...\n",
            "pass 31300, training loss 7.832822799682617\n",
            "Evaluating Performance...\n",
            "pass 31310, training loss 10.709537506103516\n",
            "Evaluating Performance...\n",
            "pass 31320, training loss 6.0739569664001465\n",
            "Evaluating Performance...\n",
            "pass 31330, training loss 8.867634773254395\n",
            "Evaluating Performance...\n",
            "pass 31340, training loss 8.061525344848633\n",
            "Evaluating Performance...\n",
            "pass 31350, training loss 9.086739540100098\n",
            "Evaluating Performance...\n",
            "pass 31360, training loss 7.066121578216553\n",
            "Evaluating Performance...\n",
            "pass 31370, training loss 7.767337322235107\n",
            "Evaluating Performance...\n",
            "pass 31380, training loss 9.445608139038086\n",
            "Evaluating Performance...\n",
            "pass 31390, training loss 8.016833305358887\n",
            "Evaluating Performance...\n",
            "pass 31400, training loss 9.324469566345215\n",
            "Evaluating Performance...\n",
            "pass 31410, training loss 9.015901565551758\n",
            "Evaluating Performance...\n",
            "pass 31420, training loss 6.996730327606201\n",
            "Evaluating Performance...\n",
            "pass 31430, training loss 7.359314918518066\n",
            "Evaluating Performance...\n",
            "pass 31440, training loss 9.527593612670898\n",
            "Evaluating Performance...\n",
            "pass 31450, training loss 10.322755813598633\n",
            "Evaluating Performance...\n",
            "pass 31460, training loss 9.408692359924316\n",
            "Evaluating Performance...\n",
            "pass 31470, training loss 7.359296798706055\n",
            "Evaluating Performance...\n",
            "pass 31480, training loss 5.73929500579834\n",
            "Evaluating Performance...\n",
            "pass 31490, training loss 7.035707950592041\n",
            "Evaluating Performance...\n",
            "pass 31500, training loss 9.935362815856934\n",
            "Evaluating Performance...\n",
            "pass 31510, training loss 8.568263053894043\n",
            "Evaluating Performance...\n",
            "pass 31520, training loss 8.693257331848145\n",
            "Evaluating Performance...\n",
            "pass 31530, training loss 8.654653549194336\n",
            "Evaluating Performance...\n",
            "pass 31540, training loss 7.212779521942139\n",
            "Evaluating Performance...\n",
            "pass 31550, training loss 7.954222679138184\n",
            "Evaluating Performance...\n",
            "pass 31560, training loss 8.920384407043457\n",
            "Evaluating Performance...\n",
            "pass 31570, training loss 6.332355976104736\n",
            "Evaluating Performance...\n",
            "pass 31580, training loss 6.363142967224121\n",
            "Evaluating Performance...\n",
            "pass 31590, training loss 8.182957649230957\n",
            "Evaluating Performance...\n",
            "pass 31600, training loss 8.411714553833008\n",
            "Evaluating Performance...\n",
            "pass 31610, training loss 11.140873908996582\n",
            "Evaluating Performance...\n",
            "pass 31620, training loss 13.194059371948242\n",
            "Evaluating Performance...\n",
            "pass 31630, training loss 8.071406364440918\n",
            "Evaluating Performance...\n",
            "pass 31640, training loss 8.026044845581055\n",
            "Evaluating Performance...\n",
            "pass 31650, training loss 10.995916366577148\n",
            "Evaluating Performance...\n",
            "pass 31660, training loss 6.659590721130371\n",
            "Evaluating Performance...\n",
            "pass 31670, training loss 8.65692138671875\n",
            "Evaluating Performance...\n",
            "pass 31680, training loss 9.037349700927734\n",
            "Evaluating Performance...\n",
            "pass 31690, training loss 6.403322219848633\n",
            "Evaluating Performance...\n",
            "pass 31700, training loss 8.409218788146973\n",
            "Evaluating Performance...\n",
            "pass 31710, training loss 7.673215866088867\n",
            "Evaluating Performance...\n",
            "pass 31720, training loss 7.965057373046875\n",
            "Evaluating Performance...\n",
            "pass 31730, training loss 8.21992301940918\n",
            "Evaluating Performance...\n",
            "pass 31740, training loss 11.216562271118164\n",
            "Evaluating Performance...\n",
            "pass 31750, training loss 8.161406517028809\n",
            "Evaluating Performance...\n",
            "pass 31760, training loss 10.050670623779297\n",
            "Evaluating Performance...\n",
            "pass 31770, training loss 6.884982109069824\n",
            "Evaluating Performance...\n",
            "pass 31780, training loss 8.155232429504395\n",
            "Evaluating Performance...\n",
            "pass 31790, training loss 8.438750267028809\n",
            "Evaluating Performance...\n",
            "pass 31800, training loss 8.177785873413086\n",
            "Evaluating Performance...\n",
            "pass 31810, training loss 6.9886908531188965\n",
            "Evaluating Performance...\n",
            "pass 31820, training loss 7.767673969268799\n",
            "Evaluating Performance...\n",
            "pass 31830, training loss 5.953314304351807\n",
            "Evaluating Performance...\n",
            "pass 31840, training loss 8.587434768676758\n",
            "Evaluating Performance...\n",
            "pass 31850, training loss 8.649516105651855\n",
            "Evaluating Performance...\n",
            "pass 31860, training loss 7.26374626159668\n",
            "Evaluating Performance...\n",
            "pass 31870, training loss 8.874004364013672\n",
            "Evaluating Performance...\n",
            "pass 31880, training loss 11.126754760742188\n",
            "Evaluating Performance...\n",
            "pass 31890, training loss 8.208307266235352\n",
            "Evaluating Performance...\n",
            "pass 31900, training loss 13.089173316955566\n",
            "Evaluating Performance...\n",
            "pass 31910, training loss 9.226594924926758\n",
            "Evaluating Performance...\n",
            "pass 31920, training loss 9.788381576538086\n",
            "Evaluating Performance...\n",
            "pass 31930, training loss 7.559810638427734\n",
            "Evaluating Performance...\n",
            "pass 31940, training loss 8.563156127929688\n",
            "Evaluating Performance...\n",
            "pass 31950, training loss 7.512159824371338\n",
            "Evaluating Performance...\n",
            "pass 31960, training loss 8.687413215637207\n",
            "Evaluating Performance...\n",
            "pass 31970, training loss 8.77789306640625\n",
            "Evaluating Performance...\n",
            "pass 31980, training loss 9.459698677062988\n",
            "Evaluating Performance...\n",
            "pass 31990, training loss 7.510830402374268\n",
            "Evaluating Performance...\n",
            "pass 32000, training loss 8.348430633544922\n",
            "Evaluating Performance...\n",
            "pass 32010, training loss 8.855856895446777\n",
            "Evaluating Performance...\n",
            "pass 32020, training loss 7.48707914352417\n",
            "Evaluating Performance...\n",
            "pass 32030, training loss 6.278867244720459\n",
            "Evaluating Performance...\n",
            "pass 32040, training loss 9.121801376342773\n",
            "Evaluating Performance...\n",
            "pass 32050, training loss 7.605710983276367\n",
            "Evaluating Performance...\n",
            "pass 32060, training loss 8.798709869384766\n",
            "Evaluating Performance...\n",
            "pass 32070, training loss 7.5191569328308105\n",
            "Evaluating Performance...\n",
            "pass 32080, training loss 8.253674507141113\n",
            "Evaluating Performance...\n",
            "pass 32090, training loss 7.746837139129639\n",
            "Evaluating Performance...\n",
            "pass 32100, training loss 9.937807083129883\n",
            "Evaluating Performance...\n",
            "pass 32110, training loss 8.269874572753906\n",
            "Evaluating Performance...\n",
            "pass 32120, training loss 7.979575157165527\n",
            "Evaluating Performance...\n",
            "pass 32130, training loss 6.8312273025512695\n",
            "Evaluating Performance...\n",
            "pass 32140, training loss 8.667076110839844\n",
            "Evaluating Performance...\n",
            "pass 32150, training loss 7.838449478149414\n",
            "Evaluating Performance...\n",
            "pass 32160, training loss 9.330199241638184\n",
            "Evaluating Performance...\n",
            "pass 32170, training loss 10.078634262084961\n",
            "Evaluating Performance...\n",
            "pass 32180, training loss 8.42273998260498\n",
            "Evaluating Performance...\n",
            "pass 32190, training loss 8.61917781829834\n",
            "Evaluating Performance...\n",
            "pass 32200, training loss 10.422718048095703\n",
            "Evaluating Performance...\n",
            "pass 32210, training loss 7.8929619789123535\n",
            "Evaluating Performance...\n",
            "pass 32220, training loss 8.758628845214844\n",
            "Evaluating Performance...\n",
            "pass 32230, training loss 9.117000579833984\n",
            "Evaluating Performance...\n",
            "pass 32240, training loss 8.0159330368042\n",
            "Evaluating Performance...\n",
            "pass 32250, training loss 7.76370096206665\n",
            "Evaluating Performance...\n",
            "pass 32260, training loss 8.150884628295898\n",
            "Evaluating Performance...\n",
            "pass 32270, training loss 8.425030708312988\n",
            "Evaluating Performance...\n",
            "pass 32280, training loss 8.578022003173828\n",
            "Evaluating Performance...\n",
            "pass 32290, training loss 7.341490745544434\n",
            "Evaluating Performance...\n",
            "pass 32300, training loss 7.1847825050354\n",
            "Evaluating Performance...\n",
            "pass 32310, training loss 8.36678695678711\n",
            "Evaluating Performance...\n",
            "pass 32320, training loss 8.813467025756836\n",
            "Evaluating Performance...\n",
            "pass 32330, training loss 8.29629135131836\n",
            "Evaluating Performance...\n",
            "pass 32340, training loss 6.5921244621276855\n",
            "Evaluating Performance...\n",
            "pass 32350, training loss 6.877543926239014\n",
            "Evaluating Performance...\n",
            "pass 32360, training loss 6.57002067565918\n",
            "Evaluating Performance...\n",
            "pass 32370, training loss 7.922733783721924\n",
            "Evaluating Performance...\n",
            "pass 32380, training loss 8.72733211517334\n",
            "Evaluating Performance...\n",
            "pass 32390, training loss 7.023491859436035\n",
            "Evaluating Performance...\n",
            "pass 32400, training loss 7.218286514282227\n",
            "Evaluating Performance...\n",
            "pass 32410, training loss 8.157602310180664\n",
            "Evaluating Performance...\n",
            "pass 32420, training loss 6.661520004272461\n",
            "Evaluating Performance...\n",
            "pass 32430, training loss 7.074734210968018\n",
            "Evaluating Performance...\n",
            "pass 32440, training loss 8.810781478881836\n",
            "Evaluating Performance...\n",
            "pass 32450, training loss 8.653406143188477\n",
            "Evaluating Performance...\n",
            "pass 32460, training loss 6.4913010597229\n",
            "Evaluating Performance...\n",
            "pass 32470, training loss 9.692209243774414\n",
            "Evaluating Performance...\n",
            "pass 32480, training loss 7.830000877380371\n",
            "Evaluating Performance...\n",
            "pass 32490, training loss 9.713278770446777\n",
            "Evaluating Performance...\n",
            "pass 32500, training loss 7.915795803070068\n",
            "Evaluating Performance...\n",
            "pass 32510, training loss 7.264798641204834\n",
            "Evaluating Performance...\n",
            "pass 32520, training loss 8.628705024719238\n",
            "Evaluating Performance...\n",
            "pass 32530, training loss 6.198088645935059\n",
            "Evaluating Performance...\n",
            "pass 32540, training loss 8.41633415222168\n",
            "Evaluating Performance...\n",
            "pass 32550, training loss 7.475715160369873\n",
            "Evaluating Performance...\n",
            "pass 32560, training loss 8.884522438049316\n",
            "Evaluating Performance...\n",
            "pass 32570, training loss 8.381453514099121\n",
            "Evaluating Performance...\n",
            "pass 32580, training loss 8.292704582214355\n",
            "Evaluating Performance...\n",
            "pass 32590, training loss 7.6085205078125\n",
            "Evaluating Performance...\n",
            "pass 32600, training loss 8.958144187927246\n",
            "Evaluating Performance...\n",
            "pass 32610, training loss 8.670303344726562\n",
            "Evaluating Performance...\n",
            "pass 32620, training loss 8.800068855285645\n",
            "Evaluating Performance...\n",
            "pass 32630, training loss 6.103509902954102\n",
            "Evaluating Performance...\n",
            "pass 32640, training loss 8.37409496307373\n",
            "Evaluating Performance...\n",
            "pass 32650, training loss 7.576261043548584\n",
            "Evaluating Performance...\n",
            "pass 32660, training loss 6.838300704956055\n",
            "Evaluating Performance...\n",
            "pass 32670, training loss 7.077880859375\n",
            "Evaluating Performance...\n",
            "pass 32680, training loss 9.14727783203125\n",
            "Evaluating Performance...\n",
            "pass 32690, training loss 6.683955669403076\n",
            "Evaluating Performance...\n",
            "pass 32700, training loss 8.273140907287598\n",
            "Evaluating Performance...\n",
            "pass 32710, training loss 7.884925365447998\n",
            "Evaluating Performance...\n",
            "pass 32720, training loss 7.063714027404785\n",
            "Evaluating Performance...\n",
            "pass 32730, training loss 10.98676872253418\n",
            "Evaluating Performance...\n",
            "pass 32740, training loss 8.749842643737793\n",
            "Evaluating Performance...\n",
            "pass 32750, training loss 6.367466449737549\n",
            "Evaluating Performance...\n",
            "pass 32760, training loss 8.023879051208496\n",
            "Evaluating Performance...\n",
            "pass 32770, training loss 11.645085334777832\n",
            "Evaluating Performance...\n",
            "pass 32780, training loss 8.834769248962402\n",
            "Evaluating Performance...\n",
            "pass 32790, training loss 7.104446887969971\n",
            "Evaluating Performance...\n",
            "pass 32800, training loss 8.351720809936523\n",
            "Evaluating Performance...\n",
            "pass 32810, training loss 7.550243377685547\n",
            "Evaluating Performance...\n",
            "pass 32820, training loss 8.159235954284668\n",
            "Evaluating Performance...\n",
            "pass 32830, training loss 9.01120376586914\n",
            "Evaluating Performance...\n",
            "pass 32840, training loss 9.910840034484863\n",
            "Evaluating Performance...\n",
            "pass 32850, training loss 10.534539222717285\n",
            "Evaluating Performance...\n",
            "pass 32860, training loss 9.304731369018555\n",
            "Evaluating Performance...\n",
            "pass 32870, training loss 8.340456008911133\n",
            "Evaluating Performance...\n",
            "pass 32880, training loss 8.436400413513184\n",
            "Evaluating Performance...\n",
            "pass 32890, training loss 7.680363178253174\n",
            "Evaluating Performance...\n",
            "pass 32900, training loss 7.2082648277282715\n",
            "Evaluating Performance...\n",
            "pass 32910, training loss 11.540772438049316\n",
            "Evaluating Performance...\n",
            "pass 32920, training loss 6.6200761795043945\n",
            "Evaluating Performance...\n",
            "pass 32930, training loss 7.960146427154541\n",
            "Evaluating Performance...\n",
            "pass 32940, training loss 8.826814651489258\n",
            "Evaluating Performance...\n",
            "pass 32950, training loss 5.8627753257751465\n",
            "Evaluating Performance...\n",
            "pass 32960, training loss 9.077155113220215\n",
            "Evaluating Performance...\n",
            "pass 32970, training loss 6.692667484283447\n",
            "Evaluating Performance...\n",
            "pass 32980, training loss 9.512760162353516\n",
            "Evaluating Performance...\n",
            "pass 32990, training loss 6.15008544921875\n",
            "Evaluating Performance...\n",
            "pass 33000, training loss 8.78034782409668\n",
            "Evaluating Performance...\n",
            "pass 33010, training loss 13.02183723449707\n",
            "Evaluating Performance...\n",
            "pass 33020, training loss 9.351712226867676\n",
            "Evaluating Performance...\n",
            "pass 33030, training loss 6.774259567260742\n",
            "Evaluating Performance...\n",
            "pass 33040, training loss 7.693101406097412\n",
            "Evaluating Performance...\n",
            "pass 33050, training loss 7.587855815887451\n",
            "Evaluating Performance...\n",
            "pass 33060, training loss 7.33393669128418\n",
            "Evaluating Performance...\n",
            "pass 33070, training loss 8.142102241516113\n",
            "Evaluating Performance...\n",
            "pass 33080, training loss 7.736089706420898\n",
            "Evaluating Performance...\n",
            "pass 33090, training loss 8.869473457336426\n",
            "Evaluating Performance...\n",
            "pass 33100, training loss 7.765511989593506\n",
            "Evaluating Performance...\n",
            "pass 33110, training loss 9.487283706665039\n",
            "Evaluating Performance...\n",
            "pass 33120, training loss 7.589156150817871\n",
            "Evaluating Performance...\n",
            "pass 33130, training loss 8.379419326782227\n",
            "Evaluating Performance...\n",
            "pass 33140, training loss 6.596772193908691\n",
            "Evaluating Performance...\n",
            "pass 33150, training loss 6.788849830627441\n",
            "Evaluating Performance...\n",
            "pass 33160, training loss 7.633706569671631\n",
            "Evaluating Performance...\n",
            "pass 33170, training loss 9.00483226776123\n",
            "Evaluating Performance...\n",
            "pass 33180, training loss 8.628877639770508\n",
            "Evaluating Performance...\n",
            "pass 33190, training loss 8.364395141601562\n",
            "Evaluating Performance...\n",
            "pass 33200, training loss 9.9713134765625\n",
            "Evaluating Performance...\n",
            "pass 33210, training loss 9.879642486572266\n",
            "Evaluating Performance...\n",
            "pass 33220, training loss 6.6376633644104\n",
            "Evaluating Performance...\n",
            "pass 33230, training loss 11.25312328338623\n",
            "Evaluating Performance...\n",
            "pass 33240, training loss 8.09453296661377\n",
            "Evaluating Performance...\n",
            "pass 33250, training loss 8.0105562210083\n",
            "Evaluating Performance...\n",
            "pass 33260, training loss 7.765660285949707\n",
            "Evaluating Performance...\n",
            "pass 33270, training loss 6.872843265533447\n",
            "Evaluating Performance...\n",
            "pass 33280, training loss 6.57529878616333\n",
            "Evaluating Performance...\n",
            "pass 33290, training loss 7.694767951965332\n",
            "Evaluating Performance...\n",
            "pass 33300, training loss 7.469460487365723\n",
            "Evaluating Performance...\n",
            "pass 33310, training loss 8.803228378295898\n",
            "Evaluating Performance...\n",
            "pass 33320, training loss 8.145771026611328\n",
            "Evaluating Performance...\n",
            "pass 33330, training loss 7.727134704589844\n",
            "Evaluating Performance...\n",
            "pass 33340, training loss 9.80877685546875\n",
            "Evaluating Performance...\n",
            "pass 33350, training loss 8.764599800109863\n",
            "Evaluating Performance...\n",
            "pass 33360, training loss 7.165600299835205\n",
            "Evaluating Performance...\n",
            "pass 33370, training loss 6.961319923400879\n",
            "Evaluating Performance...\n",
            "pass 33380, training loss 8.301667213439941\n",
            "Evaluating Performance...\n",
            "pass 33390, training loss 8.732107162475586\n",
            "Evaluating Performance...\n",
            "pass 33400, training loss 8.831793785095215\n",
            "Evaluating Performance...\n",
            "pass 33410, training loss 7.977762699127197\n",
            "Evaluating Performance...\n",
            "pass 33420, training loss 8.289066314697266\n",
            "Evaluating Performance...\n",
            "pass 33430, training loss 9.289993286132812\n",
            "Evaluating Performance...\n",
            "pass 33440, training loss 8.394644737243652\n",
            "Evaluating Performance...\n",
            "pass 33450, training loss 8.970288276672363\n",
            "Evaluating Performance...\n",
            "pass 33460, training loss 8.801493644714355\n",
            "Evaluating Performance...\n",
            "pass 33470, training loss 9.22779369354248\n",
            "Evaluating Performance...\n",
            "pass 33480, training loss 9.60566520690918\n",
            "Evaluating Performance...\n",
            "pass 33490, training loss 8.63207721710205\n",
            "Evaluating Performance...\n",
            "pass 33500, training loss 7.725839614868164\n",
            "Evaluating Performance...\n",
            "pass 33510, training loss 9.011703491210938\n",
            "Evaluating Performance...\n",
            "pass 33520, training loss 9.300640106201172\n",
            "Evaluating Performance...\n",
            "pass 33530, training loss 7.2574896812438965\n",
            "Evaluating Performance...\n",
            "pass 33540, training loss 8.4332275390625\n",
            "Evaluating Performance...\n",
            "pass 33550, training loss 6.940610408782959\n",
            "Evaluating Performance...\n",
            "pass 33560, training loss 7.752590656280518\n",
            "Evaluating Performance...\n",
            "pass 33570, training loss 11.06407356262207\n",
            "Evaluating Performance...\n",
            "pass 33580, training loss 7.630770683288574\n",
            "Evaluating Performance...\n",
            "pass 33590, training loss 9.493090629577637\n",
            "Evaluating Performance...\n",
            "pass 33600, training loss 8.874849319458008\n",
            "Evaluating Performance...\n",
            "pass 33610, training loss 10.770634651184082\n",
            "Evaluating Performance...\n",
            "pass 33620, training loss 7.73458194732666\n",
            "Evaluating Performance...\n",
            "pass 33630, training loss 7.521663665771484\n",
            "Evaluating Performance...\n",
            "pass 33640, training loss 10.668408393859863\n",
            "Evaluating Performance...\n",
            "pass 33650, training loss 7.699410438537598\n",
            "Evaluating Performance...\n",
            "pass 33660, training loss 8.487496376037598\n",
            "Evaluating Performance...\n",
            "pass 33670, training loss 6.804923057556152\n",
            "Evaluating Performance...\n",
            "pass 33680, training loss 9.150318145751953\n",
            "Evaluating Performance...\n",
            "pass 33690, training loss 10.131735801696777\n",
            "Evaluating Performance...\n",
            "pass 33700, training loss 8.997130393981934\n",
            "Evaluating Performance...\n",
            "pass 33710, training loss 8.12692928314209\n",
            "Evaluating Performance...\n",
            "pass 33720, training loss 7.583773612976074\n",
            "Evaluating Performance...\n",
            "pass 33730, training loss 8.01374626159668\n",
            "Evaluating Performance...\n",
            "pass 33740, training loss 9.153464317321777\n",
            "Evaluating Performance...\n",
            "pass 33750, training loss 7.941525936126709\n",
            "Evaluating Performance...\n",
            "pass 33760, training loss 6.357490062713623\n",
            "Evaluating Performance...\n",
            "pass 33770, training loss 10.591212272644043\n",
            "Evaluating Performance...\n",
            "pass 33780, training loss 7.587497234344482\n",
            "Evaluating Performance...\n",
            "pass 33790, training loss 7.982437610626221\n",
            "Evaluating Performance...\n",
            "pass 33800, training loss 7.248386859893799\n",
            "Evaluating Performance...\n",
            "pass 33810, training loss 7.7439799308776855\n",
            "Evaluating Performance...\n",
            "pass 33820, training loss 9.45842456817627\n",
            "Evaluating Performance...\n",
            "pass 33830, training loss 8.679632186889648\n",
            "Evaluating Performance...\n",
            "pass 33840, training loss 10.514474868774414\n",
            "Evaluating Performance...\n",
            "pass 33850, training loss 8.47488784790039\n",
            "Evaluating Performance...\n",
            "pass 33860, training loss 7.286177158355713\n",
            "Evaluating Performance...\n",
            "pass 33870, training loss 10.875832557678223\n",
            "Evaluating Performance...\n",
            "pass 33880, training loss 7.346116542816162\n",
            "Evaluating Performance...\n",
            "pass 33890, training loss 7.052900791168213\n",
            "Evaluating Performance...\n",
            "pass 33900, training loss 7.114565849304199\n",
            "Evaluating Performance...\n",
            "pass 33910, training loss 8.9944429397583\n",
            "Evaluating Performance...\n",
            "pass 33920, training loss 6.8256072998046875\n",
            "Evaluating Performance...\n",
            "pass 33930, training loss 6.200883865356445\n",
            "Evaluating Performance...\n",
            "pass 33940, training loss 8.918940544128418\n",
            "Evaluating Performance...\n",
            "pass 33950, training loss 7.792656898498535\n",
            "Evaluating Performance...\n",
            "pass 33960, training loss 7.698101043701172\n",
            "Evaluating Performance...\n",
            "pass 33970, training loss 9.760894775390625\n",
            "Evaluating Performance...\n",
            "pass 33980, training loss 10.593816757202148\n",
            "Evaluating Performance...\n",
            "pass 33990, training loss 8.390121459960938\n",
            "Evaluating Performance...\n",
            "pass 34000, training loss 7.333591938018799\n",
            "Evaluating Performance...\n",
            "pass 34010, training loss 5.5179853439331055\n",
            "Evaluating Performance...\n",
            "pass 34020, training loss 6.07849645614624\n",
            "Evaluating Performance...\n",
            "pass 34030, training loss 9.100496292114258\n",
            "Evaluating Performance...\n",
            "pass 34040, training loss 8.464995384216309\n",
            "Evaluating Performance...\n",
            "pass 34050, training loss 8.805007934570312\n",
            "Evaluating Performance...\n",
            "pass 34060, training loss 9.599863052368164\n",
            "Evaluating Performance...\n",
            "pass 34070, training loss 7.486057758331299\n",
            "Evaluating Performance...\n",
            "pass 34080, training loss 8.50943660736084\n",
            "Evaluating Performance...\n",
            "pass 34090, training loss 7.553700923919678\n",
            "Evaluating Performance...\n",
            "pass 34100, training loss 9.52790641784668\n",
            "Evaluating Performance...\n",
            "pass 34110, training loss 9.641712188720703\n",
            "Evaluating Performance...\n",
            "pass 34120, training loss 8.77786636352539\n",
            "Evaluating Performance...\n",
            "pass 34130, training loss 8.552957534790039\n",
            "Evaluating Performance...\n",
            "pass 34140, training loss 6.766706466674805\n",
            "Evaluating Performance...\n",
            "pass 34150, training loss 7.022777080535889\n",
            "Evaluating Performance...\n",
            "pass 34160, training loss 7.109934329986572\n",
            "Evaluating Performance...\n",
            "pass 34170, training loss 9.052098274230957\n",
            "Evaluating Performance...\n",
            "pass 34180, training loss 8.319544792175293\n",
            "Evaluating Performance...\n",
            "pass 34190, training loss 7.3607177734375\n",
            "Evaluating Performance...\n",
            "pass 34200, training loss 7.777687072753906\n",
            "Evaluating Performance...\n",
            "pass 34210, training loss 6.81823205947876\n",
            "Evaluating Performance...\n",
            "pass 34220, training loss 9.62902545928955\n",
            "Evaluating Performance...\n",
            "pass 34230, training loss 8.68475341796875\n",
            "Evaluating Performance...\n",
            "pass 34240, training loss 8.593852996826172\n",
            "Evaluating Performance...\n",
            "pass 34250, training loss 6.749701976776123\n",
            "Evaluating Performance...\n",
            "pass 34260, training loss 8.460246086120605\n",
            "Evaluating Performance...\n",
            "pass 34270, training loss 7.5210089683532715\n",
            "Evaluating Performance...\n",
            "pass 34280, training loss 8.498030662536621\n",
            "Evaluating Performance...\n",
            "pass 34290, training loss 9.501659393310547\n",
            "Evaluating Performance...\n",
            "pass 34300, training loss 7.429964542388916\n",
            "Evaluating Performance...\n",
            "pass 34310, training loss 6.971495151519775\n",
            "Evaluating Performance...\n",
            "pass 34320, training loss 8.306340217590332\n",
            "Evaluating Performance...\n",
            "pass 34330, training loss 6.495468616485596\n",
            "Evaluating Performance...\n",
            "pass 34340, training loss 9.905604362487793\n",
            "Evaluating Performance...\n",
            "pass 34350, training loss 8.347397804260254\n",
            "Evaluating Performance...\n",
            "pass 34360, training loss 9.68128490447998\n",
            "Evaluating Performance...\n",
            "pass 34370, training loss 10.649677276611328\n",
            "Evaluating Performance...\n",
            "pass 34380, training loss 8.777605056762695\n",
            "Evaluating Performance...\n",
            "pass 34390, training loss 9.94000244140625\n",
            "Evaluating Performance...\n",
            "pass 34400, training loss 7.981694221496582\n",
            "Evaluating Performance...\n",
            "pass 34410, training loss 7.435369491577148\n",
            "Evaluating Performance...\n",
            "pass 34420, training loss 5.847193241119385\n",
            "Evaluating Performance...\n",
            "pass 34430, training loss 7.273806095123291\n",
            "Evaluating Performance...\n",
            "pass 34440, training loss 10.038148880004883\n",
            "Evaluating Performance...\n",
            "pass 34450, training loss 6.516102313995361\n",
            "Evaluating Performance...\n",
            "pass 34460, training loss 8.393622398376465\n",
            "Evaluating Performance...\n",
            "pass 34470, training loss 7.020910263061523\n",
            "Evaluating Performance...\n",
            "pass 34480, training loss 6.2206645011901855\n",
            "Evaluating Performance...\n",
            "pass 34490, training loss 6.895665168762207\n",
            "Evaluating Performance...\n",
            "pass 34500, training loss 7.882697105407715\n",
            "Evaluating Performance...\n",
            "pass 34510, training loss 7.699445724487305\n",
            "Evaluating Performance...\n",
            "pass 34520, training loss 8.128850936889648\n",
            "Evaluating Performance...\n",
            "pass 34530, training loss 10.193251609802246\n",
            "Evaluating Performance...\n",
            "pass 34540, training loss 8.197819709777832\n",
            "Evaluating Performance...\n",
            "pass 34550, training loss 8.008461952209473\n",
            "Evaluating Performance...\n",
            "pass 34560, training loss 8.353408813476562\n",
            "Evaluating Performance...\n",
            "pass 34570, training loss 9.537222862243652\n",
            "Evaluating Performance...\n",
            "pass 34580, training loss 7.287330150604248\n",
            "Evaluating Performance...\n",
            "pass 34590, training loss 8.589643478393555\n",
            "Evaluating Performance...\n",
            "pass 34600, training loss 7.33142614364624\n",
            "Evaluating Performance...\n",
            "pass 34610, training loss 8.22659969329834\n",
            "Evaluating Performance...\n",
            "pass 34620, training loss 7.279515266418457\n",
            "Evaluating Performance...\n",
            "pass 34630, training loss 9.044475555419922\n",
            "Evaluating Performance...\n",
            "pass 34640, training loss 6.8972368240356445\n",
            "Evaluating Performance...\n",
            "pass 34650, training loss 7.204315185546875\n",
            "Evaluating Performance...\n",
            "pass 34660, training loss 6.440358638763428\n",
            "Evaluating Performance...\n",
            "pass 34670, training loss 6.942976474761963\n",
            "Evaluating Performance...\n",
            "pass 34680, training loss 7.867072105407715\n",
            "Evaluating Performance...\n",
            "pass 34690, training loss 7.527278900146484\n",
            "Evaluating Performance...\n",
            "pass 34700, training loss 9.748350143432617\n",
            "Evaluating Performance...\n",
            "pass 34710, training loss 7.256895542144775\n",
            "Evaluating Performance...\n",
            "pass 34720, training loss 8.473092079162598\n",
            "Evaluating Performance...\n",
            "pass 34730, training loss 8.417316436767578\n",
            "Evaluating Performance...\n",
            "pass 34740, training loss 8.801799774169922\n",
            "Evaluating Performance...\n",
            "pass 34750, training loss 7.368699550628662\n",
            "Evaluating Performance...\n",
            "pass 34760, training loss 7.767402648925781\n",
            "Evaluating Performance...\n",
            "pass 34770, training loss 8.414105415344238\n",
            "Evaluating Performance...\n",
            "pass 34780, training loss 8.867534637451172\n",
            "Evaluating Performance...\n",
            "pass 34790, training loss 8.022634506225586\n",
            "Evaluating Performance...\n",
            "pass 34800, training loss 8.694792747497559\n",
            "Evaluating Performance...\n",
            "pass 34810, training loss 9.889244079589844\n",
            "Evaluating Performance...\n",
            "pass 34820, training loss 8.29554271697998\n",
            "Evaluating Performance...\n",
            "pass 34830, training loss 7.429157733917236\n",
            "Evaluating Performance...\n",
            "pass 34840, training loss 8.167683601379395\n",
            "Evaluating Performance...\n",
            "pass 34850, training loss 7.454816818237305\n",
            "Evaluating Performance...\n",
            "pass 34860, training loss 5.565207481384277\n",
            "Evaluating Performance...\n",
            "pass 34870, training loss 6.9635467529296875\n",
            "Evaluating Performance...\n",
            "pass 34880, training loss 9.910456657409668\n",
            "Evaluating Performance...\n",
            "pass 34890, training loss 6.820755481719971\n",
            "Evaluating Performance...\n",
            "pass 34900, training loss 7.235706806182861\n",
            "Evaluating Performance...\n",
            "pass 34910, training loss 8.342226028442383\n",
            "Evaluating Performance...\n",
            "pass 34920, training loss 7.848572254180908\n",
            "Evaluating Performance...\n",
            "pass 34930, training loss 7.6652045249938965\n",
            "Evaluating Performance...\n",
            "pass 34940, training loss 8.258691787719727\n",
            "Evaluating Performance...\n",
            "pass 34950, training loss 7.641209602355957\n",
            "Evaluating Performance...\n",
            "pass 34960, training loss 8.500849723815918\n",
            "Evaluating Performance...\n",
            "pass 34970, training loss 9.037437438964844\n",
            "Evaluating Performance...\n",
            "pass 34980, training loss 10.634138107299805\n",
            "Evaluating Performance...\n",
            "pass 34990, training loss 7.053007125854492\n",
            "Evaluating Performance...\n",
            "pass 35000, training loss 9.298789024353027\n",
            "Evaluating Performance...\n",
            "pass 35010, training loss 9.517839431762695\n",
            "Evaluating Performance...\n",
            "pass 35020, training loss 10.038013458251953\n",
            "Evaluating Performance...\n",
            "pass 35030, training loss 10.302945137023926\n",
            "Evaluating Performance...\n",
            "pass 35040, training loss 8.728583335876465\n",
            "Evaluating Performance...\n",
            "pass 35050, training loss 7.406010627746582\n",
            "Evaluating Performance...\n",
            "pass 35060, training loss 6.26848840713501\n",
            "Evaluating Performance...\n",
            "pass 35070, training loss 8.33975887298584\n",
            "Evaluating Performance...\n",
            "pass 35080, training loss 6.530592918395996\n",
            "Evaluating Performance...\n",
            "pass 35090, training loss 9.136299133300781\n",
            "Evaluating Performance...\n",
            "pass 35100, training loss 6.5519633293151855\n",
            "Evaluating Performance...\n",
            "pass 35110, training loss 7.456817626953125\n",
            "Evaluating Performance...\n",
            "pass 35120, training loss 9.626541137695312\n",
            "Evaluating Performance...\n",
            "pass 35130, training loss 8.012581825256348\n",
            "Evaluating Performance...\n",
            "pass 35140, training loss 9.158190727233887\n",
            "Evaluating Performance...\n",
            "pass 35150, training loss 6.66267204284668\n",
            "Evaluating Performance...\n",
            "pass 35160, training loss 7.24689245223999\n",
            "Evaluating Performance...\n",
            "pass 35170, training loss 8.274895668029785\n",
            "Evaluating Performance...\n",
            "pass 35180, training loss 6.74147367477417\n",
            "Evaluating Performance...\n",
            "pass 35190, training loss 7.959897994995117\n",
            "Evaluating Performance...\n",
            "pass 35200, training loss 6.845589637756348\n",
            "Evaluating Performance...\n",
            "pass 35210, training loss 7.965566635131836\n",
            "Evaluating Performance...\n",
            "pass 35220, training loss 8.684514999389648\n",
            "Evaluating Performance...\n",
            "pass 35230, training loss 7.707136154174805\n",
            "Evaluating Performance...\n",
            "pass 35240, training loss 7.227756023406982\n",
            "Evaluating Performance...\n",
            "pass 35250, training loss 7.295907020568848\n",
            "Evaluating Performance...\n",
            "pass 35260, training loss 9.009687423706055\n",
            "Evaluating Performance...\n",
            "pass 35270, training loss 5.902710914611816\n",
            "Evaluating Performance...\n",
            "pass 35280, training loss 8.17721939086914\n",
            "Evaluating Performance...\n",
            "pass 35290, training loss 8.350722312927246\n",
            "Evaluating Performance...\n",
            "pass 35300, training loss 9.972505569458008\n",
            "Evaluating Performance...\n",
            "pass 35310, training loss 9.733283996582031\n",
            "Evaluating Performance...\n",
            "pass 35320, training loss 8.864696502685547\n",
            "Evaluating Performance...\n",
            "pass 35330, training loss 9.859371185302734\n",
            "Evaluating Performance...\n",
            "pass 35340, training loss 7.596047401428223\n",
            "Evaluating Performance...\n",
            "pass 35350, training loss 8.364816665649414\n",
            "Evaluating Performance...\n",
            "pass 35360, training loss 6.80766487121582\n",
            "Evaluating Performance...\n",
            "pass 35370, training loss 7.500721454620361\n",
            "Evaluating Performance...\n",
            "pass 35380, training loss 7.40745210647583\n",
            "Evaluating Performance...\n",
            "pass 35390, training loss 7.568247318267822\n",
            "Evaluating Performance...\n",
            "pass 35400, training loss 5.656974792480469\n",
            "Evaluating Performance...\n",
            "pass 35410, training loss 7.143781661987305\n",
            "Evaluating Performance...\n",
            "pass 35420, training loss 6.553081512451172\n",
            "Evaluating Performance...\n",
            "pass 35430, training loss 8.711682319641113\n",
            "Evaluating Performance...\n",
            "pass 35440, training loss 6.374523162841797\n",
            "Evaluating Performance...\n",
            "pass 35450, training loss 8.350709915161133\n",
            "Evaluating Performance...\n",
            "pass 35460, training loss 8.78471565246582\n",
            "Evaluating Performance...\n",
            "pass 35470, training loss 8.430566787719727\n",
            "Evaluating Performance...\n",
            "pass 35480, training loss 6.789481163024902\n",
            "Evaluating Performance...\n",
            "pass 35490, training loss 5.9595794677734375\n",
            "Evaluating Performance...\n",
            "pass 35500, training loss 8.349638938903809\n",
            "Evaluating Performance...\n",
            "pass 35510, training loss 7.042936325073242\n",
            "Evaluating Performance...\n",
            "pass 35520, training loss 8.112520217895508\n",
            "Evaluating Performance...\n",
            "pass 35530, training loss 10.016960144042969\n",
            "Evaluating Performance...\n",
            "pass 35540, training loss 6.921638011932373\n",
            "Evaluating Performance...\n",
            "pass 35550, training loss 9.27836799621582\n",
            "Evaluating Performance...\n",
            "pass 35560, training loss 7.122669696807861\n",
            "Evaluating Performance...\n",
            "pass 35570, training loss 7.511918544769287\n",
            "Evaluating Performance...\n",
            "pass 35580, training loss 8.478839874267578\n",
            "Evaluating Performance...\n",
            "pass 35590, training loss 8.946333885192871\n",
            "Evaluating Performance...\n",
            "pass 35600, training loss 9.373713493347168\n",
            "Evaluating Performance...\n",
            "pass 35610, training loss 8.596086502075195\n",
            "Evaluating Performance...\n",
            "pass 35620, training loss 9.908061027526855\n",
            "Evaluating Performance...\n",
            "pass 35630, training loss 8.703451156616211\n",
            "Evaluating Performance...\n",
            "pass 35640, training loss 7.169072151184082\n",
            "Evaluating Performance...\n",
            "pass 35650, training loss 7.737409591674805\n",
            "Evaluating Performance...\n",
            "pass 35660, training loss 6.94366979598999\n",
            "Evaluating Performance...\n",
            "pass 35670, training loss 8.183931350708008\n",
            "Evaluating Performance...\n",
            "pass 35680, training loss 8.066308975219727\n",
            "Evaluating Performance...\n",
            "pass 35690, training loss 6.633667469024658\n",
            "Evaluating Performance...\n",
            "pass 35700, training loss 9.313305854797363\n",
            "Evaluating Performance...\n",
            "pass 35710, training loss 9.741835594177246\n",
            "Evaluating Performance...\n",
            "pass 35720, training loss 6.193536281585693\n",
            "Evaluating Performance...\n",
            "pass 35730, training loss 8.878958702087402\n",
            "Evaluating Performance...\n",
            "pass 35740, training loss 7.318836688995361\n",
            "Evaluating Performance...\n",
            "pass 35750, training loss 8.209755897521973\n",
            "Evaluating Performance...\n",
            "pass 35760, training loss 8.358880996704102\n",
            "Evaluating Performance...\n",
            "pass 35770, training loss 7.766098499298096\n",
            "Evaluating Performance...\n",
            "pass 35780, training loss 9.046109199523926\n",
            "Evaluating Performance...\n",
            "pass 35790, training loss 7.719645977020264\n",
            "Evaluating Performance...\n",
            "pass 35800, training loss 7.821572303771973\n",
            "Evaluating Performance...\n",
            "pass 35810, training loss 6.536535263061523\n",
            "Evaluating Performance...\n",
            "pass 35820, training loss 6.83549165725708\n",
            "Evaluating Performance...\n",
            "pass 35830, training loss 5.341686725616455\n",
            "Evaluating Performance...\n",
            "pass 35840, training loss 7.701587200164795\n",
            "Evaluating Performance...\n",
            "pass 35850, training loss 8.262298583984375\n",
            "Evaluating Performance...\n",
            "pass 35860, training loss 7.593482494354248\n",
            "Evaluating Performance...\n",
            "pass 35870, training loss 7.100588321685791\n",
            "Evaluating Performance...\n",
            "pass 35880, training loss 8.51055908203125\n",
            "Evaluating Performance...\n",
            "pass 35890, training loss 6.94534158706665\n",
            "Evaluating Performance...\n",
            "pass 35900, training loss 7.750264644622803\n",
            "Evaluating Performance...\n",
            "pass 35910, training loss 7.045399188995361\n",
            "Evaluating Performance...\n",
            "pass 35920, training loss 8.330108642578125\n",
            "Evaluating Performance...\n",
            "pass 35930, training loss 7.820600509643555\n",
            "Evaluating Performance...\n",
            "pass 35940, training loss 5.696625232696533\n",
            "Evaluating Performance...\n",
            "pass 35950, training loss 6.180537700653076\n",
            "Evaluating Performance...\n",
            "pass 35960, training loss 9.623262405395508\n",
            "Evaluating Performance...\n",
            "pass 35970, training loss 7.8932623863220215\n",
            "Evaluating Performance...\n",
            "pass 35980, training loss 8.270421981811523\n",
            "Evaluating Performance...\n",
            "pass 35990, training loss 7.795074939727783\n",
            "Evaluating Performance...\n",
            "pass 36000, training loss 5.962245464324951\n",
            "Evaluating Performance...\n",
            "pass 36010, training loss 8.939565658569336\n",
            "Evaluating Performance...\n",
            "pass 36020, training loss 7.526493549346924\n",
            "Evaluating Performance...\n",
            "pass 36030, training loss 8.067818641662598\n",
            "Evaluating Performance...\n",
            "pass 36040, training loss 6.507603168487549\n",
            "Evaluating Performance...\n",
            "pass 36050, training loss 8.774168968200684\n",
            "Evaluating Performance...\n",
            "pass 36060, training loss 7.436011791229248\n",
            "Evaluating Performance...\n",
            "pass 36070, training loss 8.826204299926758\n",
            "Evaluating Performance...\n",
            "pass 36080, training loss 7.744834899902344\n",
            "Evaluating Performance...\n",
            "pass 36090, training loss 7.4295268058776855\n",
            "Evaluating Performance...\n",
            "pass 36100, training loss 7.35667085647583\n",
            "Evaluating Performance...\n",
            "pass 36110, training loss 7.3846330642700195\n",
            "Evaluating Performance...\n",
            "pass 36120, training loss 7.810318470001221\n",
            "Evaluating Performance...\n",
            "pass 36130, training loss 7.414229393005371\n",
            "Evaluating Performance...\n",
            "pass 36140, training loss 7.815986633300781\n",
            "Evaluating Performance...\n",
            "pass 36150, training loss 6.306474685668945\n",
            "Evaluating Performance...\n",
            "pass 36160, training loss 6.933350563049316\n",
            "Evaluating Performance...\n",
            "pass 36170, training loss 10.436263084411621\n",
            "Evaluating Performance...\n",
            "pass 36180, training loss 7.2234015464782715\n",
            "Evaluating Performance...\n",
            "pass 36190, training loss 6.995728015899658\n",
            "Evaluating Performance...\n",
            "pass 36200, training loss 7.162729740142822\n",
            "Evaluating Performance...\n",
            "pass 36210, training loss 6.934671878814697\n",
            "Evaluating Performance...\n",
            "pass 36220, training loss 9.140509605407715\n",
            "Evaluating Performance...\n",
            "pass 36230, training loss 8.416997909545898\n",
            "Evaluating Performance...\n",
            "pass 36240, training loss 7.781050682067871\n",
            "Evaluating Performance...\n",
            "pass 36250, training loss 7.2992424964904785\n",
            "Evaluating Performance...\n",
            "pass 36260, training loss 8.021449089050293\n",
            "Evaluating Performance...\n",
            "pass 36270, training loss 8.908963203430176\n",
            "Evaluating Performance...\n",
            "pass 36280, training loss 7.869526386260986\n",
            "Evaluating Performance...\n",
            "pass 36290, training loss 8.046998977661133\n",
            "Evaluating Performance...\n",
            "pass 36300, training loss 7.769716739654541\n",
            "Evaluating Performance...\n",
            "pass 36310, training loss 7.635204792022705\n",
            "Evaluating Performance...\n",
            "pass 36320, training loss 7.625414848327637\n",
            "Evaluating Performance...\n",
            "pass 36330, training loss 6.227133274078369\n",
            "Evaluating Performance...\n",
            "pass 36340, training loss 7.529552936553955\n",
            "Evaluating Performance...\n",
            "pass 36350, training loss 10.273958206176758\n",
            "Evaluating Performance...\n",
            "pass 36360, training loss 9.284358024597168\n",
            "Evaluating Performance...\n",
            "pass 36370, training loss 6.678268909454346\n",
            "Evaluating Performance...\n",
            "pass 36380, training loss 8.322118759155273\n",
            "Evaluating Performance...\n",
            "pass 36390, training loss 8.655869483947754\n",
            "Evaluating Performance...\n",
            "pass 36400, training loss 8.134401321411133\n",
            "Evaluating Performance...\n",
            "pass 36410, training loss 7.387887001037598\n",
            "Evaluating Performance...\n",
            "pass 36420, training loss 7.964332103729248\n",
            "Evaluating Performance...\n",
            "pass 36430, training loss 7.430269718170166\n",
            "Evaluating Performance...\n",
            "pass 36440, training loss 7.795896530151367\n",
            "Evaluating Performance...\n",
            "pass 36450, training loss 7.378030300140381\n",
            "Evaluating Performance...\n",
            "pass 36460, training loss 8.877205848693848\n",
            "Evaluating Performance...\n",
            "pass 36470, training loss 8.675966262817383\n",
            "Evaluating Performance...\n",
            "pass 36480, training loss 6.247068405151367\n",
            "Evaluating Performance...\n",
            "pass 36490, training loss 7.603879451751709\n",
            "Evaluating Performance...\n",
            "pass 36500, training loss 7.266129970550537\n",
            "Evaluating Performance...\n",
            "pass 36510, training loss 6.570539474487305\n",
            "Evaluating Performance...\n",
            "pass 36520, training loss 11.400177001953125\n",
            "Evaluating Performance...\n",
            "pass 36530, training loss 8.218250274658203\n",
            "Evaluating Performance...\n",
            "pass 36540, training loss 7.180429935455322\n",
            "Evaluating Performance...\n",
            "pass 36550, training loss 9.23306941986084\n",
            "Evaluating Performance...\n",
            "pass 36560, training loss 5.871843338012695\n",
            "Evaluating Performance...\n",
            "pass 36570, training loss 8.155265808105469\n",
            "Evaluating Performance...\n",
            "pass 36580, training loss 7.298859596252441\n",
            "Evaluating Performance...\n",
            "pass 36590, training loss 7.042182445526123\n",
            "Evaluating Performance...\n",
            "pass 36600, training loss 7.946431636810303\n",
            "Evaluating Performance...\n",
            "pass 36610, training loss 8.833822250366211\n",
            "Evaluating Performance...\n",
            "pass 36620, training loss 6.011923789978027\n",
            "Evaluating Performance...\n",
            "pass 36630, training loss 8.029950141906738\n",
            "Evaluating Performance...\n",
            "pass 36640, training loss 7.334024429321289\n",
            "Evaluating Performance...\n",
            "pass 36650, training loss 8.41122817993164\n",
            "Evaluating Performance...\n",
            "pass 36660, training loss 8.998316764831543\n",
            "Evaluating Performance...\n",
            "pass 36670, training loss 7.04897403717041\n",
            "Evaluating Performance...\n",
            "pass 36680, training loss 6.407754421234131\n",
            "Evaluating Performance...\n",
            "pass 36690, training loss 5.274880886077881\n",
            "Evaluating Performance...\n",
            "pass 36700, training loss 7.216739177703857\n",
            "Evaluating Performance...\n",
            "pass 36710, training loss 9.133670806884766\n",
            "Evaluating Performance...\n",
            "pass 36720, training loss 8.18671989440918\n",
            "Evaluating Performance...\n",
            "pass 36730, training loss 6.874013423919678\n",
            "Evaluating Performance...\n",
            "pass 36740, training loss 4.9551239013671875\n",
            "Evaluating Performance...\n",
            "pass 36750, training loss 8.926989555358887\n",
            "Evaluating Performance...\n",
            "pass 36760, training loss 9.496970176696777\n",
            "Evaluating Performance...\n",
            "pass 36770, training loss 7.804141521453857\n",
            "Evaluating Performance...\n",
            "pass 36780, training loss 5.873709201812744\n",
            "Evaluating Performance...\n",
            "pass 36790, training loss 8.49890422821045\n",
            "Evaluating Performance...\n",
            "pass 36800, training loss 7.641805171966553\n",
            "Evaluating Performance...\n",
            "pass 36810, training loss 8.68455696105957\n",
            "Evaluating Performance...\n",
            "pass 36820, training loss 6.627408981323242\n",
            "Evaluating Performance...\n",
            "pass 36830, training loss 9.06342601776123\n",
            "Evaluating Performance...\n",
            "pass 36840, training loss 7.63801908493042\n",
            "Evaluating Performance...\n",
            "pass 36850, training loss 6.952264308929443\n",
            "Evaluating Performance...\n",
            "pass 36860, training loss 6.982753276824951\n",
            "Evaluating Performance...\n",
            "pass 36870, training loss 6.828004837036133\n",
            "Evaluating Performance...\n",
            "pass 36880, training loss 8.09438419342041\n",
            "Evaluating Performance...\n",
            "pass 36890, training loss 8.836475372314453\n",
            "Evaluating Performance...\n",
            "pass 36900, training loss 6.961947917938232\n",
            "Evaluating Performance...\n",
            "pass 36910, training loss 7.731535911560059\n",
            "Evaluating Performance...\n",
            "pass 36920, training loss 8.95001220703125\n",
            "Evaluating Performance...\n",
            "pass 36930, training loss 13.027066230773926\n",
            "Evaluating Performance...\n",
            "pass 36940, training loss 5.995227336883545\n",
            "Evaluating Performance...\n",
            "pass 36950, training loss 9.503658294677734\n",
            "Evaluating Performance...\n",
            "pass 36960, training loss 7.794389247894287\n",
            "Evaluating Performance...\n",
            "pass 36970, training loss 8.012523651123047\n",
            "Evaluating Performance...\n",
            "pass 36980, training loss 7.252254009246826\n",
            "Evaluating Performance...\n",
            "pass 36990, training loss 5.485116958618164\n",
            "Evaluating Performance...\n",
            "pass 37000, training loss 7.143914699554443\n",
            "Evaluating Performance...\n",
            "pass 37010, training loss 10.4248046875\n",
            "Evaluating Performance...\n",
            "pass 37020, training loss 7.717421531677246\n",
            "Evaluating Performance...\n",
            "pass 37030, training loss 8.230880737304688\n",
            "Evaluating Performance...\n",
            "pass 37040, training loss 10.211064338684082\n",
            "Evaluating Performance...\n",
            "pass 37050, training loss 6.452895164489746\n",
            "Evaluating Performance...\n",
            "pass 37060, training loss 9.281275749206543\n",
            "Evaluating Performance...\n",
            "pass 37070, training loss 7.891818046569824\n",
            "Evaluating Performance...\n",
            "pass 37080, training loss 6.93781852722168\n",
            "Evaluating Performance...\n",
            "pass 37090, training loss 8.065176010131836\n",
            "Evaluating Performance...\n",
            "pass 37100, training loss 6.83660364151001\n",
            "Evaluating Performance...\n",
            "pass 37110, training loss 7.206758975982666\n",
            "Evaluating Performance...\n",
            "pass 37120, training loss 7.350072383880615\n",
            "Evaluating Performance...\n",
            "pass 37130, training loss 8.67902660369873\n",
            "Evaluating Performance...\n",
            "pass 37140, training loss 8.428359031677246\n",
            "Evaluating Performance...\n",
            "pass 37150, training loss 6.3343729972839355\n",
            "Evaluating Performance...\n",
            "pass 37160, training loss 8.997355461120605\n",
            "Evaluating Performance...\n",
            "pass 37170, training loss 7.172197341918945\n",
            "Evaluating Performance...\n",
            "pass 37180, training loss 8.359753608703613\n",
            "Evaluating Performance...\n",
            "pass 37190, training loss 6.415613651275635\n",
            "Evaluating Performance...\n",
            "pass 37200, training loss 6.809603691101074\n",
            "Evaluating Performance...\n",
            "pass 37210, training loss 7.10736608505249\n",
            "Evaluating Performance...\n",
            "pass 37220, training loss 10.417581558227539\n",
            "Evaluating Performance...\n",
            "pass 37230, training loss 6.694224834442139\n",
            "Evaluating Performance...\n",
            "pass 37240, training loss 7.679244041442871\n",
            "Evaluating Performance...\n",
            "pass 37250, training loss 8.27182674407959\n",
            "Evaluating Performance...\n",
            "pass 37260, training loss 8.167146682739258\n",
            "Evaluating Performance...\n",
            "pass 37270, training loss 7.047996520996094\n",
            "Evaluating Performance...\n",
            "pass 37280, training loss 6.890913486480713\n",
            "Evaluating Performance...\n",
            "pass 37290, training loss 9.190155029296875\n",
            "Evaluating Performance...\n",
            "pass 37300, training loss 6.797248840332031\n",
            "Evaluating Performance...\n",
            "pass 37310, training loss 8.91693115234375\n",
            "Evaluating Performance...\n",
            "pass 37320, training loss 6.7148590087890625\n",
            "Evaluating Performance...\n",
            "pass 37330, training loss 7.648444652557373\n",
            "Evaluating Performance...\n",
            "pass 37340, training loss 6.083393573760986\n",
            "Evaluating Performance...\n",
            "pass 37350, training loss 9.745648384094238\n",
            "Evaluating Performance...\n",
            "pass 37360, training loss 6.331249237060547\n",
            "Evaluating Performance...\n",
            "pass 37370, training loss 9.712103843688965\n",
            "Evaluating Performance...\n",
            "pass 37380, training loss 9.06338882446289\n",
            "Evaluating Performance...\n",
            "pass 37390, training loss 6.9754252433776855\n",
            "Evaluating Performance...\n",
            "pass 37400, training loss 8.242359161376953\n",
            "Evaluating Performance...\n",
            "pass 37410, training loss 9.64693546295166\n",
            "Evaluating Performance...\n",
            "pass 37420, training loss 7.139764308929443\n",
            "Evaluating Performance...\n",
            "pass 37430, training loss 6.020195960998535\n",
            "Evaluating Performance...\n",
            "pass 37440, training loss 6.9347076416015625\n",
            "Evaluating Performance...\n",
            "pass 37450, training loss 7.349363803863525\n",
            "Evaluating Performance...\n",
            "pass 37460, training loss 7.298421859741211\n",
            "Evaluating Performance...\n",
            "pass 37470, training loss 8.753192901611328\n",
            "Evaluating Performance...\n",
            "pass 37480, training loss 6.455353736877441\n",
            "Evaluating Performance...\n",
            "pass 37490, training loss 7.470803737640381\n",
            "Evaluating Performance...\n",
            "pass 37500, training loss 9.279536247253418\n",
            "Evaluating Performance...\n",
            "pass 37510, training loss 6.641798496246338\n",
            "Evaluating Performance...\n",
            "pass 37520, training loss 7.214724063873291\n",
            "Evaluating Performance...\n",
            "pass 37530, training loss 7.071192264556885\n",
            "Evaluating Performance...\n",
            "pass 37540, training loss 8.323084831237793\n",
            "Evaluating Performance...\n",
            "pass 37550, training loss 8.619964599609375\n",
            "Evaluating Performance...\n",
            "pass 37560, training loss 7.224612712860107\n",
            "Evaluating Performance...\n",
            "pass 37570, training loss 8.212685585021973\n",
            "Evaluating Performance...\n",
            "pass 37580, training loss 6.213900566101074\n",
            "Evaluating Performance...\n",
            "pass 37590, training loss 7.509406566619873\n",
            "Evaluating Performance...\n",
            "pass 37600, training loss 8.106940269470215\n",
            "Evaluating Performance...\n",
            "pass 37610, training loss 7.182245254516602\n",
            "Evaluating Performance...\n",
            "pass 37620, training loss 8.250812530517578\n",
            "Evaluating Performance...\n",
            "pass 37630, training loss 6.7568359375\n",
            "Evaluating Performance...\n",
            "pass 37640, training loss 7.278428077697754\n",
            "Evaluating Performance...\n",
            "pass 37650, training loss 7.966078758239746\n",
            "Evaluating Performance...\n",
            "pass 37660, training loss 11.188153266906738\n",
            "Evaluating Performance...\n",
            "pass 37670, training loss 9.798826217651367\n",
            "Evaluating Performance...\n",
            "pass 37680, training loss 7.320809364318848\n",
            "Evaluating Performance...\n",
            "pass 37690, training loss 8.191861152648926\n",
            "Evaluating Performance...\n",
            "pass 37700, training loss 9.708772659301758\n",
            "Evaluating Performance...\n",
            "pass 37710, training loss 7.033161640167236\n",
            "Evaluating Performance...\n",
            "pass 37720, training loss 8.365336418151855\n",
            "Evaluating Performance...\n",
            "pass 37730, training loss 7.427979946136475\n",
            "Evaluating Performance...\n",
            "pass 37740, training loss 9.1572847366333\n",
            "Evaluating Performance...\n",
            "pass 37750, training loss 7.551743030548096\n",
            "Evaluating Performance...\n",
            "pass 37760, training loss 9.266461372375488\n",
            "Evaluating Performance...\n",
            "pass 37770, training loss 9.230209350585938\n",
            "Evaluating Performance...\n",
            "pass 37780, training loss 11.044791221618652\n",
            "Evaluating Performance...\n",
            "pass 37790, training loss 7.3321533203125\n",
            "Evaluating Performance...\n",
            "pass 37800, training loss 7.396432399749756\n",
            "Evaluating Performance...\n",
            "pass 37810, training loss 5.661262512207031\n",
            "Evaluating Performance...\n",
            "pass 37820, training loss 5.982728958129883\n",
            "Evaluating Performance...\n",
            "pass 37830, training loss 7.520218372344971\n",
            "Evaluating Performance...\n",
            "pass 37840, training loss 8.256872177124023\n",
            "Evaluating Performance...\n",
            "pass 37850, training loss 9.82526969909668\n",
            "Evaluating Performance...\n",
            "pass 37860, training loss 6.185749530792236\n",
            "Evaluating Performance...\n",
            "pass 37870, training loss 8.8610200881958\n",
            "Evaluating Performance...\n",
            "pass 37880, training loss 6.0271897315979\n",
            "Evaluating Performance...\n",
            "pass 37890, training loss 10.253693580627441\n",
            "Evaluating Performance...\n",
            "pass 37900, training loss 7.636106491088867\n",
            "Evaluating Performance...\n",
            "pass 37910, training loss 7.961121082305908\n",
            "Evaluating Performance...\n",
            "pass 37920, training loss 8.66476058959961\n",
            "Evaluating Performance...\n",
            "pass 37930, training loss 9.185566902160645\n",
            "Evaluating Performance...\n",
            "pass 37940, training loss 6.46743631362915\n",
            "Evaluating Performance...\n",
            "pass 37950, training loss 8.058101654052734\n",
            "Evaluating Performance...\n",
            "pass 37960, training loss 8.133082389831543\n",
            "Evaluating Performance...\n",
            "pass 37970, training loss 9.802715301513672\n",
            "Evaluating Performance...\n",
            "pass 37980, training loss 7.786018371582031\n",
            "Evaluating Performance...\n",
            "pass 37990, training loss 7.153388023376465\n",
            "Evaluating Performance...\n",
            "pass 38000, training loss 8.445364952087402\n",
            "Evaluating Performance...\n",
            "pass 38010, training loss 8.08979320526123\n",
            "Evaluating Performance...\n",
            "pass 38020, training loss 6.800730228424072\n",
            "Evaluating Performance...\n",
            "pass 38030, training loss 7.256130695343018\n",
            "Evaluating Performance...\n",
            "pass 38040, training loss 6.638240337371826\n",
            "Evaluating Performance...\n",
            "pass 38050, training loss 8.356245994567871\n",
            "Evaluating Performance...\n",
            "pass 38060, training loss 7.990819931030273\n",
            "Evaluating Performance...\n",
            "pass 38070, training loss 8.422616004943848\n",
            "Evaluating Performance...\n",
            "pass 38080, training loss 9.04582405090332\n",
            "Evaluating Performance...\n",
            "pass 38090, training loss 8.111503601074219\n",
            "Evaluating Performance...\n",
            "pass 38100, training loss 9.479423522949219\n",
            "Evaluating Performance...\n",
            "pass 38110, training loss 6.0463690757751465\n",
            "Evaluating Performance...\n",
            "pass 38120, training loss 5.803192138671875\n",
            "Evaluating Performance...\n",
            "pass 38130, training loss 9.805877685546875\n",
            "Evaluating Performance...\n",
            "pass 38140, training loss 8.250767707824707\n",
            "Evaluating Performance...\n",
            "pass 38150, training loss 7.569907188415527\n",
            "Evaluating Performance...\n",
            "pass 38160, training loss 7.953877925872803\n",
            "Evaluating Performance...\n",
            "pass 38170, training loss 7.625079154968262\n",
            "Evaluating Performance...\n",
            "pass 38180, training loss 7.684111595153809\n",
            "Evaluating Performance...\n",
            "pass 38190, training loss 8.09030532836914\n",
            "Evaluating Performance...\n",
            "pass 38200, training loss 8.081618309020996\n",
            "Evaluating Performance...\n",
            "pass 38210, training loss 8.908881187438965\n",
            "Evaluating Performance...\n",
            "pass 38220, training loss 9.164642333984375\n",
            "Evaluating Performance...\n",
            "pass 38230, training loss 9.155227661132812\n",
            "Evaluating Performance...\n",
            "pass 38240, training loss 7.9341044425964355\n",
            "Evaluating Performance...\n",
            "pass 38250, training loss 6.90754508972168\n",
            "Evaluating Performance...\n",
            "pass 38260, training loss 4.550857067108154\n",
            "Evaluating Performance...\n",
            "pass 38270, training loss 7.064403533935547\n",
            "Evaluating Performance...\n",
            "pass 38280, training loss 6.042264461517334\n",
            "Evaluating Performance...\n",
            "pass 38290, training loss 6.572381019592285\n",
            "Evaluating Performance...\n",
            "pass 38300, training loss 6.687865257263184\n",
            "Evaluating Performance...\n",
            "pass 38310, training loss 6.741558074951172\n",
            "Evaluating Performance...\n",
            "pass 38320, training loss 6.663252353668213\n",
            "Evaluating Performance...\n",
            "pass 38330, training loss 6.674907684326172\n",
            "Evaluating Performance...\n",
            "pass 38340, training loss 7.968947410583496\n",
            "Evaluating Performance...\n",
            "pass 38350, training loss 9.327400207519531\n",
            "Evaluating Performance...\n",
            "pass 38360, training loss 6.803743362426758\n",
            "Evaluating Performance...\n",
            "pass 38370, training loss 9.848173141479492\n",
            "Evaluating Performance...\n",
            "pass 38380, training loss 7.420925617218018\n",
            "Evaluating Performance...\n",
            "pass 38390, training loss 6.651452541351318\n",
            "Evaluating Performance...\n",
            "pass 38400, training loss 8.800774574279785\n",
            "Evaluating Performance...\n",
            "pass 38410, training loss 8.098248481750488\n",
            "Evaluating Performance...\n",
            "pass 38420, training loss 6.670890808105469\n",
            "Evaluating Performance...\n",
            "pass 38430, training loss 6.537709712982178\n",
            "Evaluating Performance...\n",
            "pass 38440, training loss 7.298523902893066\n",
            "Evaluating Performance...\n",
            "pass 38450, training loss 7.463723659515381\n",
            "Evaluating Performance...\n",
            "pass 38460, training loss 6.811197280883789\n",
            "Evaluating Performance...\n",
            "pass 38470, training loss 7.080252647399902\n",
            "Evaluating Performance...\n",
            "pass 38480, training loss 10.283726692199707\n",
            "Evaluating Performance...\n",
            "pass 38490, training loss 7.660324573516846\n",
            "Evaluating Performance...\n",
            "pass 38500, training loss 7.443780899047852\n",
            "Evaluating Performance...\n",
            "pass 38510, training loss 8.570808410644531\n",
            "Evaluating Performance...\n",
            "pass 38520, training loss 7.860101222991943\n",
            "Evaluating Performance...\n",
            "pass 38530, training loss 11.237519264221191\n",
            "Evaluating Performance...\n",
            "pass 38540, training loss 6.852654457092285\n",
            "Evaluating Performance...\n",
            "pass 38550, training loss 6.808762550354004\n",
            "Evaluating Performance...\n",
            "pass 38560, training loss 8.038527488708496\n",
            "Evaluating Performance...\n",
            "pass 38570, training loss 8.0184965133667\n",
            "Evaluating Performance...\n",
            "pass 38580, training loss 7.512869834899902\n",
            "Evaluating Performance...\n",
            "pass 38590, training loss 7.939565181732178\n",
            "Evaluating Performance...\n",
            "pass 38600, training loss 6.180346965789795\n",
            "Evaluating Performance...\n",
            "pass 38610, training loss 6.256068706512451\n",
            "Evaluating Performance...\n",
            "pass 38620, training loss 7.703657150268555\n",
            "Evaluating Performance...\n",
            "pass 38630, training loss 7.728640556335449\n",
            "Evaluating Performance...\n",
            "pass 38640, training loss 8.924616813659668\n",
            "Evaluating Performance...\n",
            "pass 38650, training loss 7.455776214599609\n",
            "Evaluating Performance...\n",
            "pass 38660, training loss 9.57105827331543\n",
            "Evaluating Performance...\n",
            "pass 38670, training loss 6.191822528839111\n",
            "Evaluating Performance...\n",
            "pass 38680, training loss 7.254824161529541\n",
            "Evaluating Performance...\n",
            "pass 38690, training loss 9.258722305297852\n",
            "Evaluating Performance...\n",
            "pass 38700, training loss 7.002448081970215\n",
            "Evaluating Performance...\n",
            "pass 38710, training loss 8.361312866210938\n",
            "Evaluating Performance...\n",
            "pass 38720, training loss 7.388715744018555\n",
            "Evaluating Performance...\n",
            "pass 38730, training loss 8.640931129455566\n",
            "Evaluating Performance...\n",
            "pass 38740, training loss 6.657549858093262\n",
            "Evaluating Performance...\n",
            "pass 38750, training loss 6.936881065368652\n",
            "Evaluating Performance...\n",
            "pass 38760, training loss 7.439342498779297\n",
            "Evaluating Performance...\n",
            "pass 38770, training loss 7.1427812576293945\n",
            "Evaluating Performance...\n",
            "pass 38780, training loss 8.042008399963379\n",
            "Evaluating Performance...\n",
            "pass 38790, training loss 7.5225653648376465\n",
            "Evaluating Performance...\n",
            "pass 38800, training loss 8.0922212600708\n",
            "Evaluating Performance...\n",
            "pass 38810, training loss 7.67598819732666\n",
            "Evaluating Performance...\n",
            "pass 38820, training loss 7.850944519042969\n",
            "Evaluating Performance...\n",
            "pass 38830, training loss 7.7060675621032715\n",
            "Evaluating Performance...\n",
            "pass 38840, training loss 6.706426620483398\n",
            "Evaluating Performance...\n",
            "pass 38850, training loss 6.844089031219482\n",
            "Evaluating Performance...\n",
            "pass 38860, training loss 8.63254165649414\n",
            "Evaluating Performance...\n",
            "pass 38870, training loss 7.139062881469727\n",
            "Evaluating Performance...\n",
            "pass 38880, training loss 6.971088886260986\n",
            "Evaluating Performance...\n",
            "pass 38890, training loss 6.516959190368652\n",
            "Evaluating Performance...\n",
            "pass 38900, training loss 7.702558994293213\n",
            "Evaluating Performance...\n",
            "pass 38910, training loss 7.3749003410339355\n",
            "Evaluating Performance...\n",
            "pass 38920, training loss 8.283975601196289\n",
            "Evaluating Performance...\n",
            "pass 38930, training loss 9.118246078491211\n",
            "Evaluating Performance...\n",
            "pass 38940, training loss 7.881208419799805\n",
            "Evaluating Performance...\n",
            "pass 38950, training loss 6.770817279815674\n",
            "Evaluating Performance...\n",
            "pass 38960, training loss 7.429825782775879\n",
            "Evaluating Performance...\n",
            "pass 38970, training loss 9.04487133026123\n",
            "Evaluating Performance...\n",
            "pass 38980, training loss 6.69340181350708\n",
            "Evaluating Performance...\n",
            "pass 38990, training loss 7.51201868057251\n",
            "Evaluating Performance...\n",
            "pass 39000, training loss 7.31343412399292\n",
            "Evaluating Performance...\n",
            "pass 39010, training loss 10.35163688659668\n",
            "Evaluating Performance...\n",
            "pass 39020, training loss 6.384140491485596\n",
            "Evaluating Performance...\n",
            "pass 39030, training loss 7.881618022918701\n",
            "Evaluating Performance...\n",
            "pass 39040, training loss 8.654260635375977\n",
            "Evaluating Performance...\n",
            "pass 39050, training loss 7.09766149520874\n",
            "Evaluating Performance...\n",
            "pass 39060, training loss 9.03056812286377\n",
            "Evaluating Performance...\n",
            "pass 39070, training loss 10.118399620056152\n",
            "Evaluating Performance...\n",
            "pass 39080, training loss 7.512190818786621\n",
            "Evaluating Performance...\n",
            "pass 39090, training loss 9.207263946533203\n",
            "Evaluating Performance...\n",
            "pass 39100, training loss 5.754719257354736\n",
            "Evaluating Performance...\n",
            "pass 39110, training loss 8.013616561889648\n",
            "Evaluating Performance...\n",
            "pass 39120, training loss 8.08191204071045\n",
            "Evaluating Performance...\n",
            "pass 39130, training loss 8.308531761169434\n",
            "Evaluating Performance...\n",
            "pass 39140, training loss 9.280522346496582\n",
            "Evaluating Performance...\n",
            "pass 39150, training loss 6.875089168548584\n",
            "Evaluating Performance...\n",
            "pass 39160, training loss 6.355302810668945\n",
            "Evaluating Performance...\n",
            "pass 39170, training loss 6.556553840637207\n",
            "Evaluating Performance...\n",
            "pass 39180, training loss 6.613373756408691\n",
            "Evaluating Performance...\n",
            "pass 39190, training loss 8.494829177856445\n",
            "Evaluating Performance...\n",
            "pass 39200, training loss 5.498929977416992\n",
            "Evaluating Performance...\n",
            "pass 39210, training loss 7.982430934906006\n",
            "Evaluating Performance...\n",
            "pass 39220, training loss 10.841972351074219\n",
            "Evaluating Performance...\n",
            "pass 39230, training loss 6.495847225189209\n",
            "Evaluating Performance...\n",
            "pass 39240, training loss 9.874791145324707\n",
            "Evaluating Performance...\n",
            "pass 39250, training loss 7.398190021514893\n",
            "Evaluating Performance...\n",
            "pass 39260, training loss 9.556289672851562\n",
            "Evaluating Performance...\n",
            "pass 39270, training loss 7.910180568695068\n",
            "Evaluating Performance...\n",
            "pass 39280, training loss 6.4351067543029785\n",
            "Evaluating Performance...\n",
            "pass 39290, training loss 7.693284511566162\n",
            "Evaluating Performance...\n",
            "pass 39300, training loss 7.737034320831299\n",
            "Evaluating Performance...\n",
            "pass 39310, training loss 8.2576265335083\n",
            "Evaluating Performance...\n",
            "pass 39320, training loss 8.722237586975098\n",
            "Evaluating Performance...\n",
            "pass 39330, training loss 6.344219207763672\n",
            "Evaluating Performance...\n",
            "pass 39340, training loss 7.424346923828125\n",
            "Evaluating Performance...\n",
            "pass 39350, training loss 7.675728797912598\n",
            "Evaluating Performance...\n",
            "pass 39360, training loss 7.612067699432373\n",
            "Evaluating Performance...\n",
            "pass 39370, training loss 8.80787181854248\n",
            "Evaluating Performance...\n",
            "pass 39380, training loss 7.119483947753906\n",
            "Evaluating Performance...\n",
            "pass 39390, training loss 8.128279685974121\n",
            "Evaluating Performance...\n",
            "pass 39400, training loss 6.03341007232666\n",
            "Evaluating Performance...\n",
            "pass 39410, training loss 7.138177871704102\n",
            "Evaluating Performance...\n",
            "pass 39420, training loss 8.332547187805176\n",
            "Evaluating Performance...\n",
            "pass 39430, training loss 8.253179550170898\n",
            "Evaluating Performance...\n",
            "pass 39440, training loss 8.382882118225098\n",
            "Evaluating Performance...\n",
            "pass 39450, training loss 7.192327499389648\n",
            "Evaluating Performance...\n",
            "pass 39460, training loss 7.356781482696533\n",
            "Evaluating Performance...\n",
            "pass 39470, training loss 8.018641471862793\n",
            "Evaluating Performance...\n",
            "pass 39480, training loss 7.5521559715271\n",
            "Evaluating Performance...\n",
            "pass 39490, training loss 8.098965644836426\n",
            "Evaluating Performance...\n",
            "pass 39500, training loss 9.490402221679688\n",
            "Evaluating Performance...\n",
            "pass 39510, training loss 7.8482184410095215\n",
            "Evaluating Performance...\n",
            "pass 39520, training loss 7.576357841491699\n",
            "Evaluating Performance...\n",
            "pass 39530, training loss 8.235697746276855\n",
            "Evaluating Performance...\n",
            "pass 39540, training loss 7.524490833282471\n",
            "Evaluating Performance...\n",
            "pass 39550, training loss 7.554876327514648\n",
            "Evaluating Performance...\n",
            "pass 39560, training loss 7.291687488555908\n",
            "Evaluating Performance...\n",
            "pass 39570, training loss 7.7283244132995605\n",
            "Evaluating Performance...\n",
            "pass 39580, training loss 6.566926002502441\n",
            "Evaluating Performance...\n",
            "pass 39590, training loss 7.46486234664917\n",
            "Evaluating Performance...\n",
            "pass 39600, training loss 8.220354080200195\n",
            "Evaluating Performance...\n",
            "pass 39610, training loss 9.900092124938965\n",
            "Evaluating Performance...\n",
            "pass 39620, training loss 7.241635799407959\n",
            "Evaluating Performance...\n",
            "pass 39630, training loss 7.46056604385376\n",
            "Evaluating Performance...\n",
            "pass 39640, training loss 7.5203537940979\n",
            "Evaluating Performance...\n",
            "pass 39650, training loss 9.575638771057129\n",
            "Evaluating Performance...\n",
            "pass 39660, training loss 7.130798816680908\n",
            "Evaluating Performance...\n",
            "pass 39670, training loss 8.48570442199707\n",
            "Evaluating Performance...\n",
            "pass 39680, training loss 7.552661418914795\n",
            "Evaluating Performance...\n",
            "pass 39690, training loss 6.979861736297607\n",
            "Evaluating Performance...\n",
            "pass 39700, training loss 6.679807662963867\n",
            "Evaluating Performance...\n",
            "pass 39710, training loss 6.7849860191345215\n",
            "Evaluating Performance...\n",
            "pass 39720, training loss 6.730734825134277\n",
            "Evaluating Performance...\n",
            "pass 39730, training loss 8.523347854614258\n",
            "Evaluating Performance...\n",
            "pass 39740, training loss 8.15218448638916\n",
            "Evaluating Performance...\n",
            "pass 39750, training loss 7.210552215576172\n",
            "Evaluating Performance...\n",
            "pass 39760, training loss 7.449555397033691\n",
            "Evaluating Performance...\n",
            "pass 39770, training loss 8.11353588104248\n",
            "Evaluating Performance...\n",
            "pass 39780, training loss 9.024494171142578\n",
            "Evaluating Performance...\n",
            "pass 39790, training loss 7.064151287078857\n",
            "Evaluating Performance...\n",
            "pass 39800, training loss 7.605964660644531\n",
            "Evaluating Performance...\n",
            "pass 39810, training loss 7.392256259918213\n",
            "Evaluating Performance...\n",
            "pass 39820, training loss 6.710737228393555\n",
            "Evaluating Performance...\n",
            "pass 39830, training loss 7.918993949890137\n",
            "Evaluating Performance...\n",
            "pass 39840, training loss 7.374705791473389\n",
            "Evaluating Performance...\n",
            "pass 39850, training loss 7.067688941955566\n",
            "Evaluating Performance...\n",
            "pass 39860, training loss 5.733603477478027\n",
            "Evaluating Performance...\n",
            "pass 39870, training loss 6.990230560302734\n",
            "Evaluating Performance...\n",
            "pass 39880, training loss 9.900464057922363\n",
            "Evaluating Performance...\n",
            "pass 39890, training loss 7.293224334716797\n",
            "Evaluating Performance...\n",
            "pass 39900, training loss 7.028858661651611\n",
            "Evaluating Performance...\n",
            "pass 39910, training loss 7.674550533294678\n",
            "Evaluating Performance...\n",
            "pass 39920, training loss 8.82354736328125\n",
            "Evaluating Performance...\n",
            "pass 39930, training loss 10.43433952331543\n",
            "Evaluating Performance...\n",
            "pass 39940, training loss 7.474616527557373\n",
            "Evaluating Performance...\n",
            "pass 39950, training loss 9.163784980773926\n",
            "Evaluating Performance...\n",
            "pass 39960, training loss 10.212515830993652\n",
            "Evaluating Performance...\n",
            "pass 39970, training loss 6.053203105926514\n",
            "Evaluating Performance...\n",
            "pass 39980, training loss 5.546716213226318\n",
            "Evaluating Performance...\n",
            "pass 39990, training loss 8.747966766357422\n",
            "Evaluating Performance...\n",
            "pass 40000, training loss 7.428820610046387\n",
            "Saving Checkpoint...\n",
            "checkpoint saved\n",
            "Evaluating Performance...\n",
            "pass 40010, training loss 8.47675609588623\n",
            "Evaluating Performance...\n",
            "pass 40020, training loss 9.055047035217285\n",
            "Evaluating Performance...\n",
            "pass 40030, training loss 8.775495529174805\n",
            "Evaluating Performance...\n",
            "pass 40040, training loss 7.894347190856934\n",
            "Evaluating Performance...\n",
            "pass 40050, training loss 6.697431564331055\n",
            "Evaluating Performance...\n",
            "pass 40060, training loss 8.658061981201172\n",
            "Evaluating Performance...\n",
            "pass 40070, training loss 7.140601634979248\n",
            "Evaluating Performance...\n",
            "pass 40080, training loss 7.896717548370361\n",
            "Evaluating Performance...\n",
            "pass 40090, training loss 6.9594316482543945\n",
            "Evaluating Performance...\n",
            "pass 40100, training loss 10.212698936462402\n",
            "Evaluating Performance...\n",
            "pass 40110, training loss 8.291300773620605\n",
            "Evaluating Performance...\n",
            "pass 40120, training loss 8.519107818603516\n",
            "Evaluating Performance...\n",
            "pass 40130, training loss 7.573577880859375\n",
            "Evaluating Performance...\n",
            "pass 40140, training loss 6.707882404327393\n",
            "Evaluating Performance...\n",
            "pass 40150, training loss 6.610601425170898\n",
            "Evaluating Performance...\n",
            "pass 40160, training loss 8.033923149108887\n",
            "Evaluating Performance...\n",
            "pass 40170, training loss 7.196222305297852\n",
            "Evaluating Performance...\n",
            "pass 40180, training loss 9.299858093261719\n",
            "Evaluating Performance...\n",
            "pass 40190, training loss 7.02702522277832\n",
            "Evaluating Performance...\n",
            "pass 40200, training loss 8.595808982849121\n",
            "Evaluating Performance...\n",
            "pass 40210, training loss 6.62922477722168\n",
            "Evaluating Performance...\n",
            "pass 40220, training loss 7.624230861663818\n",
            "Evaluating Performance...\n",
            "pass 40230, training loss 7.611238956451416\n",
            "Evaluating Performance...\n",
            "pass 40240, training loss 7.394313812255859\n",
            "Evaluating Performance...\n",
            "pass 40250, training loss 8.307154655456543\n",
            "Evaluating Performance...\n",
            "pass 40260, training loss 7.074640274047852\n",
            "Evaluating Performance...\n",
            "pass 40270, training loss 7.556813716888428\n",
            "Evaluating Performance...\n",
            "pass 40280, training loss 6.171860694885254\n",
            "Evaluating Performance...\n",
            "pass 40290, training loss 7.919806480407715\n",
            "Evaluating Performance...\n",
            "pass 40300, training loss 7.369588375091553\n",
            "Evaluating Performance...\n",
            "pass 40310, training loss 9.893463134765625\n",
            "Evaluating Performance...\n",
            "pass 40320, training loss 7.029064655303955\n",
            "Evaluating Performance...\n",
            "pass 40330, training loss 9.384638786315918\n",
            "Evaluating Performance...\n",
            "pass 40340, training loss 8.139897346496582\n",
            "Evaluating Performance...\n",
            "pass 40350, training loss 7.319423198699951\n",
            "Evaluating Performance...\n",
            "pass 40360, training loss 7.379799842834473\n",
            "Evaluating Performance...\n",
            "pass 40370, training loss 7.241037845611572\n",
            "Evaluating Performance...\n",
            "pass 40380, training loss 8.5691499710083\n",
            "Evaluating Performance...\n",
            "pass 40390, training loss 8.066976547241211\n",
            "Evaluating Performance...\n",
            "pass 40400, training loss 7.318641185760498\n",
            "Evaluating Performance...\n",
            "pass 40410, training loss 7.941971778869629\n",
            "Evaluating Performance...\n",
            "pass 40420, training loss 8.322250366210938\n",
            "Evaluating Performance...\n",
            "pass 40430, training loss 7.276548862457275\n",
            "Evaluating Performance...\n",
            "pass 40440, training loss 8.532404899597168\n",
            "Evaluating Performance...\n",
            "pass 40450, training loss 6.220399856567383\n",
            "Evaluating Performance...\n",
            "pass 40460, training loss 6.687594890594482\n",
            "Evaluating Performance...\n",
            "pass 40470, training loss 8.452899932861328\n",
            "Evaluating Performance...\n",
            "pass 40480, training loss 8.437788963317871\n",
            "Evaluating Performance...\n",
            "pass 40490, training loss 8.920059204101562\n",
            "Evaluating Performance...\n",
            "pass 40500, training loss 7.129401683807373\n",
            "Evaluating Performance...\n",
            "pass 40510, training loss 7.058205604553223\n",
            "Evaluating Performance...\n",
            "pass 40520, training loss 6.524198532104492\n",
            "Evaluating Performance...\n",
            "pass 40530, training loss 8.616711616516113\n",
            "Evaluating Performance...\n",
            "pass 40540, training loss 7.816834449768066\n",
            "Evaluating Performance...\n",
            "pass 40550, training loss 9.377218246459961\n",
            "Evaluating Performance...\n",
            "pass 40560, training loss 7.829209327697754\n",
            "Evaluating Performance...\n",
            "pass 40570, training loss 7.0717010498046875\n",
            "Evaluating Performance...\n",
            "pass 40580, training loss 7.844820499420166\n",
            "Evaluating Performance...\n",
            "pass 40590, training loss 7.962766170501709\n",
            "Evaluating Performance...\n",
            "pass 40600, training loss 5.804747581481934\n",
            "Evaluating Performance...\n",
            "pass 40610, training loss 9.456618309020996\n",
            "Evaluating Performance...\n",
            "pass 40620, training loss 7.173134803771973\n",
            "Evaluating Performance...\n",
            "pass 40630, training loss 6.572754383087158\n",
            "Evaluating Performance...\n",
            "pass 40640, training loss 9.099584579467773\n",
            "Evaluating Performance...\n",
            "pass 40650, training loss 7.608989238739014\n",
            "Evaluating Performance...\n",
            "pass 40660, training loss 11.187548637390137\n",
            "Evaluating Performance...\n",
            "pass 40670, training loss 7.759006977081299\n",
            "Evaluating Performance...\n",
            "pass 40680, training loss 7.329166889190674\n",
            "Evaluating Performance...\n",
            "pass 40690, training loss 7.140232563018799\n",
            "Evaluating Performance...\n",
            "pass 40700, training loss 6.452850341796875\n",
            "Evaluating Performance...\n",
            "pass 40710, training loss 7.744213581085205\n",
            "Evaluating Performance...\n",
            "pass 40720, training loss 9.840052604675293\n",
            "Evaluating Performance...\n",
            "pass 40730, training loss 8.444843292236328\n",
            "Evaluating Performance...\n",
            "pass 40740, training loss 8.052240371704102\n",
            "Evaluating Performance...\n",
            "pass 40750, training loss 8.607181549072266\n",
            "Evaluating Performance...\n",
            "pass 40760, training loss 6.742128849029541\n",
            "Evaluating Performance...\n",
            "pass 40770, training loss 8.904314041137695\n",
            "Evaluating Performance...\n",
            "pass 40780, training loss 7.800069332122803\n",
            "Evaluating Performance...\n",
            "pass 40790, training loss 8.367778778076172\n",
            "Evaluating Performance...\n",
            "pass 40800, training loss 6.2406439781188965\n",
            "Evaluating Performance...\n",
            "pass 40810, training loss 7.458370208740234\n",
            "Evaluating Performance...\n",
            "pass 40820, training loss 8.107556343078613\n",
            "Evaluating Performance...\n",
            "pass 40830, training loss 8.708690643310547\n",
            "Evaluating Performance...\n",
            "pass 40840, training loss 4.897439002990723\n",
            "Evaluating Performance...\n",
            "pass 40850, training loss 7.340885639190674\n",
            "Evaluating Performance...\n",
            "pass 40860, training loss 7.06650972366333\n",
            "Evaluating Performance...\n",
            "pass 40870, training loss 8.197724342346191\n",
            "Evaluating Performance...\n",
            "pass 40880, training loss 6.637385845184326\n",
            "Evaluating Performance...\n",
            "pass 40890, training loss 8.22799015045166\n",
            "Evaluating Performance...\n",
            "pass 40900, training loss 7.811793804168701\n",
            "Evaluating Performance...\n",
            "pass 40910, training loss 6.443915843963623\n",
            "Evaluating Performance...\n",
            "pass 40920, training loss 8.609068870544434\n",
            "Evaluating Performance...\n",
            "pass 40930, training loss 7.211047649383545\n",
            "Evaluating Performance...\n",
            "pass 40940, training loss 7.463021278381348\n",
            "Evaluating Performance...\n",
            "pass 40950, training loss 7.08189058303833\n",
            "Evaluating Performance...\n",
            "pass 40960, training loss 8.366772651672363\n",
            "Evaluating Performance...\n",
            "pass 40970, training loss 7.916447162628174\n",
            "Evaluating Performance...\n",
            "pass 40980, training loss 5.373666763305664\n",
            "Evaluating Performance...\n",
            "pass 40990, training loss 7.924168109893799\n",
            "Evaluating Performance...\n",
            "pass 41000, training loss 7.943398952484131\n",
            "Evaluating Performance...\n",
            "pass 41010, training loss 10.060572624206543\n",
            "Evaluating Performance...\n",
            "pass 41020, training loss 7.482590198516846\n",
            "Evaluating Performance...\n",
            "pass 41030, training loss 7.539230823516846\n",
            "Evaluating Performance...\n",
            "pass 41040, training loss 7.08461332321167\n",
            "Evaluating Performance...\n",
            "pass 41050, training loss 8.877212524414062\n",
            "Evaluating Performance...\n",
            "pass 41060, training loss 8.356536865234375\n",
            "Evaluating Performance...\n",
            "pass 41070, training loss 6.884437084197998\n",
            "Evaluating Performance...\n",
            "pass 41080, training loss 8.589686393737793\n",
            "Evaluating Performance...\n",
            "pass 41090, training loss 7.765172481536865\n",
            "Evaluating Performance...\n",
            "pass 41100, training loss 9.37208080291748\n",
            "Evaluating Performance...\n",
            "pass 41110, training loss 7.9441962242126465\n",
            "Evaluating Performance...\n",
            "pass 41120, training loss 6.344162940979004\n",
            "Evaluating Performance...\n",
            "pass 41130, training loss 8.521486282348633\n",
            "Evaluating Performance...\n",
            "pass 41140, training loss 9.944153785705566\n",
            "Evaluating Performance...\n",
            "pass 41150, training loss 7.599293231964111\n",
            "Evaluating Performance...\n",
            "pass 41160, training loss 9.353649139404297\n",
            "Evaluating Performance...\n",
            "pass 41170, training loss 7.438204288482666\n",
            "Evaluating Performance...\n",
            "pass 41180, training loss 7.08888053894043\n",
            "Evaluating Performance...\n",
            "pass 41190, training loss 5.421136379241943\n",
            "Evaluating Performance...\n",
            "pass 41200, training loss 8.585638999938965\n",
            "Evaluating Performance...\n",
            "pass 41210, training loss 5.834967136383057\n",
            "Evaluating Performance...\n",
            "pass 41220, training loss 5.676332473754883\n",
            "Evaluating Performance...\n",
            "pass 41230, training loss 7.156059265136719\n",
            "Evaluating Performance...\n",
            "pass 41240, training loss 7.534295558929443\n",
            "Evaluating Performance...\n",
            "pass 41250, training loss 9.320183753967285\n",
            "Evaluating Performance...\n",
            "pass 41260, training loss 7.20505428314209\n",
            "Evaluating Performance...\n",
            "pass 41270, training loss 6.8129191398620605\n",
            "Evaluating Performance...\n",
            "pass 41280, training loss 6.6084442138671875\n",
            "Evaluating Performance...\n",
            "pass 41290, training loss 5.628747940063477\n",
            "Evaluating Performance...\n",
            "pass 41300, training loss 6.540961265563965\n",
            "Evaluating Performance...\n",
            "pass 41310, training loss 7.965334892272949\n",
            "Evaluating Performance...\n",
            "pass 41320, training loss 10.017132759094238\n",
            "Evaluating Performance...\n",
            "pass 41330, training loss 7.2176995277404785\n",
            "Evaluating Performance...\n",
            "pass 41340, training loss 8.929289817810059\n",
            "Evaluating Performance...\n",
            "pass 41350, training loss 6.905930519104004\n",
            "Evaluating Performance...\n",
            "pass 41360, training loss 8.454951286315918\n",
            "Evaluating Performance...\n",
            "pass 41370, training loss 7.310560703277588\n",
            "Evaluating Performance...\n",
            "pass 41380, training loss 5.692989349365234\n",
            "Evaluating Performance...\n",
            "pass 41390, training loss 7.9743332862854\n",
            "Evaluating Performance...\n",
            "pass 41400, training loss 7.648080825805664\n",
            "Evaluating Performance...\n",
            "pass 41410, training loss 8.0968017578125\n",
            "Evaluating Performance...\n",
            "pass 41420, training loss 8.070374488830566\n",
            "Evaluating Performance...\n",
            "pass 41430, training loss 7.5834574699401855\n",
            "Evaluating Performance...\n",
            "pass 41440, training loss 6.909614562988281\n",
            "Evaluating Performance...\n",
            "pass 41450, training loss 7.969754695892334\n",
            "Evaluating Performance...\n",
            "pass 41460, training loss 7.057656288146973\n",
            "Evaluating Performance...\n",
            "pass 41470, training loss 5.782827854156494\n",
            "Evaluating Performance...\n",
            "pass 41480, training loss 6.280447959899902\n",
            "Evaluating Performance...\n",
            "pass 41490, training loss 6.940244674682617\n",
            "Evaluating Performance...\n",
            "pass 41500, training loss 9.37936019897461\n",
            "Evaluating Performance...\n",
            "pass 41510, training loss 6.289694786071777\n",
            "Evaluating Performance...\n",
            "pass 41520, training loss 8.237874984741211\n",
            "Evaluating Performance...\n",
            "pass 41530, training loss 7.023100852966309\n",
            "Evaluating Performance...\n",
            "pass 41540, training loss 5.377922058105469\n",
            "Evaluating Performance...\n",
            "pass 41550, training loss 6.527327060699463\n",
            "Evaluating Performance...\n",
            "pass 41560, training loss 7.41253137588501\n",
            "Evaluating Performance...\n",
            "pass 41570, training loss 8.066070556640625\n",
            "Evaluating Performance...\n",
            "pass 41580, training loss 7.445103645324707\n",
            "Evaluating Performance...\n",
            "pass 41590, training loss 7.4229607582092285\n",
            "Evaluating Performance...\n",
            "pass 41600, training loss 7.7968668937683105\n",
            "Evaluating Performance...\n",
            "pass 41610, training loss 6.847194671630859\n",
            "Evaluating Performance...\n",
            "pass 41620, training loss 8.765522956848145\n",
            "Evaluating Performance...\n",
            "pass 41630, training loss 6.280104637145996\n",
            "Evaluating Performance...\n",
            "pass 41640, training loss 6.625633239746094\n",
            "Evaluating Performance...\n",
            "pass 41650, training loss 8.941855430603027\n",
            "Evaluating Performance...\n",
            "pass 41660, training loss 7.527690887451172\n",
            "Evaluating Performance...\n",
            "pass 41670, training loss 8.708510398864746\n",
            "Evaluating Performance...\n",
            "pass 41680, training loss 7.331777095794678\n",
            "Evaluating Performance...\n",
            "pass 41690, training loss 6.764553070068359\n",
            "Evaluating Performance...\n",
            "pass 41700, training loss 6.450433254241943\n",
            "Evaluating Performance...\n",
            "pass 41710, training loss 8.173187255859375\n",
            "Evaluating Performance...\n",
            "pass 41720, training loss 8.676218032836914\n",
            "Evaluating Performance...\n",
            "pass 41730, training loss 6.01544713973999\n",
            "Evaluating Performance...\n",
            "pass 41740, training loss 8.152778625488281\n",
            "Evaluating Performance...\n",
            "pass 41750, training loss 6.756862163543701\n",
            "Evaluating Performance...\n",
            "pass 41760, training loss 9.401228904724121\n",
            "Evaluating Performance...\n",
            "pass 41770, training loss 6.998793601989746\n",
            "Evaluating Performance...\n",
            "pass 41780, training loss 7.867584228515625\n",
            "Evaluating Performance...\n",
            "pass 41790, training loss 7.523569107055664\n",
            "Evaluating Performance...\n",
            "pass 41800, training loss 6.751092910766602\n",
            "Evaluating Performance...\n",
            "pass 41810, training loss 7.14191198348999\n",
            "Evaluating Performance...\n",
            "pass 41820, training loss 8.050800323486328\n",
            "Evaluating Performance...\n",
            "pass 41830, training loss 6.061102390289307\n",
            "Evaluating Performance...\n",
            "pass 41840, training loss 6.15650749206543\n",
            "Evaluating Performance...\n",
            "pass 41850, training loss 5.711579322814941\n",
            "Evaluating Performance...\n",
            "pass 41860, training loss 6.324977397918701\n",
            "Evaluating Performance...\n",
            "pass 41870, training loss 7.720412254333496\n",
            "Evaluating Performance...\n",
            "pass 41880, training loss 7.405135154724121\n",
            "Evaluating Performance...\n",
            "pass 41890, training loss 6.634766101837158\n",
            "Evaluating Performance...\n",
            "pass 41900, training loss 8.049361228942871\n",
            "Evaluating Performance...\n",
            "pass 41910, training loss 7.273199081420898\n",
            "Evaluating Performance...\n",
            "pass 41920, training loss 6.763913631439209\n",
            "Evaluating Performance...\n",
            "pass 41930, training loss 8.821497917175293\n",
            "Evaluating Performance...\n",
            "pass 41940, training loss 6.808267116546631\n",
            "Evaluating Performance...\n",
            "pass 41950, training loss 6.99442195892334\n",
            "Evaluating Performance...\n",
            "pass 41960, training loss 6.07667350769043\n",
            "Evaluating Performance...\n",
            "pass 41970, training loss 5.505359172821045\n",
            "Evaluating Performance...\n",
            "pass 41980, training loss 6.588778972625732\n",
            "Evaluating Performance...\n",
            "pass 41990, training loss 7.125126838684082\n",
            "Evaluating Performance...\n",
            "pass 42000, training loss 7.569713115692139\n",
            "Evaluating Performance...\n",
            "pass 42010, training loss 7.404964447021484\n",
            "Evaluating Performance...\n",
            "pass 42020, training loss 7.210899353027344\n",
            "Evaluating Performance...\n",
            "pass 42030, training loss 6.579532146453857\n",
            "Evaluating Performance...\n",
            "pass 42040, training loss 7.040286064147949\n",
            "Evaluating Performance...\n",
            "pass 42050, training loss 7.206068515777588\n",
            "Evaluating Performance...\n",
            "pass 42060, training loss 8.878358840942383\n",
            "Evaluating Performance...\n",
            "pass 42070, training loss 9.414371490478516\n",
            "Evaluating Performance...\n",
            "pass 42080, training loss 8.30323314666748\n",
            "Evaluating Performance...\n",
            "pass 42090, training loss 5.582470417022705\n",
            "Evaluating Performance...\n",
            "pass 42100, training loss 8.800094604492188\n",
            "Evaluating Performance...\n",
            "pass 42110, training loss 7.223643779754639\n",
            "Evaluating Performance...\n",
            "pass 42120, training loss 5.930235862731934\n",
            "Evaluating Performance...\n",
            "pass 42130, training loss 7.217252254486084\n",
            "Evaluating Performance...\n",
            "pass 42140, training loss 8.355441093444824\n",
            "Evaluating Performance...\n",
            "pass 42150, training loss 7.04680871963501\n",
            "Evaluating Performance...\n",
            "pass 42160, training loss 7.652687072753906\n",
            "Evaluating Performance...\n",
            "pass 42170, training loss 6.241839408874512\n",
            "Evaluating Performance...\n",
            "pass 42180, training loss 6.533146381378174\n",
            "Evaluating Performance...\n",
            "pass 42190, training loss 7.569167137145996\n",
            "Evaluating Performance...\n",
            "pass 42200, training loss 7.196549415588379\n",
            "Evaluating Performance...\n",
            "pass 42210, training loss 7.070911884307861\n",
            "Evaluating Performance...\n",
            "pass 42220, training loss 8.640281677246094\n",
            "Evaluating Performance...\n",
            "pass 42230, training loss 6.570404529571533\n",
            "Evaluating Performance...\n",
            "pass 42240, training loss 6.452613830566406\n",
            "Evaluating Performance...\n",
            "pass 42250, training loss 7.822224140167236\n",
            "Evaluating Performance...\n",
            "pass 42260, training loss 7.979283332824707\n",
            "Evaluating Performance...\n",
            "pass 42270, training loss 5.477869987487793\n",
            "Evaluating Performance...\n",
            "pass 42280, training loss 11.251032829284668\n",
            "Evaluating Performance...\n",
            "pass 42290, training loss 8.413182258605957\n",
            "Evaluating Performance...\n",
            "pass 42300, training loss 7.32828950881958\n",
            "Evaluating Performance...\n",
            "pass 42310, training loss 7.484304428100586\n",
            "Evaluating Performance...\n",
            "pass 42320, training loss 7.447335720062256\n",
            "Evaluating Performance...\n",
            "pass 42330, training loss 6.600485801696777\n",
            "Evaluating Performance...\n",
            "pass 42340, training loss 6.488945960998535\n",
            "Evaluating Performance...\n",
            "pass 42350, training loss 7.7605299949646\n",
            "Evaluating Performance...\n",
            "pass 42360, training loss 6.639194965362549\n",
            "Evaluating Performance...\n",
            "pass 42370, training loss 8.471464157104492\n",
            "Evaluating Performance...\n",
            "pass 42380, training loss 9.285346984863281\n",
            "Evaluating Performance...\n",
            "pass 42390, training loss 6.885354518890381\n",
            "Evaluating Performance...\n",
            "pass 42400, training loss 5.9816203117370605\n",
            "Evaluating Performance...\n",
            "pass 42410, training loss 8.115039825439453\n",
            "Evaluating Performance...\n",
            "pass 42420, training loss 9.112860679626465\n",
            "Evaluating Performance...\n",
            "pass 42430, training loss 6.411280155181885\n",
            "Evaluating Performance...\n",
            "pass 42440, training loss 6.717776775360107\n",
            "Evaluating Performance...\n",
            "pass 42450, training loss 6.7816057205200195\n",
            "Evaluating Performance...\n",
            "pass 42460, training loss 7.8214921951293945\n",
            "Evaluating Performance...\n",
            "pass 42470, training loss 8.265901565551758\n",
            "Evaluating Performance...\n",
            "pass 42480, training loss 6.202619552612305\n",
            "Evaluating Performance...\n",
            "pass 42490, training loss 6.901574611663818\n",
            "Evaluating Performance...\n",
            "pass 42500, training loss 9.623830795288086\n",
            "Evaluating Performance...\n",
            "pass 42510, training loss 7.553294658660889\n",
            "Evaluating Performance...\n",
            "pass 42520, training loss 7.772162914276123\n",
            "Evaluating Performance...\n",
            "pass 42530, training loss 7.001673221588135\n",
            "Evaluating Performance...\n",
            "pass 42540, training loss 8.006144523620605\n",
            "Evaluating Performance...\n",
            "pass 42550, training loss 7.919239521026611\n",
            "Evaluating Performance...\n",
            "pass 42560, training loss 7.3834710121154785\n",
            "Evaluating Performance...\n",
            "pass 42570, training loss 7.171590805053711\n",
            "Evaluating Performance...\n",
            "pass 42580, training loss 7.458660125732422\n",
            "Evaluating Performance...\n",
            "pass 42590, training loss 6.805954456329346\n",
            "Evaluating Performance...\n",
            "pass 42600, training loss 6.691501140594482\n",
            "Evaluating Performance...\n",
            "pass 42610, training loss 9.556550979614258\n",
            "Evaluating Performance...\n",
            "pass 42620, training loss 9.024577140808105\n",
            "Evaluating Performance...\n",
            "pass 42630, training loss 8.820159912109375\n",
            "Evaluating Performance...\n",
            "pass 42640, training loss 6.206437110900879\n",
            "Evaluating Performance...\n",
            "pass 42650, training loss 8.379551887512207\n",
            "Evaluating Performance...\n",
            "pass 42660, training loss 7.598058700561523\n",
            "Evaluating Performance...\n",
            "pass 42670, training loss 6.234622478485107\n",
            "Evaluating Performance...\n",
            "pass 42680, training loss 6.6493096351623535\n",
            "Evaluating Performance...\n",
            "pass 42690, training loss 7.683316707611084\n",
            "Evaluating Performance...\n",
            "pass 42700, training loss 7.0081071853637695\n",
            "Evaluating Performance...\n",
            "pass 42710, training loss 7.402940273284912\n",
            "Evaluating Performance...\n",
            "pass 42720, training loss 8.370270729064941\n",
            "Evaluating Performance...\n",
            "pass 42730, training loss 6.179296970367432\n",
            "Evaluating Performance...\n",
            "pass 42740, training loss 8.214898109436035\n",
            "Evaluating Performance...\n",
            "pass 42750, training loss 6.185231685638428\n",
            "Evaluating Performance...\n",
            "pass 42760, training loss 6.6606879234313965\n",
            "Evaluating Performance...\n",
            "pass 42770, training loss 8.483866691589355\n",
            "Evaluating Performance...\n",
            "pass 42780, training loss 7.956053256988525\n",
            "Evaluating Performance...\n",
            "pass 42790, training loss 7.415856838226318\n",
            "Evaluating Performance...\n",
            "pass 42800, training loss 7.441335678100586\n",
            "Evaluating Performance...\n",
            "pass 42810, training loss 7.264111042022705\n",
            "Evaluating Performance...\n",
            "pass 42820, training loss 9.144351959228516\n",
            "Evaluating Performance...\n",
            "pass 42830, training loss 7.737916469573975\n",
            "Evaluating Performance...\n",
            "pass 42840, training loss 6.21917724609375\n",
            "Evaluating Performance...\n",
            "pass 42850, training loss 6.844720363616943\n",
            "Evaluating Performance...\n",
            "pass 42860, training loss 7.234383583068848\n",
            "Evaluating Performance...\n",
            "pass 42870, training loss 6.552260398864746\n",
            "Evaluating Performance...\n",
            "pass 42880, training loss 7.994154930114746\n",
            "Evaluating Performance...\n",
            "pass 42890, training loss 8.361635208129883\n",
            "Evaluating Performance...\n",
            "pass 42900, training loss 6.876739501953125\n",
            "Evaluating Performance...\n",
            "pass 42910, training loss 7.125205039978027\n",
            "Evaluating Performance...\n",
            "pass 42920, training loss 7.706217288970947\n",
            "Evaluating Performance...\n",
            "pass 42930, training loss 9.014765739440918\n",
            "Evaluating Performance...\n",
            "pass 42940, training loss 6.8367743492126465\n",
            "Evaluating Performance...\n",
            "pass 42950, training loss 7.007046222686768\n",
            "Evaluating Performance...\n",
            "pass 42960, training loss 7.94040584564209\n",
            "Evaluating Performance...\n",
            "pass 42970, training loss 6.300844192504883\n",
            "Evaluating Performance...\n",
            "pass 42980, training loss 7.18674898147583\n",
            "Evaluating Performance...\n",
            "pass 42990, training loss 5.863827228546143\n",
            "Evaluating Performance...\n",
            "pass 43000, training loss 6.933140754699707\n",
            "Evaluating Performance...\n",
            "pass 43010, training loss 7.838547229766846\n",
            "Evaluating Performance...\n",
            "pass 43020, training loss 7.3579583168029785\n",
            "Evaluating Performance...\n",
            "pass 43030, training loss 7.677137851715088\n",
            "Evaluating Performance...\n",
            "pass 43040, training loss 6.492892265319824\n",
            "Evaluating Performance...\n",
            "pass 43050, training loss 6.4326910972595215\n",
            "Evaluating Performance...\n",
            "pass 43060, training loss 6.206666946411133\n",
            "Evaluating Performance...\n",
            "pass 43070, training loss 8.479755401611328\n",
            "Evaluating Performance...\n",
            "pass 43080, training loss 8.141711235046387\n",
            "Evaluating Performance...\n",
            "pass 43090, training loss 5.968044757843018\n",
            "Evaluating Performance...\n",
            "pass 43100, training loss 7.678037166595459\n",
            "Evaluating Performance...\n",
            "pass 43110, training loss 5.618800640106201\n",
            "Evaluating Performance...\n",
            "pass 43120, training loss 6.320387363433838\n",
            "Evaluating Performance...\n",
            "pass 43130, training loss 6.751777648925781\n",
            "Evaluating Performance...\n",
            "pass 43140, training loss 6.042510986328125\n",
            "Evaluating Performance...\n",
            "pass 43150, training loss 7.943865776062012\n",
            "Evaluating Performance...\n",
            "pass 43160, training loss 7.0347580909729\n",
            "Evaluating Performance...\n",
            "pass 43170, training loss 5.508579730987549\n",
            "Evaluating Performance...\n",
            "pass 43180, training loss 7.663349628448486\n",
            "Evaluating Performance...\n",
            "pass 43190, training loss 7.062570095062256\n",
            "Evaluating Performance...\n",
            "pass 43200, training loss 7.856986999511719\n",
            "Evaluating Performance...\n",
            "pass 43210, training loss 7.399000644683838\n",
            "Evaluating Performance...\n",
            "pass 43220, training loss 6.812554359436035\n",
            "Evaluating Performance...\n",
            "pass 43230, training loss 8.799470901489258\n",
            "Evaluating Performance...\n",
            "pass 43240, training loss 7.943216323852539\n",
            "Evaluating Performance...\n",
            "pass 43250, training loss 6.605225086212158\n",
            "Evaluating Performance...\n",
            "pass 43260, training loss 8.344082832336426\n",
            "Evaluating Performance...\n",
            "pass 43270, training loss 8.798754692077637\n",
            "Evaluating Performance...\n",
            "pass 43280, training loss 7.301860332489014\n",
            "Evaluating Performance...\n",
            "pass 43290, training loss 7.496002674102783\n",
            "Evaluating Performance...\n",
            "pass 43300, training loss 6.719384670257568\n",
            "Evaluating Performance...\n",
            "pass 43310, training loss 6.540122985839844\n",
            "Evaluating Performance...\n",
            "pass 43320, training loss 7.203637599945068\n",
            "Evaluating Performance...\n",
            "pass 43330, training loss 6.952184200286865\n",
            "Evaluating Performance...\n",
            "pass 43340, training loss 9.0568208694458\n",
            "Evaluating Performance...\n",
            "pass 43350, training loss 6.8475022315979\n",
            "Evaluating Performance...\n",
            "pass 43360, training loss 7.917439937591553\n",
            "Evaluating Performance...\n",
            "pass 43370, training loss 9.35824203491211\n",
            "Evaluating Performance...\n",
            "pass 43380, training loss 8.363525390625\n",
            "Evaluating Performance...\n",
            "pass 43390, training loss 8.979186058044434\n",
            "Evaluating Performance...\n",
            "pass 43400, training loss 8.008012771606445\n",
            "Evaluating Performance...\n",
            "pass 43410, training loss 7.080385684967041\n",
            "Evaluating Performance...\n",
            "pass 43420, training loss 8.03311824798584\n",
            "Evaluating Performance...\n",
            "pass 43430, training loss 8.327595710754395\n",
            "Evaluating Performance...\n",
            "pass 43440, training loss 7.422786235809326\n",
            "Evaluating Performance...\n",
            "pass 43450, training loss 6.588047504425049\n",
            "Evaluating Performance...\n",
            "pass 43460, training loss 9.59426212310791\n",
            "Evaluating Performance...\n",
            "pass 43470, training loss 9.613269805908203\n",
            "Evaluating Performance...\n",
            "pass 43480, training loss 7.221188068389893\n",
            "Evaluating Performance...\n",
            "pass 43490, training loss 5.764115333557129\n",
            "Evaluating Performance...\n",
            "pass 43500, training loss 7.894965171813965\n",
            "Evaluating Performance...\n",
            "pass 43510, training loss 8.000494003295898\n",
            "Evaluating Performance...\n",
            "pass 43520, training loss 8.364412307739258\n",
            "Evaluating Performance...\n",
            "pass 43530, training loss 7.889188766479492\n",
            "Evaluating Performance...\n",
            "pass 43540, training loss 6.2710490226745605\n",
            "Evaluating Performance...\n",
            "pass 43550, training loss 5.654438495635986\n",
            "Evaluating Performance...\n",
            "pass 43560, training loss 7.8741583824157715\n",
            "Evaluating Performance...\n",
            "pass 43570, training loss 5.52098274230957\n",
            "Evaluating Performance...\n",
            "pass 43580, training loss 6.677167892456055\n",
            "Evaluating Performance...\n",
            "pass 43590, training loss 8.062132835388184\n",
            "Evaluating Performance...\n",
            "pass 43600, training loss 6.07118558883667\n",
            "Evaluating Performance...\n",
            "pass 43610, training loss 6.370355606079102\n",
            "Evaluating Performance...\n",
            "pass 43620, training loss 6.760276794433594\n",
            "Evaluating Performance...\n",
            "pass 43630, training loss 7.227009296417236\n",
            "Evaluating Performance...\n",
            "pass 43640, training loss 5.770460605621338\n",
            "Evaluating Performance...\n",
            "pass 43650, training loss 8.030717849731445\n",
            "Evaluating Performance...\n",
            "pass 43660, training loss 6.771352291107178\n",
            "Evaluating Performance...\n",
            "pass 43670, training loss 9.557727813720703\n",
            "Evaluating Performance...\n",
            "pass 43680, training loss 8.785650253295898\n",
            "Evaluating Performance...\n",
            "pass 43690, training loss 7.159095764160156\n",
            "Evaluating Performance...\n",
            "pass 43700, training loss 7.985032558441162\n",
            "Evaluating Performance...\n",
            "pass 43710, training loss 7.322511196136475\n",
            "Evaluating Performance...\n",
            "pass 43720, training loss 7.981331825256348\n",
            "Evaluating Performance...\n",
            "pass 43730, training loss 8.804620742797852\n",
            "Evaluating Performance...\n",
            "pass 43740, training loss 7.137723445892334\n",
            "Evaluating Performance...\n",
            "pass 43750, training loss 6.564869403839111\n",
            "Evaluating Performance...\n",
            "pass 43760, training loss 7.223404407501221\n",
            "Evaluating Performance...\n",
            "pass 43770, training loss 6.789485454559326\n",
            "Evaluating Performance...\n",
            "pass 43780, training loss 9.920010566711426\n",
            "Evaluating Performance...\n",
            "pass 43790, training loss 7.558907508850098\n",
            "Evaluating Performance...\n",
            "pass 43800, training loss 7.668712615966797\n",
            "Evaluating Performance...\n",
            "pass 43810, training loss 6.949182510375977\n",
            "Evaluating Performance...\n",
            "pass 43820, training loss 9.3139009475708\n",
            "Evaluating Performance...\n",
            "pass 43830, training loss 6.831554889678955\n",
            "Evaluating Performance...\n",
            "pass 43840, training loss 7.526429653167725\n",
            "Evaluating Performance...\n",
            "pass 43850, training loss 7.595747470855713\n",
            "Evaluating Performance...\n",
            "pass 43860, training loss 7.480203628540039\n",
            "Evaluating Performance...\n",
            "pass 43870, training loss 5.746515274047852\n",
            "Evaluating Performance...\n",
            "pass 43880, training loss 6.405692100524902\n",
            "Evaluating Performance...\n",
            "pass 43890, training loss 6.484087944030762\n",
            "Evaluating Performance...\n",
            "pass 43900, training loss 6.233115196228027\n",
            "Evaluating Performance...\n",
            "pass 43910, training loss 7.38107967376709\n",
            "Evaluating Performance...\n",
            "pass 43920, training loss 8.449557304382324\n",
            "Evaluating Performance...\n",
            "pass 43930, training loss 6.614783763885498\n",
            "Evaluating Performance...\n",
            "pass 43940, training loss 8.382107734680176\n",
            "Evaluating Performance...\n",
            "pass 43950, training loss 7.774716854095459\n",
            "Evaluating Performance...\n",
            "pass 43960, training loss 6.503861427307129\n",
            "Evaluating Performance...\n",
            "pass 43970, training loss 8.206847190856934\n",
            "Evaluating Performance...\n",
            "pass 43980, training loss 7.506849765777588\n",
            "Evaluating Performance...\n",
            "pass 43990, training loss 6.117506980895996\n",
            "Evaluating Performance...\n",
            "pass 44000, training loss 7.371641159057617\n",
            "Evaluating Performance...\n",
            "pass 44010, training loss 7.594166278839111\n",
            "Evaluating Performance...\n",
            "pass 44020, training loss 9.248427391052246\n",
            "Evaluating Performance...\n",
            "pass 44030, training loss 7.774311542510986\n",
            "Evaluating Performance...\n",
            "pass 44040, training loss 5.647748947143555\n",
            "Evaluating Performance...\n",
            "pass 44050, training loss 6.708073616027832\n",
            "Evaluating Performance...\n",
            "pass 44060, training loss 8.240814208984375\n",
            "Evaluating Performance...\n",
            "pass 44070, training loss 6.432033538818359\n",
            "Evaluating Performance...\n",
            "pass 44080, training loss 8.266005516052246\n",
            "Evaluating Performance...\n",
            "pass 44090, training loss 8.007070541381836\n",
            "Evaluating Performance...\n",
            "pass 44100, training loss 7.66498327255249\n",
            "Evaluating Performance...\n",
            "pass 44110, training loss 6.546537399291992\n",
            "Evaluating Performance...\n",
            "pass 44120, training loss 6.537035942077637\n",
            "Evaluating Performance...\n",
            "pass 44130, training loss 5.982324600219727\n",
            "Evaluating Performance...\n",
            "pass 44140, training loss 7.558330535888672\n",
            "Evaluating Performance...\n",
            "pass 44150, training loss 7.992588996887207\n",
            "Evaluating Performance...\n",
            "pass 44160, training loss 5.052825450897217\n",
            "Evaluating Performance...\n",
            "pass 44170, training loss 5.984480381011963\n",
            "Evaluating Performance...\n",
            "pass 44180, training loss 7.26258659362793\n",
            "Evaluating Performance...\n",
            "pass 44190, training loss 5.722474098205566\n",
            "Evaluating Performance...\n",
            "pass 44200, training loss 8.580077171325684\n",
            "Evaluating Performance...\n",
            "pass 44210, training loss 7.136499404907227\n",
            "Evaluating Performance...\n",
            "pass 44220, training loss 5.883477210998535\n",
            "Evaluating Performance...\n",
            "pass 44230, training loss 8.078438758850098\n",
            "Evaluating Performance...\n",
            "pass 44240, training loss 9.093806266784668\n",
            "Evaluating Performance...\n",
            "pass 44250, training loss 5.532248497009277\n",
            "Evaluating Performance...\n",
            "pass 44260, training loss 8.060477256774902\n",
            "Evaluating Performance...\n",
            "pass 44270, training loss 6.790834903717041\n",
            "Evaluating Performance...\n",
            "pass 44280, training loss 7.5633978843688965\n",
            "Evaluating Performance...\n",
            "pass 44290, training loss 8.265583038330078\n",
            "Evaluating Performance...\n",
            "pass 44300, training loss 6.20326042175293\n",
            "Evaluating Performance...\n",
            "pass 44310, training loss 6.345890998840332\n",
            "Evaluating Performance...\n",
            "pass 44320, training loss 6.169190406799316\n",
            "Evaluating Performance...\n",
            "pass 44330, training loss 6.490564346313477\n",
            "Evaluating Performance...\n",
            "pass 44340, training loss 7.738997459411621\n",
            "Evaluating Performance...\n",
            "pass 44350, training loss 6.925605297088623\n",
            "Evaluating Performance...\n",
            "pass 44360, training loss 7.276055812835693\n",
            "Evaluating Performance...\n",
            "pass 44370, training loss 8.572521209716797\n",
            "Evaluating Performance...\n",
            "pass 44380, training loss 6.9614715576171875\n",
            "Evaluating Performance...\n",
            "pass 44390, training loss 7.7124223709106445\n",
            "Evaluating Performance...\n",
            "pass 44400, training loss 7.529642105102539\n",
            "Evaluating Performance...\n",
            "pass 44410, training loss 8.964322090148926\n",
            "Evaluating Performance...\n",
            "pass 44420, training loss 8.939697265625\n",
            "Evaluating Performance...\n",
            "pass 44430, training loss 6.662323474884033\n",
            "Evaluating Performance...\n",
            "pass 44440, training loss 9.485705375671387\n",
            "Evaluating Performance...\n",
            "pass 44450, training loss 7.590674877166748\n",
            "Evaluating Performance...\n",
            "pass 44460, training loss 6.110649108886719\n",
            "Evaluating Performance...\n",
            "pass 44470, training loss 7.085187911987305\n",
            "Evaluating Performance...\n",
            "pass 44480, training loss 8.4219331741333\n",
            "Evaluating Performance...\n",
            "pass 44490, training loss 7.723681449890137\n",
            "Evaluating Performance...\n",
            "pass 44500, training loss 7.611891746520996\n",
            "Evaluating Performance...\n",
            "pass 44510, training loss 7.390848159790039\n",
            "Evaluating Performance...\n",
            "pass 44520, training loss 7.692817687988281\n",
            "Evaluating Performance...\n",
            "pass 44530, training loss 5.718820571899414\n",
            "Evaluating Performance...\n",
            "pass 44540, training loss 6.5996222496032715\n",
            "Evaluating Performance...\n",
            "pass 44550, training loss 7.792664051055908\n",
            "Evaluating Performance...\n",
            "pass 44560, training loss 6.284115314483643\n",
            "Evaluating Performance...\n",
            "pass 44570, training loss 7.871979236602783\n",
            "Evaluating Performance...\n",
            "pass 44580, training loss 6.885073184967041\n",
            "Evaluating Performance...\n",
            "pass 44590, training loss 6.6909918785095215\n",
            "Evaluating Performance...\n",
            "pass 44600, training loss 9.052092552185059\n",
            "Evaluating Performance...\n",
            "pass 44610, training loss 8.18970775604248\n",
            "Evaluating Performance...\n",
            "pass 44620, training loss 6.119272708892822\n",
            "Evaluating Performance...\n",
            "pass 44630, training loss 9.378921508789062\n",
            "Evaluating Performance...\n",
            "pass 44640, training loss 6.726350784301758\n",
            "Evaluating Performance...\n",
            "pass 44650, training loss 8.278578758239746\n",
            "Evaluating Performance...\n",
            "pass 44660, training loss 9.055991172790527\n",
            "Evaluating Performance...\n",
            "pass 44670, training loss 6.844455242156982\n",
            "Evaluating Performance...\n",
            "pass 44680, training loss 6.433823108673096\n",
            "Evaluating Performance...\n",
            "pass 44690, training loss 9.611649513244629\n",
            "Evaluating Performance...\n",
            "pass 44700, training loss 9.911462783813477\n",
            "Evaluating Performance...\n",
            "pass 44710, training loss 6.823051452636719\n",
            "Evaluating Performance...\n",
            "pass 44720, training loss 7.798768043518066\n",
            "Evaluating Performance...\n",
            "pass 44730, training loss 7.331951141357422\n",
            "Evaluating Performance...\n",
            "pass 44740, training loss 9.15230655670166\n",
            "Evaluating Performance...\n",
            "pass 44750, training loss 7.961722373962402\n",
            "Evaluating Performance...\n",
            "pass 44760, training loss 7.8393425941467285\n",
            "Evaluating Performance...\n",
            "pass 44770, training loss 6.23204231262207\n",
            "Evaluating Performance...\n",
            "pass 44780, training loss 7.922971248626709\n",
            "Evaluating Performance...\n",
            "pass 44790, training loss 8.67578125\n",
            "Evaluating Performance...\n",
            "pass 44800, training loss 4.788784980773926\n",
            "Evaluating Performance...\n",
            "pass 44810, training loss 6.2525177001953125\n",
            "Evaluating Performance...\n",
            "pass 44820, training loss 8.675447463989258\n",
            "Evaluating Performance...\n",
            "pass 44830, training loss 6.607603073120117\n",
            "Evaluating Performance...\n",
            "pass 44840, training loss 6.708786964416504\n",
            "Evaluating Performance...\n",
            "pass 44850, training loss 6.739962100982666\n",
            "Evaluating Performance...\n",
            "pass 44860, training loss 7.070069313049316\n",
            "Evaluating Performance...\n",
            "pass 44870, training loss 7.749689102172852\n",
            "Evaluating Performance...\n",
            "pass 44880, training loss 6.768616199493408\n",
            "Evaluating Performance...\n",
            "pass 44890, training loss 8.298709869384766\n",
            "Evaluating Performance...\n",
            "pass 44900, training loss 6.914279460906982\n",
            "Evaluating Performance...\n",
            "pass 44910, training loss 7.513519287109375\n",
            "Evaluating Performance...\n",
            "pass 44920, training loss 6.968785285949707\n",
            "Evaluating Performance...\n",
            "pass 44930, training loss 7.547022342681885\n",
            "Evaluating Performance...\n",
            "pass 44940, training loss 7.495769500732422\n",
            "Evaluating Performance...\n",
            "pass 44950, training loss 8.276479721069336\n",
            "Evaluating Performance...\n",
            "pass 44960, training loss 5.229190349578857\n",
            "Evaluating Performance...\n",
            "pass 44970, training loss 7.599679946899414\n",
            "Evaluating Performance...\n",
            "pass 44980, training loss 6.418666839599609\n",
            "Evaluating Performance...\n",
            "pass 44990, training loss 7.262601852416992\n",
            "Evaluating Performance...\n",
            "pass 45000, training loss 7.3340349197387695\n",
            "Evaluating Performance...\n",
            "pass 45010, training loss 8.958698272705078\n",
            "Evaluating Performance...\n",
            "pass 45020, training loss 8.778160095214844\n",
            "Evaluating Performance...\n",
            "pass 45030, training loss 7.058713436126709\n",
            "Evaluating Performance...\n",
            "pass 45040, training loss 8.558828353881836\n",
            "Evaluating Performance...\n",
            "pass 45050, training loss 7.708255290985107\n",
            "Evaluating Performance...\n",
            "pass 45060, training loss 8.873279571533203\n",
            "Evaluating Performance...\n",
            "pass 45070, training loss 8.79323673248291\n",
            "Evaluating Performance...\n",
            "pass 45080, training loss 6.625705242156982\n",
            "Evaluating Performance...\n",
            "pass 45090, training loss 7.149454116821289\n",
            "Evaluating Performance...\n",
            "pass 45100, training loss 6.391874313354492\n",
            "Evaluating Performance...\n",
            "pass 45110, training loss 6.316494464874268\n",
            "Evaluating Performance...\n",
            "pass 45120, training loss 7.805980682373047\n",
            "Evaluating Performance...\n",
            "pass 45130, training loss 8.642385482788086\n",
            "Evaluating Performance...\n",
            "pass 45140, training loss 7.347928524017334\n",
            "Evaluating Performance...\n",
            "pass 45150, training loss 7.05642032623291\n",
            "Evaluating Performance...\n",
            "pass 45160, training loss 6.831546306610107\n",
            "Evaluating Performance...\n",
            "pass 45170, training loss 6.639993667602539\n",
            "Evaluating Performance...\n",
            "pass 45180, training loss 6.508528232574463\n",
            "Evaluating Performance...\n",
            "pass 45190, training loss 6.509281635284424\n",
            "Evaluating Performance...\n",
            "pass 45200, training loss 8.594964981079102\n",
            "Evaluating Performance...\n",
            "pass 45210, training loss 7.928294658660889\n",
            "Evaluating Performance...\n",
            "pass 45220, training loss 7.87319803237915\n",
            "Evaluating Performance...\n",
            "pass 45230, training loss 6.779970645904541\n",
            "Evaluating Performance...\n",
            "pass 45240, training loss 5.767895221710205\n",
            "Evaluating Performance...\n",
            "pass 45250, training loss 6.246737957000732\n",
            "Evaluating Performance...\n",
            "pass 45260, training loss 8.164559364318848\n",
            "Evaluating Performance...\n",
            "pass 45270, training loss 7.179640769958496\n",
            "Evaluating Performance...\n",
            "pass 45280, training loss 6.584746360778809\n",
            "Evaluating Performance...\n",
            "pass 45290, training loss 9.112602233886719\n",
            "Evaluating Performance...\n",
            "pass 45300, training loss 6.4140143394470215\n",
            "Evaluating Performance...\n",
            "pass 45310, training loss 8.614270210266113\n",
            "Evaluating Performance...\n",
            "pass 45320, training loss 6.8917412757873535\n",
            "Evaluating Performance...\n",
            "pass 45330, training loss 8.339371681213379\n",
            "Evaluating Performance...\n",
            "pass 45340, training loss 7.371387004852295\n",
            "Evaluating Performance...\n",
            "pass 45350, training loss 7.355288505554199\n",
            "Evaluating Performance...\n",
            "pass 45360, training loss 7.634195804595947\n",
            "Evaluating Performance...\n",
            "pass 45370, training loss 6.641292572021484\n",
            "Evaluating Performance...\n",
            "pass 45380, training loss 7.647090911865234\n",
            "Evaluating Performance...\n",
            "pass 45390, training loss 7.898165702819824\n",
            "Evaluating Performance...\n",
            "pass 45400, training loss 6.226377487182617\n",
            "Evaluating Performance...\n",
            "pass 45410, training loss 6.529814720153809\n",
            "Evaluating Performance...\n",
            "pass 45420, training loss 7.682014465332031\n",
            "Evaluating Performance...\n",
            "pass 45430, training loss 9.020773887634277\n",
            "Evaluating Performance...\n",
            "pass 45440, training loss 8.761540412902832\n",
            "Evaluating Performance...\n",
            "pass 45450, training loss 6.20728874206543\n",
            "Evaluating Performance...\n",
            "pass 45460, training loss 5.966188907623291\n",
            "Evaluating Performance...\n",
            "pass 45470, training loss 7.500455379486084\n",
            "Evaluating Performance...\n",
            "pass 45480, training loss 7.8959221839904785\n",
            "Evaluating Performance...\n",
            "pass 45490, training loss 6.127681732177734\n",
            "Evaluating Performance...\n",
            "pass 45500, training loss 7.297867298126221\n",
            "Evaluating Performance...\n",
            "pass 45510, training loss 6.376415729522705\n",
            "Evaluating Performance...\n",
            "pass 45520, training loss 6.644671440124512\n",
            "Evaluating Performance...\n",
            "pass 45530, training loss 8.428173065185547\n",
            "Evaluating Performance...\n",
            "pass 45540, training loss 8.289182662963867\n",
            "Evaluating Performance...\n",
            "pass 45550, training loss 8.309255599975586\n",
            "Evaluating Performance...\n",
            "pass 45560, training loss 6.611700057983398\n",
            "Evaluating Performance...\n",
            "pass 45570, training loss 6.935188293457031\n",
            "Evaluating Performance...\n",
            "pass 45580, training loss 6.871026515960693\n",
            "Evaluating Performance...\n",
            "pass 45590, training loss 7.095388412475586\n",
            "Evaluating Performance...\n",
            "pass 45600, training loss 8.933761596679688\n",
            "Evaluating Performance...\n",
            "pass 45610, training loss 7.03465461730957\n",
            "Evaluating Performance...\n",
            "pass 45620, training loss 8.540453910827637\n",
            "Evaluating Performance...\n",
            "pass 45630, training loss 6.615114212036133\n",
            "Evaluating Performance...\n",
            "pass 45640, training loss 7.632518291473389\n",
            "Evaluating Performance...\n",
            "pass 45650, training loss 6.489898681640625\n",
            "Evaluating Performance...\n",
            "pass 45660, training loss 5.945535182952881\n",
            "Evaluating Performance...\n",
            "pass 45670, training loss 7.232634544372559\n",
            "Evaluating Performance...\n",
            "pass 45680, training loss 8.33602237701416\n",
            "Evaluating Performance...\n",
            "pass 45690, training loss 8.463252067565918\n",
            "Evaluating Performance...\n",
            "pass 45700, training loss 7.897818565368652\n",
            "Evaluating Performance...\n",
            "pass 45710, training loss 6.511892318725586\n",
            "Evaluating Performance...\n",
            "pass 45720, training loss 7.48113489151001\n",
            "Evaluating Performance...\n",
            "pass 45730, training loss 8.519266128540039\n",
            "Evaluating Performance...\n",
            "pass 45740, training loss 8.23007583618164\n",
            "Evaluating Performance...\n",
            "pass 45750, training loss 7.908759593963623\n",
            "Evaluating Performance...\n",
            "pass 45760, training loss 7.771373271942139\n",
            "Evaluating Performance...\n",
            "pass 45770, training loss 8.242860794067383\n",
            "Evaluating Performance...\n",
            "pass 45780, training loss 8.637893676757812\n",
            "Evaluating Performance...\n",
            "pass 45790, training loss 7.825372695922852\n",
            "Evaluating Performance...\n",
            "pass 45800, training loss 7.047015190124512\n",
            "Evaluating Performance...\n",
            "pass 45810, training loss 7.197930812835693\n",
            "Evaluating Performance...\n",
            "pass 45820, training loss 7.070042610168457\n",
            "Evaluating Performance...\n",
            "pass 45830, training loss 9.861959457397461\n",
            "Evaluating Performance...\n",
            "pass 45840, training loss 6.7573466300964355\n",
            "Evaluating Performance...\n",
            "pass 45850, training loss 9.966741561889648\n",
            "Evaluating Performance...\n",
            "pass 45860, training loss 6.368206024169922\n",
            "Evaluating Performance...\n",
            "pass 45870, training loss 6.550860404968262\n",
            "Evaluating Performance...\n",
            "pass 45880, training loss 7.226861000061035\n",
            "Evaluating Performance...\n",
            "pass 45890, training loss 7.104530334472656\n",
            "Evaluating Performance...\n",
            "pass 45900, training loss 8.261157035827637\n",
            "Evaluating Performance...\n",
            "pass 45910, training loss 7.316915512084961\n",
            "Evaluating Performance...\n",
            "pass 45920, training loss 8.324312210083008\n",
            "Evaluating Performance...\n",
            "pass 45930, training loss 6.690325736999512\n",
            "Evaluating Performance...\n",
            "pass 45940, training loss 6.102244853973389\n",
            "Evaluating Performance...\n",
            "pass 45950, training loss 6.142446041107178\n",
            "Evaluating Performance...\n",
            "pass 45960, training loss 5.601641654968262\n",
            "Evaluating Performance...\n",
            "pass 45970, training loss 8.637689590454102\n",
            "Evaluating Performance...\n",
            "pass 45980, training loss 7.572826862335205\n",
            "Evaluating Performance...\n",
            "pass 45990, training loss 6.589617729187012\n",
            "Evaluating Performance...\n",
            "pass 46000, training loss 7.027099609375\n",
            "Evaluating Performance...\n",
            "pass 46010, training loss 6.744227886199951\n",
            "Evaluating Performance...\n",
            "pass 46020, training loss 6.128778457641602\n",
            "Evaluating Performance...\n",
            "pass 46030, training loss 6.151646614074707\n",
            "Evaluating Performance...\n",
            "pass 46040, training loss 7.737830638885498\n",
            "Evaluating Performance...\n",
            "pass 46050, training loss 8.853382110595703\n",
            "Evaluating Performance...\n",
            "pass 46060, training loss 8.269607543945312\n",
            "Evaluating Performance...\n",
            "pass 46070, training loss 9.555085182189941\n",
            "Evaluating Performance...\n",
            "pass 46080, training loss 8.3524169921875\n",
            "Evaluating Performance...\n",
            "pass 46090, training loss 6.876791954040527\n",
            "Evaluating Performance...\n",
            "pass 46100, training loss 6.41742467880249\n",
            "Evaluating Performance...\n",
            "pass 46110, training loss 6.210490703582764\n",
            "Evaluating Performance...\n",
            "pass 46120, training loss 7.559076309204102\n",
            "Evaluating Performance...\n",
            "pass 46130, training loss 7.596373558044434\n",
            "Evaluating Performance...\n",
            "pass 46140, training loss 7.406640529632568\n",
            "Evaluating Performance...\n",
            "pass 46150, training loss 5.7978057861328125\n",
            "Evaluating Performance...\n",
            "pass 46160, training loss 6.401607513427734\n",
            "Evaluating Performance...\n",
            "pass 46170, training loss 6.785032749176025\n",
            "Evaluating Performance...\n",
            "pass 46180, training loss 9.490650177001953\n",
            "Evaluating Performance...\n",
            "pass 46190, training loss 6.257829666137695\n",
            "Evaluating Performance...\n",
            "pass 46200, training loss 8.383423805236816\n",
            "Evaluating Performance...\n",
            "pass 46210, training loss 7.959653854370117\n",
            "Evaluating Performance...\n",
            "pass 46220, training loss 8.250051498413086\n",
            "Evaluating Performance...\n",
            "pass 46230, training loss 7.629225730895996\n",
            "Evaluating Performance...\n",
            "pass 46240, training loss 7.735378265380859\n",
            "Evaluating Performance...\n",
            "pass 46250, training loss 8.178204536437988\n",
            "Evaluating Performance...\n",
            "pass 46260, training loss 6.646944522857666\n",
            "Evaluating Performance...\n",
            "pass 46270, training loss 6.317867279052734\n",
            "Evaluating Performance...\n",
            "pass 46280, training loss 7.530333042144775\n",
            "Evaluating Performance...\n",
            "pass 46290, training loss 7.5252299308776855\n",
            "Evaluating Performance...\n",
            "pass 46300, training loss 5.988491535186768\n",
            "Evaluating Performance...\n",
            "pass 46310, training loss 6.462568759918213\n",
            "Evaluating Performance...\n",
            "pass 46320, training loss 6.033555030822754\n",
            "Evaluating Performance...\n",
            "pass 46330, training loss 10.863347053527832\n",
            "Evaluating Performance...\n",
            "pass 46340, training loss 8.542031288146973\n",
            "Evaluating Performance...\n",
            "pass 46350, training loss 7.872746467590332\n",
            "Evaluating Performance...\n",
            "pass 46360, training loss 7.0105881690979\n",
            "Evaluating Performance...\n",
            "pass 46370, training loss 6.7909440994262695\n",
            "Evaluating Performance...\n",
            "pass 46380, training loss 8.36484146118164\n",
            "Evaluating Performance...\n",
            "pass 46390, training loss 7.726637363433838\n",
            "Evaluating Performance...\n",
            "pass 46400, training loss 8.314560890197754\n",
            "Evaluating Performance...\n",
            "pass 46410, training loss 8.341611862182617\n",
            "Evaluating Performance...\n",
            "pass 46420, training loss 6.002136707305908\n",
            "Evaluating Performance...\n",
            "pass 46430, training loss 7.393497943878174\n",
            "Evaluating Performance...\n",
            "pass 46440, training loss 7.063347816467285\n",
            "Evaluating Performance...\n",
            "pass 46450, training loss 8.257420539855957\n",
            "Evaluating Performance...\n",
            "pass 46460, training loss 7.3173418045043945\n",
            "Evaluating Performance...\n",
            "pass 46470, training loss 6.368829250335693\n",
            "Evaluating Performance...\n",
            "pass 46480, training loss 6.324589729309082\n",
            "Evaluating Performance...\n",
            "pass 46490, training loss 7.1805596351623535\n",
            "Evaluating Performance...\n",
            "pass 46500, training loss 6.697455406188965\n",
            "Evaluating Performance...\n",
            "pass 46510, training loss 7.541146278381348\n",
            "Evaluating Performance...\n",
            "pass 46520, training loss 7.099833011627197\n",
            "Evaluating Performance...\n",
            "pass 46530, training loss 8.244163513183594\n",
            "Evaluating Performance...\n",
            "pass 46540, training loss 8.615220069885254\n",
            "Evaluating Performance...\n",
            "pass 46550, training loss 6.183948993682861\n",
            "Evaluating Performance...\n",
            "pass 46560, training loss 6.5216827392578125\n",
            "Evaluating Performance...\n",
            "pass 46570, training loss 7.813479423522949\n",
            "Evaluating Performance...\n",
            "pass 46580, training loss 7.099876403808594\n",
            "Evaluating Performance...\n",
            "pass 46590, training loss 6.1813273429870605\n",
            "Evaluating Performance...\n",
            "pass 46600, training loss 6.7834320068359375\n",
            "Evaluating Performance...\n",
            "pass 46610, training loss 6.735306739807129\n",
            "Evaluating Performance...\n",
            "pass 46620, training loss 6.475928783416748\n",
            "Evaluating Performance...\n",
            "pass 46630, training loss 6.83692741394043\n",
            "Evaluating Performance...\n",
            "pass 46640, training loss 7.525465488433838\n",
            "Evaluating Performance...\n",
            "pass 46650, training loss 5.567944049835205\n",
            "Evaluating Performance...\n",
            "pass 46660, training loss 5.988874912261963\n",
            "Evaluating Performance...\n",
            "pass 46670, training loss 8.198929786682129\n",
            "Evaluating Performance...\n",
            "pass 46680, training loss 11.495218276977539\n",
            "Evaluating Performance...\n",
            "pass 46690, training loss 7.509160995483398\n",
            "Evaluating Performance...\n",
            "pass 46700, training loss 6.352679252624512\n",
            "Evaluating Performance...\n",
            "pass 46710, training loss 8.064577102661133\n",
            "Evaluating Performance...\n",
            "pass 46720, training loss 8.256915092468262\n",
            "Evaluating Performance...\n",
            "pass 46730, training loss 6.663856029510498\n",
            "Evaluating Performance...\n",
            "pass 46740, training loss 8.650869369506836\n",
            "Evaluating Performance...\n",
            "pass 46750, training loss 5.958744525909424\n",
            "Evaluating Performance...\n",
            "pass 46760, training loss 7.078929901123047\n",
            "Evaluating Performance...\n",
            "pass 46770, training loss 7.246488571166992\n",
            "Evaluating Performance...\n",
            "pass 46780, training loss 8.02128791809082\n",
            "Evaluating Performance...\n",
            "pass 46790, training loss 9.28244400024414\n",
            "Evaluating Performance...\n",
            "pass 46800, training loss 9.254511833190918\n",
            "Evaluating Performance...\n",
            "pass 46810, training loss 7.560196399688721\n",
            "Evaluating Performance...\n",
            "pass 46820, training loss 7.522965431213379\n",
            "Evaluating Performance...\n",
            "pass 46830, training loss 8.378528594970703\n",
            "Evaluating Performance...\n",
            "pass 46840, training loss 10.224997520446777\n",
            "Evaluating Performance...\n",
            "pass 46850, training loss 6.7277679443359375\n",
            "Evaluating Performance...\n",
            "pass 46860, training loss 8.572031021118164\n",
            "Evaluating Performance...\n",
            "pass 46870, training loss 6.088196754455566\n",
            "Evaluating Performance...\n",
            "pass 46880, training loss 7.408231735229492\n",
            "Evaluating Performance...\n",
            "pass 46890, training loss 6.097084045410156\n",
            "Evaluating Performance...\n",
            "pass 46900, training loss 6.599425315856934\n",
            "Evaluating Performance...\n",
            "pass 46910, training loss 8.240436553955078\n",
            "Evaluating Performance...\n",
            "pass 46920, training loss 5.725742816925049\n",
            "Evaluating Performance...\n",
            "pass 46930, training loss 6.8822197914123535\n",
            "Evaluating Performance...\n",
            "pass 46940, training loss 6.868051528930664\n",
            "Evaluating Performance...\n",
            "pass 46950, training loss 8.365778923034668\n",
            "Evaluating Performance...\n",
            "pass 46960, training loss 7.646958827972412\n",
            "Evaluating Performance...\n",
            "pass 46970, training loss 6.5362725257873535\n",
            "Evaluating Performance...\n",
            "pass 46980, training loss 7.156998634338379\n",
            "Evaluating Performance...\n",
            "pass 46990, training loss 8.518714904785156\n",
            "Evaluating Performance...\n",
            "pass 47000, training loss 7.617464542388916\n",
            "Evaluating Performance...\n",
            "pass 47010, training loss 8.257479667663574\n",
            "Evaluating Performance...\n",
            "pass 47020, training loss 8.846261978149414\n",
            "Evaluating Performance...\n",
            "pass 47030, training loss 7.533971786499023\n",
            "Evaluating Performance...\n",
            "pass 47040, training loss 8.542871475219727\n",
            "Evaluating Performance...\n",
            "pass 47050, training loss 6.291245460510254\n",
            "Evaluating Performance...\n",
            "pass 47060, training loss 7.082927703857422\n",
            "Evaluating Performance...\n",
            "pass 47070, training loss 6.945645809173584\n",
            "Evaluating Performance...\n",
            "pass 47080, training loss 5.871157646179199\n",
            "Evaluating Performance...\n",
            "pass 47090, training loss 7.609631061553955\n",
            "Evaluating Performance...\n",
            "pass 47100, training loss 8.697750091552734\n",
            "Evaluating Performance...\n",
            "pass 47110, training loss 7.998946189880371\n",
            "Evaluating Performance...\n",
            "pass 47120, training loss 6.158656597137451\n",
            "Evaluating Performance...\n",
            "pass 47130, training loss 8.325486183166504\n",
            "Evaluating Performance...\n",
            "pass 47140, training loss 6.485119342803955\n",
            "Evaluating Performance...\n",
            "pass 47150, training loss 7.261697292327881\n",
            "Evaluating Performance...\n",
            "pass 47160, training loss 7.827589988708496\n",
            "Evaluating Performance...\n",
            "pass 47170, training loss 6.467574119567871\n",
            "Evaluating Performance...\n",
            "pass 47180, training loss 6.637094020843506\n",
            "Evaluating Performance...\n",
            "pass 47190, training loss 8.146485328674316\n",
            "Evaluating Performance...\n",
            "pass 47200, training loss 8.522992134094238\n",
            "Evaluating Performance...\n",
            "pass 47210, training loss 6.345566272735596\n",
            "Evaluating Performance...\n",
            "pass 47220, training loss 7.739282131195068\n",
            "Evaluating Performance...\n",
            "pass 47230, training loss 7.688971519470215\n",
            "Evaluating Performance...\n",
            "pass 47240, training loss 6.267353534698486\n",
            "Evaluating Performance...\n",
            "pass 47250, training loss 9.302446365356445\n",
            "Evaluating Performance...\n",
            "pass 47260, training loss 7.513155937194824\n",
            "Evaluating Performance...\n",
            "pass 47270, training loss 7.3853983879089355\n",
            "Evaluating Performance...\n",
            "pass 47280, training loss 7.775557518005371\n",
            "Evaluating Performance...\n",
            "pass 47290, training loss 9.704774856567383\n",
            "Evaluating Performance...\n",
            "pass 47300, training loss 9.02502727508545\n",
            "Evaluating Performance...\n",
            "pass 47310, training loss 7.3401336669921875\n",
            "Evaluating Performance...\n",
            "pass 47320, training loss 7.9376444816589355\n",
            "Evaluating Performance...\n",
            "pass 47330, training loss 8.494222640991211\n",
            "Evaluating Performance...\n",
            "pass 47340, training loss 5.375988483428955\n",
            "Evaluating Performance...\n",
            "pass 47350, training loss 8.083216667175293\n",
            "Evaluating Performance...\n",
            "pass 47360, training loss 6.368948459625244\n",
            "Evaluating Performance...\n",
            "pass 47370, training loss 6.665018558502197\n",
            "Evaluating Performance...\n",
            "pass 47380, training loss 6.814548969268799\n",
            "Evaluating Performance...\n",
            "pass 47390, training loss 7.300095558166504\n",
            "Evaluating Performance...\n",
            "pass 47400, training loss 6.858736515045166\n",
            "Evaluating Performance...\n",
            "pass 47410, training loss 10.453913688659668\n",
            "Evaluating Performance...\n",
            "pass 47420, training loss 6.003397464752197\n",
            "Evaluating Performance...\n",
            "pass 47430, training loss 5.021368026733398\n",
            "Evaluating Performance...\n",
            "pass 47440, training loss 6.7834343910217285\n",
            "Evaluating Performance...\n",
            "pass 47450, training loss 8.341485023498535\n",
            "Evaluating Performance...\n",
            "pass 47460, training loss 8.041521072387695\n",
            "Evaluating Performance...\n",
            "pass 47470, training loss 6.023165225982666\n",
            "Evaluating Performance...\n",
            "pass 47480, training loss 5.720191478729248\n",
            "Evaluating Performance...\n",
            "pass 47490, training loss 7.296478748321533\n",
            "Evaluating Performance...\n",
            "pass 47500, training loss 5.827602386474609\n",
            "Evaluating Performance...\n",
            "pass 47510, training loss 10.146885871887207\n",
            "Evaluating Performance...\n",
            "pass 47520, training loss 8.886564254760742\n",
            "Evaluating Performance...\n",
            "pass 47530, training loss 7.025191307067871\n",
            "Evaluating Performance...\n",
            "pass 47540, training loss 6.374551773071289\n",
            "Evaluating Performance...\n",
            "pass 47550, training loss 5.872834205627441\n",
            "Evaluating Performance...\n",
            "pass 47560, training loss 7.927942276000977\n",
            "Evaluating Performance...\n",
            "pass 47570, training loss 6.396402835845947\n",
            "Evaluating Performance...\n",
            "pass 47580, training loss 7.100451469421387\n",
            "Evaluating Performance...\n",
            "pass 47590, training loss 7.425044536590576\n",
            "Evaluating Performance...\n",
            "pass 47600, training loss 9.031783103942871\n",
            "Evaluating Performance...\n",
            "pass 47610, training loss 5.917555809020996\n",
            "Evaluating Performance...\n",
            "pass 47620, training loss 6.700467586517334\n",
            "Evaluating Performance...\n",
            "pass 47630, training loss 6.277830600738525\n",
            "Evaluating Performance...\n",
            "pass 47640, training loss 6.452659606933594\n",
            "Evaluating Performance...\n",
            "pass 47650, training loss 6.737854480743408\n",
            "Evaluating Performance...\n",
            "pass 47660, training loss 8.198577880859375\n",
            "Evaluating Performance...\n",
            "pass 47670, training loss 6.843857765197754\n",
            "Evaluating Performance...\n",
            "pass 47680, training loss 6.50541877746582\n",
            "Evaluating Performance...\n",
            "pass 47690, training loss 5.532607078552246\n",
            "Evaluating Performance...\n",
            "pass 47700, training loss 7.1734538078308105\n",
            "Evaluating Performance...\n",
            "pass 47710, training loss 6.654726505279541\n",
            "Evaluating Performance...\n",
            "pass 47720, training loss 6.821671485900879\n",
            "Evaluating Performance...\n",
            "pass 47730, training loss 7.113805294036865\n",
            "Evaluating Performance...\n",
            "pass 47740, training loss 7.412644863128662\n",
            "Evaluating Performance...\n",
            "pass 47750, training loss 8.623174667358398\n",
            "Evaluating Performance...\n",
            "pass 47760, training loss 7.88754415512085\n",
            "Evaluating Performance...\n",
            "pass 47770, training loss 7.259474277496338\n",
            "Evaluating Performance...\n",
            "pass 47780, training loss 7.5489726066589355\n",
            "Evaluating Performance...\n",
            "pass 47790, training loss 6.846212387084961\n",
            "Evaluating Performance...\n",
            "pass 47800, training loss 8.285736083984375\n",
            "Evaluating Performance...\n",
            "pass 47810, training loss 7.917577743530273\n",
            "Evaluating Performance...\n",
            "pass 47820, training loss 8.108113288879395\n",
            "Evaluating Performance...\n",
            "pass 47830, training loss 7.3167643547058105\n",
            "Evaluating Performance...\n",
            "pass 47840, training loss 6.728046894073486\n",
            "Evaluating Performance...\n",
            "pass 47850, training loss 6.127638339996338\n",
            "Evaluating Performance...\n",
            "pass 47860, training loss 7.567820072174072\n",
            "Evaluating Performance...\n",
            "pass 47870, training loss 7.674081802368164\n",
            "Evaluating Performance...\n",
            "pass 47880, training loss 6.877464294433594\n",
            "Evaluating Performance...\n",
            "pass 47890, training loss 9.019289016723633\n",
            "Evaluating Performance...\n",
            "pass 47900, training loss 7.341246128082275\n",
            "Evaluating Performance...\n",
            "pass 47910, training loss 7.832999229431152\n",
            "Evaluating Performance...\n",
            "pass 47920, training loss 8.561721801757812\n",
            "Evaluating Performance...\n",
            "pass 47930, training loss 6.43894100189209\n",
            "Evaluating Performance...\n",
            "pass 47940, training loss 6.054919719696045\n",
            "Evaluating Performance...\n",
            "pass 47950, training loss 6.532323837280273\n",
            "Evaluating Performance...\n",
            "pass 47960, training loss 7.819569110870361\n",
            "Evaluating Performance...\n",
            "pass 47970, training loss 6.295419216156006\n",
            "Evaluating Performance...\n",
            "pass 47980, training loss 9.433090209960938\n",
            "Evaluating Performance...\n",
            "pass 47990, training loss 7.017476558685303\n",
            "Evaluating Performance...\n",
            "pass 48000, training loss 7.323631286621094\n",
            "Evaluating Performance...\n",
            "pass 48010, training loss 5.958167552947998\n",
            "Evaluating Performance...\n",
            "pass 48020, training loss 6.988658905029297\n",
            "Evaluating Performance...\n",
            "pass 48030, training loss 6.087429523468018\n",
            "Evaluating Performance...\n",
            "pass 48040, training loss 7.471045970916748\n",
            "Evaluating Performance...\n",
            "pass 48050, training loss 8.162271499633789\n",
            "Evaluating Performance...\n",
            "pass 48060, training loss 9.479336738586426\n",
            "Evaluating Performance...\n",
            "pass 48070, training loss 5.347557067871094\n",
            "Evaluating Performance...\n",
            "pass 48080, training loss 6.019467830657959\n",
            "Evaluating Performance...\n",
            "pass 48090, training loss 7.833682537078857\n",
            "Evaluating Performance...\n",
            "pass 48100, training loss 7.163615703582764\n",
            "Evaluating Performance...\n",
            "pass 48110, training loss 7.85328483581543\n",
            "Evaluating Performance...\n",
            "pass 48120, training loss 8.278997421264648\n",
            "Evaluating Performance...\n",
            "pass 48130, training loss 7.897685527801514\n",
            "Evaluating Performance...\n",
            "pass 48140, training loss 6.1344170570373535\n",
            "Evaluating Performance...\n",
            "pass 48150, training loss 7.537881851196289\n",
            "Evaluating Performance...\n",
            "pass 48160, training loss 6.2564568519592285\n",
            "Evaluating Performance...\n",
            "pass 48170, training loss 5.5066423416137695\n",
            "Evaluating Performance...\n",
            "pass 48180, training loss 7.495207786560059\n",
            "Evaluating Performance...\n",
            "pass 48190, training loss 7.271443843841553\n",
            "Evaluating Performance...\n",
            "pass 48200, training loss 6.6080169677734375\n",
            "Evaluating Performance...\n",
            "pass 48210, training loss 7.334586143493652\n",
            "Evaluating Performance...\n",
            "pass 48220, training loss 7.412681579589844\n",
            "Evaluating Performance...\n",
            "pass 48230, training loss 7.747542381286621\n",
            "Evaluating Performance...\n",
            "pass 48240, training loss 6.916032791137695\n",
            "Evaluating Performance...\n",
            "pass 48250, training loss 9.407482147216797\n",
            "Evaluating Performance...\n",
            "pass 48260, training loss 8.852542877197266\n",
            "Evaluating Performance...\n",
            "pass 48270, training loss 7.81107234954834\n",
            "Evaluating Performance...\n",
            "pass 48280, training loss 7.9103593826293945\n",
            "Evaluating Performance...\n",
            "pass 48290, training loss 6.018066883087158\n",
            "Evaluating Performance...\n",
            "pass 48300, training loss 6.78466796875\n",
            "Evaluating Performance...\n",
            "pass 48310, training loss 7.151238918304443\n",
            "Evaluating Performance...\n",
            "pass 48320, training loss 7.181049823760986\n",
            "Evaluating Performance...\n",
            "pass 48330, training loss 7.9026360511779785\n",
            "Evaluating Performance...\n",
            "pass 48340, training loss 7.389523983001709\n",
            "Evaluating Performance...\n",
            "pass 48350, training loss 7.569737911224365\n",
            "Evaluating Performance...\n",
            "pass 48360, training loss 6.890804767608643\n",
            "Evaluating Performance...\n",
            "pass 48370, training loss 6.64783239364624\n",
            "Evaluating Performance...\n",
            "pass 48380, training loss 7.497671127319336\n",
            "Evaluating Performance...\n",
            "pass 48390, training loss 6.247311115264893\n",
            "Evaluating Performance...\n",
            "pass 48400, training loss 6.683831214904785\n",
            "Evaluating Performance...\n",
            "pass 48410, training loss 8.125149726867676\n",
            "Evaluating Performance...\n",
            "pass 48420, training loss 6.624762535095215\n",
            "Evaluating Performance...\n",
            "pass 48430, training loss 7.157537460327148\n",
            "Evaluating Performance...\n",
            "pass 48440, training loss 6.225945472717285\n",
            "Evaluating Performance...\n",
            "pass 48450, training loss 6.662384986877441\n",
            "Evaluating Performance...\n",
            "pass 48460, training loss 6.741732120513916\n",
            "Evaluating Performance...\n",
            "pass 48470, training loss 6.177193641662598\n",
            "Evaluating Performance...\n",
            "pass 48480, training loss 7.456206321716309\n",
            "Evaluating Performance...\n",
            "pass 48490, training loss 7.496518611907959\n",
            "Evaluating Performance...\n",
            "pass 48500, training loss 9.708295822143555\n",
            "Evaluating Performance...\n",
            "pass 48510, training loss 7.924107551574707\n",
            "Evaluating Performance...\n",
            "pass 48520, training loss 7.66829252243042\n",
            "Evaluating Performance...\n",
            "pass 48530, training loss 7.0008225440979\n",
            "Evaluating Performance...\n",
            "pass 48540, training loss 9.759843826293945\n",
            "Evaluating Performance...\n",
            "pass 48550, training loss 7.145519256591797\n",
            "Evaluating Performance...\n",
            "pass 48560, training loss 7.922739505767822\n",
            "Evaluating Performance...\n",
            "pass 48570, training loss 7.410995960235596\n",
            "Evaluating Performance...\n",
            "pass 48580, training loss 6.311241149902344\n",
            "Evaluating Performance...\n",
            "pass 48590, training loss 7.2689080238342285\n",
            "Evaluating Performance...\n",
            "pass 48600, training loss 6.752167701721191\n",
            "Evaluating Performance...\n",
            "pass 48610, training loss 6.66756534576416\n",
            "Evaluating Performance...\n",
            "pass 48620, training loss 6.917895793914795\n",
            "Evaluating Performance...\n",
            "pass 48630, training loss 7.107522487640381\n",
            "Evaluating Performance...\n",
            "pass 48640, training loss 8.906514167785645\n",
            "Evaluating Performance...\n",
            "pass 48650, training loss 9.172903060913086\n",
            "Evaluating Performance...\n",
            "pass 48660, training loss 7.397266864776611\n",
            "Evaluating Performance...\n",
            "pass 48670, training loss 8.107264518737793\n",
            "Evaluating Performance...\n",
            "pass 48680, training loss 7.432279586791992\n",
            "Evaluating Performance...\n",
            "pass 48690, training loss 6.990128040313721\n",
            "Evaluating Performance...\n",
            "pass 48700, training loss 6.570765018463135\n",
            "Evaluating Performance...\n",
            "pass 48710, training loss 5.761213302612305\n",
            "Evaluating Performance...\n",
            "pass 48720, training loss 6.026072025299072\n",
            "Evaluating Performance...\n",
            "pass 48730, training loss 7.349362850189209\n",
            "Evaluating Performance...\n",
            "pass 48740, training loss 7.684289455413818\n",
            "Evaluating Performance...\n",
            "pass 48750, training loss 7.734059810638428\n",
            "Evaluating Performance...\n",
            "pass 48760, training loss 7.85535192489624\n",
            "Evaluating Performance...\n",
            "pass 48770, training loss 7.530961036682129\n",
            "Evaluating Performance...\n",
            "pass 48780, training loss 6.848621845245361\n",
            "Evaluating Performance...\n",
            "pass 48790, training loss 5.995219707489014\n",
            "Evaluating Performance...\n",
            "pass 48800, training loss 5.71418571472168\n",
            "Evaluating Performance...\n",
            "pass 48810, training loss 7.560580730438232\n",
            "Evaluating Performance...\n",
            "pass 48820, training loss 6.965670585632324\n",
            "Evaluating Performance...\n",
            "pass 48830, training loss 6.431872844696045\n",
            "Evaluating Performance...\n",
            "pass 48840, training loss 9.483895301818848\n",
            "Evaluating Performance...\n",
            "pass 48850, training loss 5.86716365814209\n",
            "Evaluating Performance...\n",
            "pass 48860, training loss 8.331560134887695\n",
            "Evaluating Performance...\n",
            "pass 48870, training loss 6.700891494750977\n",
            "Evaluating Performance...\n",
            "pass 48880, training loss 9.23968505859375\n",
            "Evaluating Performance...\n",
            "pass 48890, training loss 8.44296646118164\n",
            "Evaluating Performance...\n",
            "pass 48900, training loss 5.616770267486572\n",
            "Evaluating Performance...\n",
            "pass 48910, training loss 6.81907844543457\n",
            "Evaluating Performance...\n",
            "pass 48920, training loss 6.736378192901611\n",
            "Evaluating Performance...\n",
            "pass 48930, training loss 6.5384521484375\n",
            "Evaluating Performance...\n",
            "pass 48940, training loss 5.987095832824707\n",
            "Evaluating Performance...\n",
            "pass 48950, training loss 8.771768569946289\n",
            "Evaluating Performance...\n",
            "pass 48960, training loss 6.044829368591309\n",
            "Evaluating Performance...\n",
            "pass 48970, training loss 7.204987525939941\n",
            "Evaluating Performance...\n",
            "pass 48980, training loss 6.431366920471191\n",
            "Evaluating Performance...\n",
            "pass 48990, training loss 5.874453544616699\n",
            "Evaluating Performance...\n",
            "pass 49000, training loss 7.708992004394531\n",
            "Evaluating Performance...\n",
            "pass 49010, training loss 8.774703979492188\n",
            "Evaluating Performance...\n",
            "pass 49020, training loss 9.145828247070312\n",
            "Evaluating Performance...\n",
            "pass 49030, training loss 6.848948001861572\n",
            "Evaluating Performance...\n",
            "pass 49040, training loss 7.325185298919678\n",
            "Evaluating Performance...\n",
            "pass 49050, training loss 7.8987579345703125\n",
            "Evaluating Performance...\n",
            "pass 49060, training loss 6.16804313659668\n",
            "Evaluating Performance...\n",
            "pass 49070, training loss 7.318183422088623\n",
            "Evaluating Performance...\n",
            "pass 49080, training loss 5.858763217926025\n",
            "Evaluating Performance...\n",
            "pass 49090, training loss 8.679706573486328\n",
            "Evaluating Performance...\n",
            "pass 49100, training loss 7.257908344268799\n",
            "Evaluating Performance...\n",
            "pass 49110, training loss 7.5076189041137695\n",
            "Evaluating Performance...\n",
            "pass 49120, training loss 8.861485481262207\n",
            "Evaluating Performance...\n",
            "pass 49130, training loss 5.923842430114746\n",
            "Evaluating Performance...\n",
            "pass 49140, training loss 5.857020854949951\n",
            "Evaluating Performance...\n",
            "pass 49150, training loss 11.350217819213867\n",
            "Evaluating Performance...\n",
            "pass 49160, training loss 7.423740863800049\n",
            "Evaluating Performance...\n",
            "pass 49170, training loss 7.361988544464111\n",
            "Evaluating Performance...\n",
            "pass 49180, training loss 7.146308898925781\n",
            "Evaluating Performance...\n",
            "pass 49190, training loss 7.873032093048096\n",
            "Evaluating Performance...\n",
            "pass 49200, training loss 8.593385696411133\n",
            "Evaluating Performance...\n",
            "pass 49210, training loss 6.895273208618164\n",
            "Evaluating Performance...\n",
            "pass 49220, training loss 8.220778465270996\n",
            "Evaluating Performance...\n",
            "pass 49230, training loss 8.271797180175781\n",
            "Evaluating Performance...\n",
            "pass 49240, training loss 6.817749977111816\n",
            "Evaluating Performance...\n",
            "pass 49250, training loss 7.158370018005371\n",
            "Evaluating Performance...\n",
            "pass 49260, training loss 6.970608234405518\n",
            "Evaluating Performance...\n",
            "pass 49270, training loss 7.130724906921387\n",
            "Evaluating Performance...\n",
            "pass 49280, training loss 7.460996627807617\n",
            "Evaluating Performance...\n",
            "pass 49290, training loss 7.554588317871094\n",
            "Evaluating Performance...\n",
            "pass 49300, training loss 6.876057147979736\n",
            "Evaluating Performance...\n",
            "pass 49310, training loss 4.808432102203369\n",
            "Evaluating Performance...\n",
            "pass 49320, training loss 6.677774429321289\n",
            "Evaluating Performance...\n",
            "pass 49330, training loss 5.939079284667969\n",
            "Evaluating Performance...\n",
            "pass 49340, training loss 6.300400257110596\n",
            "Evaluating Performance...\n",
            "pass 49350, training loss 7.378054618835449\n",
            "Evaluating Performance...\n",
            "pass 49360, training loss 9.701128005981445\n",
            "Evaluating Performance...\n",
            "pass 49370, training loss 6.955531597137451\n",
            "Evaluating Performance...\n",
            "pass 49380, training loss 7.525516033172607\n",
            "Evaluating Performance...\n",
            "pass 49390, training loss 8.487889289855957\n",
            "Evaluating Performance...\n",
            "pass 49400, training loss 6.724064826965332\n",
            "Evaluating Performance...\n",
            "pass 49410, training loss 6.334704875946045\n",
            "Evaluating Performance...\n",
            "pass 49420, training loss 7.202465534210205\n",
            "Evaluating Performance...\n",
            "pass 49430, training loss 6.914356708526611\n",
            "Evaluating Performance...\n",
            "pass 49440, training loss 5.496643543243408\n",
            "Evaluating Performance...\n",
            "pass 49450, training loss 6.721597194671631\n",
            "Evaluating Performance...\n",
            "pass 49460, training loss 7.03575325012207\n",
            "Evaluating Performance...\n",
            "pass 49470, training loss 7.062249660491943\n",
            "Evaluating Performance...\n",
            "pass 49480, training loss 6.591082572937012\n",
            "Evaluating Performance...\n",
            "pass 49490, training loss 7.357457160949707\n",
            "Evaluating Performance...\n",
            "pass 49500, training loss 6.103682518005371\n",
            "Evaluating Performance...\n",
            "pass 49510, training loss 6.515966415405273\n",
            "Evaluating Performance...\n",
            "pass 49520, training loss 6.122303485870361\n",
            "Evaluating Performance...\n",
            "pass 49530, training loss 8.051283836364746\n",
            "Evaluating Performance...\n",
            "pass 49540, training loss 5.235301971435547\n",
            "Evaluating Performance...\n",
            "pass 49550, training loss 7.56497859954834\n",
            "Evaluating Performance...\n",
            "pass 49560, training loss 8.257698059082031\n",
            "Evaluating Performance...\n",
            "pass 49570, training loss 6.401252746582031\n",
            "Evaluating Performance...\n",
            "pass 49580, training loss 7.813869953155518\n",
            "Evaluating Performance...\n",
            "pass 49590, training loss 8.219985008239746\n",
            "Evaluating Performance...\n",
            "pass 49600, training loss 8.36370849609375\n",
            "Evaluating Performance...\n",
            "pass 49610, training loss 7.318172931671143\n",
            "Evaluating Performance...\n",
            "pass 49620, training loss 7.768868446350098\n",
            "Evaluating Performance...\n",
            "pass 49630, training loss 6.06428337097168\n",
            "Evaluating Performance...\n",
            "pass 49640, training loss 9.301319122314453\n",
            "Evaluating Performance...\n",
            "pass 49650, training loss 7.126035213470459\n",
            "Evaluating Performance...\n",
            "pass 49660, training loss 6.905961036682129\n",
            "Evaluating Performance...\n",
            "pass 49670, training loss 7.23895263671875\n",
            "Evaluating Performance...\n",
            "pass 49680, training loss 7.635053634643555\n",
            "Evaluating Performance...\n",
            "pass 49690, training loss 7.943798542022705\n",
            "Evaluating Performance...\n",
            "pass 49700, training loss 8.46937084197998\n",
            "Evaluating Performance...\n",
            "pass 49710, training loss 7.392344951629639\n",
            "Evaluating Performance...\n",
            "pass 49720, training loss 8.365204811096191\n",
            "Evaluating Performance...\n",
            "pass 49730, training loss 6.860236644744873\n",
            "Evaluating Performance...\n",
            "pass 49740, training loss 7.553922176361084\n",
            "Evaluating Performance...\n",
            "pass 49750, training loss 6.268772602081299\n",
            "Evaluating Performance...\n",
            "pass 49760, training loss 8.85374641418457\n",
            "Evaluating Performance...\n",
            "pass 49770, training loss 6.16422700881958\n",
            "Evaluating Performance...\n",
            "pass 49780, training loss 6.8808417320251465\n",
            "Evaluating Performance...\n",
            "pass 49790, training loss 6.690292835235596\n",
            "Evaluating Performance...\n",
            "pass 49800, training loss 6.0473151206970215\n",
            "Evaluating Performance...\n",
            "pass 49810, training loss 6.194931983947754\n",
            "Evaluating Performance...\n",
            "pass 49820, training loss 9.290719985961914\n",
            "Evaluating Performance...\n",
            "pass 49830, training loss 6.761538982391357\n",
            "Evaluating Performance...\n",
            "pass 49840, training loss 5.859376907348633\n",
            "Evaluating Performance...\n",
            "pass 49850, training loss 7.3305983543396\n",
            "Evaluating Performance...\n",
            "pass 49860, training loss 10.730779647827148\n",
            "Evaluating Performance...\n",
            "pass 49870, training loss 7.413053512573242\n",
            "Evaluating Performance...\n",
            "pass 49880, training loss 6.595992088317871\n",
            "Evaluating Performance...\n",
            "pass 49890, training loss 8.021124839782715\n",
            "Evaluating Performance...\n",
            "pass 49900, training loss 6.632983684539795\n",
            "Evaluating Performance...\n",
            "pass 49910, training loss 7.258250713348389\n",
            "Evaluating Performance...\n",
            "pass 49920, training loss 7.92905330657959\n",
            "Evaluating Performance...\n",
            "pass 49930, training loss 7.5407843589782715\n",
            "Evaluating Performance...\n",
            "pass 49940, training loss 7.922088623046875\n",
            "Evaluating Performance...\n",
            "pass 49950, training loss 8.963130950927734\n",
            "Evaluating Performance...\n",
            "pass 49960, training loss 7.853395938873291\n",
            "Evaluating Performance...\n",
            "pass 49970, training loss 5.542490005493164\n",
            "Evaluating Performance...\n",
            "pass 49980, training loss 7.4586358070373535\n",
            "Evaluating Performance...\n",
            "pass 49990, training loss 7.845156669616699\n",
            "Evaluating Performance...\n",
            "pass 50000, training loss 6.273677825927734\n",
            "Saving Checkpoint...\n",
            "checkpoint saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}